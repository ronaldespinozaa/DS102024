{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Aprendizaje no supervisado**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Introducci√≥n - Clasificaci√≥n _vs_ Clustering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el campo del aprendizaje autom√°tico, clasificaci√≥n y clustering son t√©cnicas cruciales para analizar datos, pero sirven a prop√≥sitos distintos.\n",
    "\n",
    "***Clasificaci√≥n***\n",
    "\n",
    "La clasificaci√≥n es un m√©todo de aprendizaje supervisado que predice la etiqueta de nuevas instancias bas√°ndose en un conjunto de entrenamiento con etiquetas conocidas. Se utiliza para tareas como detectar correos no deseados o diagnosticar enfermedades, evalu√°ndose por su precisi√≥n y capacidad de generalizaci√≥n.\n",
    "\n",
    "***Clustering***\n",
    "\n",
    "El clustering, en cambio, es un m√©todo de aprendizaje no supervisado que agrupa objetos seg√∫n su similitud sin etiquetas predefinidas. Es √∫til para descubrir estructuras en los datos y segmentar mercados, entre otros aplicaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "data.target_names\n",
    "\n",
    "plt.figure(figsize=(9, 3.5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(X[y==0, 2], X[y==0, 3], \"yo\", label=\"Iris setosa\")\n",
    "plt.plot(X[y==1, 2], X[y==1, 3], \"bs\", label=\"Iris versicolor\")\n",
    "plt.plot(X[y==2, 2], X[y==2, 3], \"g^\", label=\"Iris virginica\")\n",
    "plt.xlabel(\"Petal length\")\n",
    "plt.ylabel(\"Petal width\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(X[:, 2], X[:, 3], c=\"k\", marker=\".\")\n",
    "plt.xlabel(\"Petal length\")\n",
    "plt.tick_params(labelleft=False)\n",
    "plt.gca().set_axisbelow(True)\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota**: la siguiente celda muestra c√≥mo un modelo de mezcla gaussiana (explicado m√°s adelante en este notebook) puede realmente separar estos grupos bastante bien utilizando las 4 caracter√≠sticas: longitud y anchura de los p√©talos, y longitud y anchura de los s√©palos. Este c√≥digo asigna cada elemento a una clase. En lugar de codificar el mapeo, el c√≥digo elige la clase m√°s com√∫n para cada cluster usando la funci√≥n `scipy.stats.mode()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "y_pred = GaussianMixture(n_components=3, random_state=42).fit(X).predict(X)\n",
    "\n",
    "mapping = {}\n",
    "for class_id in np.unique(y):\n",
    "    mode, _ = stats.mode(y_pred[y==class_id])\n",
    "    mapping[mode] = class_id\n",
    "\n",
    "y_pred = np.array([mapping[cluster_id] for cluster_id in y_pred])\n",
    "\n",
    "plt.plot(X[y_pred==0, 2], X[y_pred==0, 3], \"yo\", label=\"Cluster 1\")\n",
    "plt.plot(X[y_pred==1, 2], X[y_pred==1, 3], \"bs\", label=\"Cluster 2\")\n",
    "plt.plot(X[y_pred==2, 2], X[y_pred==2, 3], \"g^\", label=\"Cluster 3\")\n",
    "plt.xlabel(\"Petal length\")\n",
    "plt.ylabel(\"Petal width\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¬øCu√°l es la proporci√≥n de plantas de iris que asignamos al grupo correcto?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_pred==y).sum() / len(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo K-Means es uno de los m√©todos de clustering m√°s utilizados en el aprendizaje no supervisado. Su objetivo es dividir un conjunto de datos en ùêæ grupos (o clusters) distintos bas√°ndose en la similitud de las caracter√≠sticas de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit y predict**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a entrenar un agrupador K-Means en un conjunto de datos de manchas. Intentar√° encontrar el centro de cada mancha y asignar cada instancia a la mancha m√°s cercana:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "\n",
    "blob_centers = np.array([[ 0.2,  2.3], [-1.5 ,  2.3], [-2.8,  1.8],\n",
    "                         [-2.8,  2.8], [-2.8,  1.3]])\n",
    "blob_std = np.array([0.4, 0.3, 0.1, 0.1, 0.1])\n",
    "X, y = make_blobs(n_samples=2000, centers=blob_centers, cluster_std=blob_std,\n",
    "                  random_state=7)\n",
    "\n",
    "k = 5\n",
    "kmeans = KMeans(n_clusters=k, n_init=10, random_state=42)\n",
    "y_pred = kmeans.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a trazarlos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters(X, y=None):\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=1)\n",
    "    plt.xlabel(\"$x_1$\")\n",
    "    plt.ylabel(\"$x_2$\", rotation=0)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plot_clusters(X)\n",
    "plt.gca().set_axisbelow(True)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada instancia se asign√≥ a uno de los 5 clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred is kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y se estimaron los siguientes 5 _centroides_ (es decir, centros de conglomerados):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ten en cuenta que la instancia `KMeans` conserva las etiquetas de las instancias con las que fue entrenada. De forma algo confusa, en este contexto, la _etiqueta_ de una instancia es el √≠ndice del cluster al que se asigna esa instancia (no son target, son predicciones):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por supuesto, podemos predecir las etiquetas de las nuevas instancias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_new = np.array([[0, 2], [3, 2], [-3, 3], [-3, 2.5]])\n",
    "kmeans.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**L√≠mites de la decisi√≥n**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a trazar los l√≠mites de decisi√≥n del modelo. Esto nos da un _diagrama de Voronoi_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_data(X):\n",
    "    plt.plot(X[:, 0], X[:, 1], 'k.', markersize=2)\n",
    "\n",
    "def plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):\n",
    "    if weights is not None:\n",
    "        centroids = centroids[weights > weights.max() / 10]\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "                marker='o', s=35, linewidths=8,\n",
    "                color=circle_color, zorder=10, alpha=0.9)\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "                marker='x', s=2, linewidths=12,\n",
    "                color=cross_color, zorder=11, alpha=1)\n",
    "\n",
    "def plot_decision_boundaries(clusterer, X, resolution=1000, show_centroids=True,\n",
    "                             show_xlabels=True, show_ylabels=True):\n",
    "    mins = X.min(axis=0) - 0.1\n",
    "    maxs = X.max(axis=0) + 0.1\n",
    "    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),\n",
    "                         np.linspace(mins[1], maxs[1], resolution))\n",
    "    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.contourf(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n",
    "                cmap=\"Pastel2\")\n",
    "    plt.contour(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n",
    "                linewidths=1, colors='k')\n",
    "    plot_data(X)\n",
    "    if show_centroids:\n",
    "        plot_centroids(clusterer.cluster_centers_)\n",
    "\n",
    "    if show_xlabels:\n",
    "        plt.xlabel(\"$x_1$\")\n",
    "    else:\n",
    "        plt.tick_params(labelbottom=False)\n",
    "    if show_ylabels:\n",
    "        plt.ylabel(\"$x_2$\", rotation=0)\n",
    "    else:\n",
    "        plt.tick_params(labelleft=False)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plot_decision_boundaries(kmeans, X)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No est√° mal. Algunas de las instancias cercanas a los bordes probablemente se asignaron al cl√∫ster equivocado, pero en general se ve bastante bien."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hard Clustering _vs_ Soft Clustering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hard Clustering\n",
    "El Hard Clustering asigna cada punto de datos a un √∫nico cluster, sin ambig√ºedad. Cada punto pertenece a un solo grupo, lo que significa que la pertenencia es exclusiva.\n",
    "\n",
    "Caracter√≠sticas:\n",
    "- Exclusividad: Cada punto es asignado a un solo cluster.\n",
    "- Determin√≠stico: La asignaci√≥n no admite incertidumbre; un punto o pertenece completamente a un cluster o no pertenece en absoluto.\n",
    "- Ejemplos comunes: K-Means y Hierarchical Clustering son t√©cnicas t√≠picas de hard clustering.\n",
    "\n",
    "Este enfoque es adecuado para aplicaciones donde las categor√≠as son mutuamente excluyentes y bien definidas, como en la clasificaci√≥n de documentos en categor√≠as espec√≠ficas donde cada documento solo puede pertenecer a un tema.\n",
    "\n",
    "### Soft Clustering\n",
    "En contraste, el Soft Clustering permite que cada punto de datos tenga un grado de pertenencia en m√∫ltiples clusters. No se trata simplemente de decidir si un punto pertenece a un grupo o no, sino m√°s bien de evaluar el grado de pertenencia o probabilidad de que el punto est√© en varios grupos simult√°neamente.\n",
    "\n",
    "Caracter√≠sticas:\n",
    "- Probabil√≠stico: Los puntos pueden pertenecer a m√∫ltiples clusters con diferentes grados de probabilidad o membres√≠a.\n",
    "- Flexible: Ideal para datos donde las categor√≠as no son mutuamente excluyentes o las relaciones son m√°s complejas.\n",
    "- Ejemplos comunes: Gaussian Mixture Models (GMM) y Fuzzy C-Means son ejemplos de t√©cnicas de soft clustering.\n",
    "\n",
    "Soft clustering es √∫til en escenarios donde los datos pueden exhibir m√∫ltiples categor√≠as simult√°neamente, como en la asignaci√≥n de g√©neros a pel√≠culas, donde una pel√≠cula puede ser tanto comedia como drama."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En lugar de elegir arbitrariamente el cluster m√°s cercano para cada instancia, lo que se llama _hard clustering_, ser√≠a mejor medir la distancia de cada instancia a los 5 centroides. Esto es lo que hace el m√©todo `transform()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.transform(X_new).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puede comprobar que se trata efectivamente de la distancia euclidiana entre cada instancia y cada centroide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.linalg.norm(np.tile(X_new, (1, k)).reshape(-1, k, 2)\n",
    "               - kmeans.cluster_centers_, axis=2).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The K-Means Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo K-Means es uno de los algoritmos de agrupaci√≥n m√°s r√°pidos, y tambi√©n uno de los m√°s sencillos:\n",
    "* Primero se inicializan $k$ centroides al azar: por ejemplo, se eligen $k$ instancias distintas al azar del conjunto de datos y se colocan los centroides en sus ubicaciones.\n",
    "* Se repite hasta la convergencia (es decir, hasta que los centroides dejan de moverse):\n",
    "    * Asignar cada instancia al centroide m√°s cercano.\n",
    "    * Actualizar los centroides para que sean la media de las instancias que se les han asignado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La clase `KMeans` utiliza por defecto una t√©cnica de inicializaci√≥n optimizada. Para obtener el algoritmo original de K-Means (s√≥lo con fines educativos), debe establecer `init=\"random\"` y `n_init=1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ejecutar el algoritmo K-Means durante 1, 2 y 3 iteraciones, para ver c√≥mo se mueven los centroides:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kmeans_iter1 = KMeans(n_clusters=5, init=\"random\", n_init=1, max_iter=1,\n",
    "                      random_state=5)\n",
    "kmeans_iter2 = KMeans(n_clusters=5, init=\"random\", n_init=1, max_iter=2,\n",
    "                      random_state=5)\n",
    "kmeans_iter3 = KMeans(n_clusters=5, init=\"random\", n_init=1, max_iter=3,\n",
    "                      random_state=5)\n",
    "kmeans_iter1.fit(X)\n",
    "kmeans_iter2.fit(X)\n",
    "kmeans_iter3.fit(X)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "plt.subplot(321)\n",
    "plot_data(X)\n",
    "plot_centroids(kmeans_iter1.cluster_centers_, circle_color='r', cross_color='w')\n",
    "plt.ylabel(\"$x_2$\", rotation=0)\n",
    "plt.tick_params(labelbottom=False)\n",
    "plt.title(\"Update the centroids (initially randomly)\")\n",
    "\n",
    "plt.subplot(322)\n",
    "plot_decision_boundaries(kmeans_iter1, X, show_xlabels=False,\n",
    "                         show_ylabels=False)\n",
    "plt.title(\"Label the instances\")\n",
    "\n",
    "plt.subplot(323)\n",
    "plot_decision_boundaries(kmeans_iter1, X, show_centroids=False,\n",
    "                         show_xlabels=False)\n",
    "plot_centroids(kmeans_iter2.cluster_centers_)\n",
    "\n",
    "plt.subplot(324)\n",
    "plot_decision_boundaries(kmeans_iter2, X, show_xlabels=False,\n",
    "                         show_ylabels=False)\n",
    "\n",
    "plt.subplot(325)\n",
    "plot_decision_boundaries(kmeans_iter2, X, show_centroids=False)\n",
    "plot_centroids(kmeans_iter3.cluster_centers_)\n",
    "\n",
    "plt.subplot(326)\n",
    "plot_decision_boundaries(kmeans_iter3, X, show_ylabels=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variabilidad K-Means**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el algoritmo original de K-Means, los centroides se inicializan aleatoriamente y el algoritmo simplemente ejecuta una √∫nica iteraci√≥n para mejorar gradualmente los centroides, como vimos anteriormente.\n",
    "\n",
    "Sin embargo, uno de los principales problemas de este enfoque es que si se ejecuta K-Means varias veces (o con diferentes semillas aleatorias), puede converger a soluciones muy diferentes, como se puede ver a continuaci√≥n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_clusterer_comparison(clusterer1, clusterer2, X, title1=None,\n",
    "                              title2=None):\n",
    "    clusterer1.fit(X)\n",
    "    clusterer2.fit(X)\n",
    "\n",
    "    plt.figure(figsize=(10, 3.2))\n",
    "\n",
    "    plt.subplot(121)\n",
    "    plot_decision_boundaries(clusterer1, X)\n",
    "    if title1:\n",
    "        plt.title(title1)\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plot_decision_boundaries(clusterer2, X, show_ylabels=False)\n",
    "    if title2:\n",
    "        plt.title(title2)\n",
    "\n",
    "kmeans_rnd_init1 = KMeans(n_clusters=5, init=\"random\", n_init=1, random_state=2)\n",
    "kmeans_rnd_init2 = KMeans(n_clusters=5, init=\"random\", n_init=1, random_state=9)\n",
    "\n",
    "plot_clusterer_comparison(kmeans_rnd_init1, kmeans_rnd_init2, X,\n",
    "                          \"Solution 1\",\n",
    "                          \"Solution 2 (with a different random init)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_init = np.array([[-3, 3], [-3, 2], [-3, 1], [-1, 2], [0, 2]])\n",
    "kmeans = KMeans(n_clusters=5, init=good_init, n_init=1, random_state=42)\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plot_decision_boundaries(kmeans, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inercia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para seleccionar el mejor modelo, necesitaremos una forma de evaluar el rendimiento de un modelo K-Mean. Por desgracia, la agrupaci√≥n es una tarea no supervisada, por lo que no disponemos de los objetivos. Pero al menos podemos medir la distancia entre cada instancia y su centroide. Esta es la idea que hay detr√°s de la m√©trica _inertia_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_rnd_init1.inertia_  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_rnd_init2.inertia_  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede comprobar f√°cilmente, la inercia es la suma de las distancias al cuadrado entre cada instancia de entrenamiento y su centroide m√°s cercano:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dist = kmeans.transform(X)\n",
    "(X_dist[np.arange(len(X_dist)), kmeans.labels_] ** 2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El m√©todo `score()` devuelve la inercia negativa. ¬øPor qu√© negativa? Pues porque el m√©todo `score()` de un predictor debe respetar siempre la regla \"_mayor es mejor_\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.score(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inicializaciones m√∫ltiples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por tanto, una forma de resolver el problema de la variabilidad es ejecutar el algoritmo K-Means varias veces con distintas inicializaciones aleatorias y seleccionar la soluci√≥n que minimice la inercia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_rnd_10_inits = KMeans(n_clusters=5, init=\"random\", n_init=10,\n",
    "                             random_state=2)\n",
    "kmeans_rnd_10_inits.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver, terminamos con el modelo inicial, que es sin duda la soluci√≥n √≥ptima de K-Means (al menos en t√©rminos de inercia, y suponiendo $k=5$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plot_decision_boundaries(kmeans_rnd_10_inits, X)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_rnd_10_inits.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M√©todos de inicializaci√≥n de centroides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En lugar de inicializar los centroides de forma totalmente aleatoria, es preferible inicializarlos utilizando el siguiente algoritmo, propuesto en un [documento de 2006](https://goo.gl/eNUPw6) por David Arthur y Sergei Vassilvitskii:\n",
    "* Tomar un centroide $c_1$, elegido uniformemente al azar del conjunto de datos.\n",
    "* Tomar un nuevo centroide $c_i$, eligiendo una instancia $\\mathbf{x}_i$ con probabilidad $D(\\mathbf{x}_i)^2$ / $\\sum\\limits_{j=1}^{m}{D(\\mathbf{x}_j)}^2$ donde $D(\\mathbf{x}_i)$ es la distancia entre la instancia $\\mathbf{x}_i$ y el centroide m√°s cercano ya elegido. Esta distribuci√≥n de probabilidad asegura que las instancias que est√°n m√°s lejos de los centroides ya elegidos tienen muchas m√°s probabilidades de ser seleccionadas como centroides.\n",
    "* Repita el paso anterior hasta que se hayan elegido todos los $k$ centroides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resto del algoritmo K-Means++ es K-Means normal. Con esta inicializaci√≥n, es mucho menos probable que el algoritmo K-Means converja a una soluci√≥n sub√≥ptima, por lo que es posible reducir `n_init` considerablemente. La mayor√≠a de las veces, esto compensa en gran medida la complejidad adicional del proceso de inicializaci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means acelerado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo K-Means puede acelerarse a veces evitando muchos c√°lculos de distancia innecesarios: esto se consigue explotando la desigualdad del tri√°ngulo (dados tres puntos A, B y C, la distancia AC es siempre tal que AC ‚â§ AB + BC) y llevando la cuenta de los l√≠mites inferior y superior de las distancias entre instancias y centroides (para m√°s detalles, v√©ase este [art√≠culo de 2003](https://www.aaai.org/Papers/ICML/2003/ICML03-022.pdf) de Charles Elkan)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-Batch K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn tambi√©n implementa una variante del algoritmo K-Means que admite minilotes (v√©ase [este documento](http://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "minibatch_kmeans = MiniBatchKMeans(n_clusters=5, n_init=3, random_state=42)\n",
    "minibatch_kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch_kmeans.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero carguemos MNIST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "mnist = fetch_openml('mnist_784', as_frame=False, parser=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a dividir el conjunto de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = mnist.data[:60000], mnist.target[:60000]\n",
    "X_test, y_test = mnist.data[60000:], mnist.target[60000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuaci√≥n, vamos a escribir el conjunto de entrenamiento en un `memmap`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"my_mnist.mmap\"\n",
    "X_memmap = np.memmap(filename, dtype='float32', mode='write',\n",
    "                     shape=X_train.shape)\n",
    "X_memmap[:] = X_train\n",
    "X_memmap.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "minibatch_kmeans = MiniBatchKMeans(n_clusters=10, batch_size=10,\n",
    "                                   n_init=3, random_state=42)\n",
    "minibatch_kmeans.fit(X_memmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a trazar la relaci√≥n de inercia y la relaci√≥n de tiempo de entrenamiento entre Mini-batch K-Means y K-Means normal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from timeit import timeit\n",
    "\n",
    "max_k = 100\n",
    "times = np.empty((max_k, 2))\n",
    "inertias = np.empty((max_k, 2))\n",
    "for k in range(1, max_k + 1):\n",
    "    kmeans_ = KMeans(n_clusters=k, algorithm=\"lloyd\", n_init=10, random_state=42)\n",
    "    minibatch_kmeans = MiniBatchKMeans(n_clusters=k, n_init=10, random_state=42)\n",
    "    print(f\"\\r{k}/{max_k}\", end=\"\")  # \\r returns to the start of line\n",
    "    times[k - 1, 0] = timeit(\"kmeans_.fit(X)\", number=10, globals=globals())\n",
    "    times[k - 1, 1] = timeit(\"minibatch_kmeans.fit(X)\", number=10,\n",
    "                             globals=globals())\n",
    "    inertias[k - 1, 0] = kmeans_.inertia_\n",
    "    inertias[k - 1, 1] = minibatch_kmeans.inertia_\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(range(1, max_k + 1), inertias[:, 0], \"r--\", label=\"K-Means\")\n",
    "plt.plot(range(1, max_k + 1), inertias[:, 1], \"b.-\", label=\"Mini-batch K-Means\")\n",
    "plt.xlabel(\"$k$\")\n",
    "plt.title(\"Inertia\")\n",
    "plt.legend()\n",
    "plt.axis([1, max_k, 0, 100])\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(range(1, max_k + 1), times[:, 0], \"r--\", label=\"K-Means\")\n",
    "plt.plot(range(1, max_k + 1), times[:, 1], \"b.-\", label=\"Mini-batch K-Means\")\n",
    "plt.xlabel(\"$k$\")\n",
    "plt.title(\"Training time (seconds)\")\n",
    "plt.axis([1, max_k, 0, 4])\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encontrar el n√∫mero √≥ptimo de conglomerados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¬øY si el n√∫mero de clusters se fija en un valor inferior o superior a 5?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kmeans_k3 = KMeans(n_clusters=3, n_init=10, random_state=42)\n",
    "kmeans_k8 = KMeans(n_clusters=8, n_init=10, random_state=42)\n",
    "\n",
    "plot_clusterer_comparison(kmeans_k3, kmeans_k8, X, \"$k=3$\", \"$k=8$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos dos modelos no se ven muy bien. ¬øY sus inercias?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_k3.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_k8.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, no podemos tomar simplemente el valor de $k$ que minimiza la inercia, ya que √©sta sigue disminuyendo a medida que aumentamos $k$. De hecho, cuantos m√°s conglomerados haya, m√°s cerca estar√° cada instancia de su centroide m√°s cercano y, por tanto, menor ser√° la inercia. Sin embargo, podemos representar la inercia en funci√≥n de $k$ y analizar la curva resultante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kmeans_per_k = [KMeans(n_clusters=k, n_init=10, random_state=42).fit(X)\n",
    "                for k in range(1, 10)]\n",
    "inertias = [model.inertia_ for model in kmeans_per_k]\n",
    "\n",
    "plt.figure(figsize=(8, 3.5))\n",
    "plt.plot(range(1, 10), inertias, \"bo-\")\n",
    "plt.xlabel(\"$k$\")\n",
    "plt.ylabel(\"Inertia\")\n",
    "plt.annotate(\"\", xy=(4, inertias[3]), xytext=(4.45, 650),\n",
    "             arrowprops=dict(facecolor='black', shrink=0.1))\n",
    "plt.text(4.5, 650, \"Elbow\", horizontalalignment=\"center\")\n",
    "plt.axis([1, 8.5, 0, 1300])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver, hay un codo en $k=4$, lo que significa que menos agrupaciones que √©sa ser√≠an malas, y m√°s agrupaciones no ayudar√≠an mucho y podr√≠an reducir las agrupaciones a la mitad. As√≠ que $k=4$ es una buena elecci√≥n. Por supuesto, en este ejemplo no es perfecto, ya que significa que las dos manchas de la parte inferior izquierda se considerar√°n como un solo conglomerado, pero no deja de ser una agrupaci√≥n bastante buena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundaries(kmeans_per_k[4 - 1], X)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otro m√©todo consiste en considerar la _puntuaci√≥n de silueta_, que es el _coeficiente de silueta_ medio de todas las instancias. El coeficiente de silueta de una instancia es igual a (_b_ - _a_) / max(_a_, _b_), donde _a_ es la distancia media a las dem√°s instancias del mismo cl√∫ster (es la _distancia media intracl√∫ster_), y _b_ es la _distancia media al cl√∫ster m√°s cercano_, es decir, la distancia media a las instancias del cl√∫ster m√°s cercano (definido como el que minimiza _b_, excluyendo el cl√∫ster de la propia instancia). El coeficiente de silueta puede variar entre -1 y +1: un coeficiente cercano a +1 significa que la instancia est√° bien dentro de su propio cluster y lejos de otros clusters, mientras que un coeficiente cercano a 0 significa que est√° cerca del l√≠mite de un cluster, y finalmente un coeficiente cercano a -1 significa que la instancia puede haber sido asignada al cluster equivocado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a trazar la puntuaci√≥n de la silueta en funci√≥n de $k$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_score(X, kmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "silhouette_scores = [silhouette_score(X, model.labels_)\n",
    "                     for model in kmeans_per_k[1:]]\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(range(2, 10), silhouette_scores, \"bo-\")\n",
    "plt.xlabel(\"$k$\")\n",
    "plt.ylabel(\"Silhouette score\")\n",
    "plt.axis([1.8, 8.5, 0.55, 0.7])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como puede ver, esta visualizaci√≥n es mucho m√°s mejor que la anterior: en particular, aunque confirma que $k=4$ es una muy buena elecci√≥n, tambi√©n subraya el hecho de que $k=5$ es bastante buena tambi√©n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El coeficiente de silueta de cada instancia, ordenado por el cluster al que est√° asignado y por el valor del coeficiente, ofrece una visualizaci√≥n a√∫n m√°s informativa. Esto se denomina _diagrama de siluetas_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import silhouette_samples\n",
    "from matplotlib.ticker import FixedLocator, FixedFormatter\n",
    "\n",
    "plt.figure(figsize=(11, 9))\n",
    "\n",
    "for k in (3, 4, 5, 6):\n",
    "    plt.subplot(2, 2, k - 2)\n",
    "    \n",
    "    y_pred = kmeans_per_k[k - 1].labels_\n",
    "    silhouette_coefficients = silhouette_samples(X, y_pred)\n",
    "\n",
    "    padding = len(X) // 30\n",
    "    pos = padding\n",
    "    ticks = []\n",
    "    for i in range(k):\n",
    "        coeffs = silhouette_coefficients[y_pred == i]\n",
    "        coeffs.sort()\n",
    "\n",
    "        color = plt.cm.Spectral(i / k)\n",
    "        plt.fill_betweenx(np.arange(pos, pos + len(coeffs)), 0, coeffs,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "        ticks.append(pos + len(coeffs) // 2)\n",
    "        pos += len(coeffs) + padding\n",
    "\n",
    "    plt.gca().yaxis.set_major_locator(FixedLocator(ticks))\n",
    "    plt.gca().yaxis.set_major_formatter(FixedFormatter(range(k)))\n",
    "    if k in (3, 5):\n",
    "        plt.ylabel(\"Cluster\")\n",
    "    \n",
    "    if k in (5, 6):\n",
    "        plt.gca().set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "        plt.xlabel(\"Silhouette Coefficient\")\n",
    "    else:\n",
    "        plt.tick_params(labelbottom=False)\n",
    "\n",
    "    plt.axvline(x=silhouette_scores[k - 2], color=\"red\", linestyle=\"--\")\n",
    "    plt.title(f\"$k={k}$\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como puede ver, $k=5$ parece la mejor opci√≥n aqu√≠, ya que todos los conglomerados tienen aproximadamente el mismo tama√±o, y todos cruzan la l√≠nea discontinua, que representa la puntuaci√≥n media de la silueta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L√≠mites de K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generemos un conjunto de datos m√°s dif√≠cil, con manchas alargadas y densidades variables, y demostremos que K-Means tiene dificultades para agruparlo correctamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X1, y1 = make_blobs(n_samples=1000, centers=((4, -4), (0, 0)), random_state=42)\n",
    "X1 = X1.dot(np.array([[0.374, 0.95], [0.732, 0.598]]))\n",
    "X2, y2 = make_blobs(n_samples=250, centers=1, random_state=42)\n",
    "X2 = X2 + [6, -8]\n",
    "X = np.r_[X1, X2]\n",
    "y = np.r_[y1, y2]\n",
    "\n",
    "kmeans_good = KMeans(n_clusters=3,\n",
    "                     init=np.array([[-1.5, 2.5], [0.5, 0], [4, 0]]),\n",
    "                     n_init=1, random_state=42)\n",
    "kmeans_bad = KMeans(n_clusters=3, n_init=10, random_state=42)\n",
    "kmeans_good.fit(X)\n",
    "kmeans_bad.fit(X)\n",
    "\n",
    "plt.figure(figsize=(10, 3.2))\n",
    "\n",
    "plt.subplot(121)\n",
    "plot_decision_boundaries(kmeans_good, X)\n",
    "plt.title(f\"Inertia = {kmeans_good.inertia_:.1f}\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plot_decision_boundaries(kmeans_bad, X, show_ylabels=False)\n",
    "plt.title(f\"Inertia = {kmeans_bad.inertia_:.1f}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uso de la agrupaci√≥n para la segmentaci√≥n de im√°genes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descarga la imagen de la mariquita:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "homl3_root = \"https://github.com/ageron/handson-ml3/raw/main/\"\n",
    "filename = \"ladybug.png\"\n",
    "directory = Path('img')\n",
    "filepath = directory / filename\n",
    "\n",
    "if not filepath.is_file():\n",
    "    print(\"Downloading\", filename)\n",
    "    url = f\"{homl3_root}/images/unsupervised_learning/{filename}\"\n",
    "    directory.mkdir(parents=True, exist_ok=True)  # Asegura que el directorio existe\n",
    "    urllib.request.urlretrieve(url, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "\n",
    "image = np.asarray(PIL.Image.open(filepath))\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = image.reshape(-1, 3)\n",
    "kmeans = KMeans(n_clusters=8, n_init=10, random_state=42).fit(X)\n",
    "segmented_img = kmeans.cluster_centers_[kmeans.labels_]\n",
    "segmented_img = segmented_img.reshape(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "segmented_imgs = []\n",
    "n_colors = (10, 8, 6, 4, 2)\n",
    "for n_clusters in n_colors:\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42).fit(X)\n",
    "    segmented_img = kmeans.cluster_centers_[kmeans.labels_]\n",
    "    segmented_imgs.append(segmented_img.reshape(image.shape))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplots_adjust(wspace=0.05, hspace=0.1)\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.imshow(image)\n",
    "plt.title(\"Original image\")\n",
    "plt.axis('off')\n",
    "\n",
    "for idx, n_clusters in enumerate(n_colors):\n",
    "    plt.subplot(2, 3, 2 + idx)\n",
    "    plt.imshow(segmented_imgs[idx] / 255)\n",
    "    plt.title(f\"{n_clusters} colors\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uso de la agrupaci√≥n para el aprendizaje semisupervisado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otro caso de uso de la agrupaci√≥n es el aprendizaje semisupervisado, cuando tenemos muchas instancias sin etiquetar y muy pocas etiquetadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a abordar el conjunto de datos _digits_, que es un conjunto de datos sencillo similar al MNIST que contiene 1.797 im√°genes de 8√ó8 en escala de grises que representan los d√≠gitos del 0 al 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "X_digits, y_digits = load_digits(return_X_y=True)\n",
    "X_train, y_train = X_digits[:1400], y_digits[:1400]\n",
    "X_test, y_test = X_digits[1400:], y_digits[1400:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos el rendimiento de un modelo de regresi√≥n log√≠stica cuando s√≥lo tenemos 50 instancias etiquetadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "n_labeled = 50\n",
    "log_reg = LogisticRegression(max_iter=10_000)\n",
    "log_reg.fit(X_train[:n_labeled], y_train[:n_labeled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# medir la precisi√≥n cuando utilizamos todo el conjunto de entrenamiento\n",
    "log_reg_full = LogisticRegression(max_iter=10_000)\n",
    "log_reg_full.fit(X_train, y_train)\n",
    "log_reg_full.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es mucho menos que antes, por supuesto. Veamos c√≥mo podemos hacerlo mejor. En primer lugar, agrupemos el conjunto de entrenamiento en 50 clusters y, a continuaci√≥n, busquemos en cada cluster la imagen m√°s cercana al centroide. A estas im√°genes las llamaremos im√°genes representativas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 50\n",
    "kmeans = KMeans(n_clusters=k, n_init=10, random_state=42)\n",
    "X_digits_dist = kmeans.fit_transform(X_train)\n",
    "representative_digit_idx = X_digits_dist.argmin(axis=0)\n",
    "X_representative_digits = X_train[representative_digit_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a trazar estas im√°genes representativas y a etiquetarlas manualmente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(8, 2))\n",
    "for index, X_representative_digit in enumerate(X_representative_digits):\n",
    "    plt.subplot(k // 10, 10, index + 1)\n",
    "    plt.imshow(X_representative_digit.reshape(8, 8), cmap=\"binary\",\n",
    "               interpolation=\"bilinear\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_representative_digits = np.array([\n",
    "    1, 3, 6, 0, 7, 9, 2, 4, 8, 9,\n",
    "    5, 4, 7, 1, 2, 6, 1, 2, 5, 1,\n",
    "    4, 1, 3, 3, 8, 8, 2, 5, 6, 9,\n",
    "    1, 4, 0, 6, 8, 3, 4, 6, 7, 2,\n",
    "    4, 1, 0, 7, 5, 1, 9, 9, 3, 7\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora tenemos un conjunto de datos con s√≥lo 50 instancias etiquetadas, pero en lugar de ser instancias completamente aleatorias, cada una de ellas es una imagen representativa de su cl√∫ster. Veamos si el rendimiento es mejor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(max_iter=10_000)\n",
    "log_reg.fit(X_representative_digits, y_representative_digits)\n",
    "log_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vaya. Hemos pasado de una precisi√≥n del 74,8% al 84,9%, aunque todav√≠a s√≥lo estamos entrenando el modelo con 50 instancias. Dado que a menudo es costoso y doloroso etiquetar instancias, especialmente cuando lo tienen que hacer manualmente expertos, es una buena idea hacer que etiqueten instancias representativas en lugar de instancias aleatorias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero quiz√° podamos ir un paso m√°s all√°: ¬øy si propagamos las etiquetas a todas las dem√°s instancias del mismo cl√∫ster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_propagated = np.empty(len(X_train), dtype=np.int64)\n",
    "for i in range(k):\n",
    "    y_train_propagated[kmeans.labels_ == i] = y_representative_digits[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(max_iter=10_000)\n",
    "log_reg.fit(X_train, y_train_propagated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos obtenido otro aumento significativo de la precisi√≥n. Veamos si podemos hacerlo a√∫n mejor ignorando el 1% de instancias que est√°n m√°s lejos del centro de su cl√∫ster: esto deber√≠a eliminar algunos valores at√≠picos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_closest = 99\n",
    "\n",
    "X_cluster_dist = X_digits_dist[np.arange(len(X_train)), kmeans.labels_]\n",
    "for i in range(k):\n",
    "    in_cluster = (kmeans.labels_ == i)\n",
    "    cluster_dist = X_cluster_dist[in_cluster]\n",
    "    cutoff_distance = np.percentile(cluster_dist, percentile_closest)\n",
    "    above_cutoff = (X_cluster_dist > cutoff_distance)\n",
    "    X_cluster_dist[in_cluster & above_cutoff] = -1\n",
    "\n",
    "partially_propagated = (X_cluster_dist != -1)\n",
    "X_train_partially_propagated = X_train[partially_propagated]\n",
    "y_train_partially_propagated = y_train_propagated[partially_propagated]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(max_iter=10_000)\n",
    "log_reg.fit(X_train_partially_propagated, y_train_partially_propagated)\n",
    "log_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vaya, ¬°otro aumento de la precisi√≥n! Incluso hemos superado ligeramente el rendimiento que obtuvimos entrenando con el conjunto de entrenamiento totalmente etiquetado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestras etiquetas propagadas son bastante buenas: su precisi√≥n ronda el 97,6%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_train_partially_propagated == y_train[partially_propagated]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podr√≠as hacer algunas iteraciones de *aprendizaje activo*:\n",
    "1. Etiquetar manualmente las instancias de las que el clasificador est√° menos seguro, si es posible, agrup√°ndolas en clusters distintos.\n",
    "2. Entrenar un nuevo modelo con estas etiquetas adicionales."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Aprendizaje no supervisado**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Introducción - Clasificación _vs_ Clustering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el campo del aprendizaje automático, clasificación y clustering son técnicas cruciales para analizar datos, pero sirven a propósitos distintos.\n",
    "\n",
    "***Clasificación***\n",
    "\n",
    "La clasificación es un método de aprendizaje supervisado que predice la etiqueta de nuevas instancias basándose en un conjunto de entrenamiento con etiquetas conocidas. Se utiliza para tareas como detectar correos no deseados o diagnosticar enfermedades, evaluándose por su precisión y capacidad de generalización.\n",
    "\n",
    "***Clustering***\n",
    "\n",
    "El clustering, en cambio, es un método de aprendizaje no supervisado que agrupa objetos según su similitud sin etiquetas predefinidas. Es útil para descubrir estructuras en los datos y segmentar mercados, entre otros aplicaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "data.target_names\n",
    "\n",
    "plt.figure(figsize=(9, 3.5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(X[y==0, 2], X[y==0, 3], \"yo\", label=\"Iris setosa\")\n",
    "plt.plot(X[y==1, 2], X[y==1, 3], \"bs\", label=\"Iris versicolor\")\n",
    "plt.plot(X[y==2, 2], X[y==2, 3], \"g^\", label=\"Iris virginica\")\n",
    "plt.xlabel(\"Petal length\")\n",
    "plt.ylabel(\"Petal width\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(X[:, 2], X[:, 3], c=\"k\", marker=\".\")\n",
    "plt.xlabel(\"Petal length\")\n",
    "plt.tick_params(labelleft=False)\n",
    "plt.gca().set_axisbelow(True)\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota**: la siguiente celda muestra cómo un modelo de mezcla gaussiana (explicado más adelante en este notebook) puede realmente separar estos grupos bastante bien utilizando las 4 características: longitud y anchura de los pétalos, y longitud y anchura de los sépalos. Este código asigna cada elemento a una clase. En lugar de codificar el mapeo, el código elige la clase más común para cada cluster usando la función `scipy.stats.mode()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "y_pred = GaussianMixture(n_components=3, random_state=42).fit(X).predict(X)\n",
    "\n",
    "mapping = {}\n",
    "for class_id in np.unique(y):\n",
    "    mode, _ = stats.mode(y_pred[y==class_id])\n",
    "    mapping[mode] = class_id\n",
    "\n",
    "y_pred = np.array([mapping[cluster_id] for cluster_id in y_pred])\n",
    "\n",
    "plt.plot(X[y_pred==0, 2], X[y_pred==0, 3], \"yo\", label=\"Cluster 1\")\n",
    "plt.plot(X[y_pred==1, 2], X[y_pred==1, 3], \"bs\", label=\"Cluster 2\")\n",
    "plt.plot(X[y_pred==2, 2], X[y_pred==2, 3], \"g^\", label=\"Cluster 3\")\n",
    "plt.xlabel(\"Petal length\")\n",
    "plt.ylabel(\"Petal width\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cuál es la proporción de plantas de iris que asignamos al grupo correcto?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_pred==y).sum() / len(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo K-Means es uno de los métodos de clustering más utilizados en el aprendizaje no supervisado. Su objetivo es dividir un conjunto de datos en 𝐾 grupos (o clusters) distintos basándose en la similitud de las características de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit y predict**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a entrenar un agrupador K-Means en un conjunto de datos de manchas. Intentará encontrar el centro de cada mancha y asignar cada instancia a la mancha más cercana:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "\n",
    "blob_centers = np.array([[ 0.2,  2.3], [-1.5 ,  2.3], [-2.8,  1.8],\n",
    "                         [-2.8,  2.8], [-2.8,  1.3]])\n",
    "blob_std = np.array([0.4, 0.3, 0.1, 0.1, 0.1])\n",
    "X, y = make_blobs(n_samples=2000, centers=blob_centers, cluster_std=blob_std,\n",
    "                  random_state=7)\n",
    "\n",
    "k = 5\n",
    "kmeans = KMeans(n_clusters=k, n_init=10, random_state=42)\n",
    "y_pred = kmeans.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a trazarlos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters(X, y=None):\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=1)\n",
    "    plt.xlabel(\"$x_1$\")\n",
    "    plt.ylabel(\"$x_2$\", rotation=0)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plot_clusters(X)\n",
    "plt.gca().set_axisbelow(True)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada instancia se asignó a uno de los 5 clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred is kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y se estimaron los siguientes 5 _centroides_ (es decir, centros de conglomerados):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ten en cuenta que la instancia `KMeans` conserva las etiquetas de las instancias con las que fue entrenada. De forma algo confusa, en este contexto, la _etiqueta_ de una instancia es el índice del cluster al que se asigna esa instancia (no son target, son predicciones):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por supuesto, podemos predecir las etiquetas de las nuevas instancias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_new = np.array([[0, 2], [3, 2], [-3, 3], [-3, 2.5]])\n",
    "kmeans.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Límites de la decisión**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a trazar los límites de decisión del modelo. Esto nos da un _diagrama de Voronoi_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_data(X):\n",
    "    plt.plot(X[:, 0], X[:, 1], 'k.', markersize=2)\n",
    "\n",
    "def plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):\n",
    "    if weights is not None:\n",
    "        centroids = centroids[weights > weights.max() / 10]\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "                marker='o', s=35, linewidths=8,\n",
    "                color=circle_color, zorder=10, alpha=0.9)\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "                marker='x', s=2, linewidths=12,\n",
    "                color=cross_color, zorder=11, alpha=1)\n",
    "\n",
    "def plot_decision_boundaries(clusterer, X, resolution=1000, show_centroids=True,\n",
    "                             show_xlabels=True, show_ylabels=True):\n",
    "    mins = X.min(axis=0) - 0.1\n",
    "    maxs = X.max(axis=0) + 0.1\n",
    "    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),\n",
    "                         np.linspace(mins[1], maxs[1], resolution))\n",
    "    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.contourf(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n",
    "                cmap=\"Pastel2\")\n",
    "    plt.contour(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n",
    "                linewidths=1, colors='k')\n",
    "    plot_data(X)\n",
    "    if show_centroids:\n",
    "        plot_centroids(clusterer.cluster_centers_)\n",
    "\n",
    "    if show_xlabels:\n",
    "        plt.xlabel(\"$x_1$\")\n",
    "    else:\n",
    "        plt.tick_params(labelbottom=False)\n",
    "    if show_ylabels:\n",
    "        plt.ylabel(\"$x_2$\", rotation=0)\n",
    "    else:\n",
    "        plt.tick_params(labelleft=False)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plot_decision_boundaries(kmeans, X)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No está mal. Algunas de las instancias cercanas a los bordes probablemente se asignaron al clúster equivocado, pero en general se ve bastante bien."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hard Clustering _vs_ Soft Clustering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hard Clustering\n",
    "El Hard Clustering asigna cada punto de datos a un único cluster, sin ambigüedad. Cada punto pertenece a un solo grupo, lo que significa que la pertenencia es exclusiva.\n",
    "\n",
    "Características:\n",
    "- Exclusividad: Cada punto es asignado a un solo cluster.\n",
    "- Determinístico: La asignación no admite incertidumbre; un punto o pertenece completamente a un cluster o no pertenece en absoluto.\n",
    "- Ejemplos comunes: K-Means y Hierarchical Clustering son técnicas típicas de hard clustering.\n",
    "\n",
    "Este enfoque es adecuado para aplicaciones donde las categorías son mutuamente excluyentes y bien definidas, como en la clasificación de documentos en categorías específicas donde cada documento solo puede pertenecer a un tema.\n",
    "\n",
    "### Soft Clustering\n",
    "En contraste, el Soft Clustering permite que cada punto de datos tenga un grado de pertenencia en múltiples clusters. No se trata simplemente de decidir si un punto pertenece a un grupo o no, sino más bien de evaluar el grado de pertenencia o probabilidad de que el punto esté en varios grupos simultáneamente.\n",
    "\n",
    "Características:\n",
    "- Probabilístico: Los puntos pueden pertenecer a múltiples clusters con diferentes grados de probabilidad o membresía.\n",
    "- Flexible: Ideal para datos donde las categorías no son mutuamente excluyentes o las relaciones son más complejas.\n",
    "- Ejemplos comunes: Gaussian Mixture Models (GMM) y Fuzzy C-Means son ejemplos de técnicas de soft clustering.\n",
    "\n",
    "Soft clustering es útil en escenarios donde los datos pueden exhibir múltiples categorías simultáneamente, como en la asignación de géneros a películas, donde una película puede ser tanto comedia como drama."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En lugar de elegir arbitrariamente el cluster más cercano para cada instancia, lo que se llama _hard clustering_, sería mejor medir la distancia de cada instancia a los 5 centroides. Esto es lo que hace el método `transform()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.transform(X_new).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puede comprobar que se trata efectivamente de la distancia euclidiana entre cada instancia y cada centroide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.linalg.norm(np.tile(X_new, (1, k)).reshape(-1, k, 2)\n",
    "               - kmeans.cluster_centers_, axis=2).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The K-Means Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo K-Means es uno de los algoritmos de agrupación más rápidos, y también uno de los más sencillos:\n",
    "* Primero se inicializan $k$ centroides al azar: por ejemplo, se eligen $k$ instancias distintas al azar del conjunto de datos y se colocan los centroides en sus ubicaciones.\n",
    "* Se repite hasta la convergencia (es decir, hasta que los centroides dejan de moverse):\n",
    "    * Asignar cada instancia al centroide más cercano.\n",
    "    * Actualizar los centroides para que sean la media de las instancias que se les han asignado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La clase `KMeans` utiliza por defecto una técnica de inicialización optimizada. Para obtener el algoritmo original de K-Means (sólo con fines educativos), debe establecer `init=\"random\"` y `n_init=1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ejecutar el algoritmo K-Means durante 1, 2 y 3 iteraciones, para ver cómo se mueven los centroides:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kmeans_iter1 = KMeans(n_clusters=5, init=\"random\", n_init=1, max_iter=1,\n",
    "                      random_state=5)\n",
    "kmeans_iter2 = KMeans(n_clusters=5, init=\"random\", n_init=1, max_iter=2,\n",
    "                      random_state=5)\n",
    "kmeans_iter3 = KMeans(n_clusters=5, init=\"random\", n_init=1, max_iter=3,\n",
    "                      random_state=5)\n",
    "kmeans_iter1.fit(X)\n",
    "kmeans_iter2.fit(X)\n",
    "kmeans_iter3.fit(X)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "plt.subplot(321)\n",
    "plot_data(X)\n",
    "plot_centroids(kmeans_iter1.cluster_centers_, circle_color='r', cross_color='w')\n",
    "plt.ylabel(\"$x_2$\", rotation=0)\n",
    "plt.tick_params(labelbottom=False)\n",
    "plt.title(\"Update the centroids (initially randomly)\")\n",
    "\n",
    "plt.subplot(322)\n",
    "plot_decision_boundaries(kmeans_iter1, X, show_xlabels=False,\n",
    "                         show_ylabels=False)\n",
    "plt.title(\"Label the instances\")\n",
    "\n",
    "plt.subplot(323)\n",
    "plot_decision_boundaries(kmeans_iter1, X, show_centroids=False,\n",
    "                         show_xlabels=False)\n",
    "plot_centroids(kmeans_iter2.cluster_centers_)\n",
    "\n",
    "plt.subplot(324)\n",
    "plot_decision_boundaries(kmeans_iter2, X, show_xlabels=False,\n",
    "                         show_ylabels=False)\n",
    "\n",
    "plt.subplot(325)\n",
    "plot_decision_boundaries(kmeans_iter2, X, show_centroids=False)\n",
    "plot_centroids(kmeans_iter3.cluster_centers_)\n",
    "\n",
    "plt.subplot(326)\n",
    "plot_decision_boundaries(kmeans_iter3, X, show_ylabels=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variabilidad K-Means**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el algoritmo original de K-Means, los centroides se inicializan aleatoriamente y el algoritmo simplemente ejecuta una única iteración para mejorar gradualmente los centroides, como vimos anteriormente.\n",
    "\n",
    "Sin embargo, uno de los principales problemas de este enfoque es que si se ejecuta K-Means varias veces (o con diferentes semillas aleatorias), puede converger a soluciones muy diferentes, como se puede ver a continuación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_clusterer_comparison(clusterer1, clusterer2, X, title1=None,\n",
    "                              title2=None):\n",
    "    clusterer1.fit(X)\n",
    "    clusterer2.fit(X)\n",
    "\n",
    "    plt.figure(figsize=(10, 3.2))\n",
    "\n",
    "    plt.subplot(121)\n",
    "    plot_decision_boundaries(clusterer1, X)\n",
    "    if title1:\n",
    "        plt.title(title1)\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plot_decision_boundaries(clusterer2, X, show_ylabels=False)\n",
    "    if title2:\n",
    "        plt.title(title2)\n",
    "\n",
    "kmeans_rnd_init1 = KMeans(n_clusters=5, init=\"random\", n_init=1, random_state=2)\n",
    "kmeans_rnd_init2 = KMeans(n_clusters=5, init=\"random\", n_init=1, random_state=9)\n",
    "\n",
    "plot_clusterer_comparison(kmeans_rnd_init1, kmeans_rnd_init2, X,\n",
    "                          \"Solution 1\",\n",
    "                          \"Solution 2 (with a different random init)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_init = np.array([[-3, 3], [-3, 2], [-3, 1], [-1, 2], [0, 2]])\n",
    "kmeans = KMeans(n_clusters=5, init=good_init, n_init=1, random_state=42)\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plot_decision_boundaries(kmeans, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inercia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para seleccionar el mejor modelo, necesitaremos una forma de evaluar el rendimiento de un modelo K-Mean. Por desgracia, la agrupación es una tarea no supervisada, por lo que no disponemos de los objetivos. Pero al menos podemos medir la distancia entre cada instancia y su centroide. Esta es la idea que hay detrás de la métrica _inertia_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_rnd_init1.inertia_  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_rnd_init2.inertia_  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede comprobar fácilmente, la inercia es la suma de las distancias al cuadrado entre cada instancia de entrenamiento y su centroide más cercano:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dist = kmeans.transform(X)\n",
    "(X_dist[np.arange(len(X_dist)), kmeans.labels_] ** 2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método `score()` devuelve la inercia negativa. ¿Por qué negativa? Pues porque el método `score()` de un predictor debe respetar siempre la regla \"_mayor es mejor_\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.score(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inicializaciones múltiples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por tanto, una forma de resolver el problema de la variabilidad es ejecutar el algoritmo K-Means varias veces con distintas inicializaciones aleatorias y seleccionar la solución que minimice la inercia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_rnd_10_inits = KMeans(n_clusters=5, init=\"random\", n_init=10,\n",
    "                             random_state=2)\n",
    "kmeans_rnd_10_inits.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver, terminamos con el modelo inicial, que es sin duda la solución óptima de K-Means (al menos en términos de inercia, y suponiendo $k=5$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plot_decision_boundaries(kmeans_rnd_10_inits, X)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_rnd_10_inits.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métodos de inicialización de centroides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En lugar de inicializar los centroides de forma totalmente aleatoria, es preferible inicializarlos utilizando el siguiente algoritmo, propuesto en un [documento de 2006](https://goo.gl/eNUPw6) por David Arthur y Sergei Vassilvitskii:\n",
    "* Tomar un centroide $c_1$, elegido uniformemente al azar del conjunto de datos.\n",
    "* Tomar un nuevo centroide $c_i$, eligiendo una instancia $\\mathbf{x}_i$ con probabilidad $D(\\mathbf{x}_i)^2$ / $\\sum\\limits_{j=1}^{m}{D(\\mathbf{x}_j)}^2$ donde $D(\\mathbf{x}_i)$ es la distancia entre la instancia $\\mathbf{x}_i$ y el centroide más cercano ya elegido. Esta distribución de probabilidad asegura que las instancias que están más lejos de los centroides ya elegidos tienen muchas más probabilidades de ser seleccionadas como centroides.\n",
    "* Repita el paso anterior hasta que se hayan elegido todos los $k$ centroides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resto del algoritmo K-Means++ es K-Means normal. Con esta inicialización, es mucho menos probable que el algoritmo K-Means converja a una solución subóptima, por lo que es posible reducir `n_init` considerablemente. La mayoría de las veces, esto compensa en gran medida la complejidad adicional del proceso de inicialización."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means acelerado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo K-Means puede acelerarse a veces evitando muchos cálculos de distancia innecesarios: esto se consigue explotando la desigualdad del triángulo (dados tres puntos A, B y C, la distancia AC es siempre tal que AC ≤ AB + BC) y llevando la cuenta de los límites inferior y superior de las distancias entre instancias y centroides (para más detalles, véase este [artículo de 2003](https://www.aaai.org/Papers/ICML/2003/ICML03-022.pdf) de Charles Elkan)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-Batch K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn también implementa una variante del algoritmo K-Means que admite minilotes (véase [este documento](http://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "minibatch_kmeans = MiniBatchKMeans(n_clusters=5, n_init=3, random_state=42)\n",
    "minibatch_kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch_kmeans.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero carguemos MNIST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "mnist = fetch_openml('mnist_784', as_frame=False, parser=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a dividir el conjunto de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = mnist.data[:60000], mnist.target[:60000]\n",
    "X_test, y_test = mnist.data[60000:], mnist.target[60000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, vamos a escribir el conjunto de entrenamiento en un `memmap`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"my_mnist.mmap\"\n",
    "X_memmap = np.memmap(filename, dtype='float32', mode='write',\n",
    "                     shape=X_train.shape)\n",
    "X_memmap[:] = X_train\n",
    "X_memmap.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "minibatch_kmeans = MiniBatchKMeans(n_clusters=10, batch_size=10,\n",
    "                                   n_init=3, random_state=42)\n",
    "minibatch_kmeans.fit(X_memmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a trazar la relación de inercia y la relación de tiempo de entrenamiento entre Mini-batch K-Means y K-Means normal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from timeit import timeit\n",
    "\n",
    "max_k = 100\n",
    "times = np.empty((max_k, 2))\n",
    "inertias = np.empty((max_k, 2))\n",
    "for k in range(1, max_k + 1):\n",
    "    kmeans_ = KMeans(n_clusters=k, algorithm=\"lloyd\", n_init=10, random_state=42)\n",
    "    minibatch_kmeans = MiniBatchKMeans(n_clusters=k, n_init=10, random_state=42)\n",
    "    print(f\"\\r{k}/{max_k}\", end=\"\")  # \\r returns to the start of line\n",
    "    times[k - 1, 0] = timeit(\"kmeans_.fit(X)\", number=10, globals=globals())\n",
    "    times[k - 1, 1] = timeit(\"minibatch_kmeans.fit(X)\", number=10,\n",
    "                             globals=globals())\n",
    "    inertias[k - 1, 0] = kmeans_.inertia_\n",
    "    inertias[k - 1, 1] = minibatch_kmeans.inertia_\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(range(1, max_k + 1), inertias[:, 0], \"r--\", label=\"K-Means\")\n",
    "plt.plot(range(1, max_k + 1), inertias[:, 1], \"b.-\", label=\"Mini-batch K-Means\")\n",
    "plt.xlabel(\"$k$\")\n",
    "plt.title(\"Inertia\")\n",
    "plt.legend()\n",
    "plt.axis([1, max_k, 0, 100])\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(range(1, max_k + 1), times[:, 0], \"r--\", label=\"K-Means\")\n",
    "plt.plot(range(1, max_k + 1), times[:, 1], \"b.-\", label=\"Mini-batch K-Means\")\n",
    "plt.xlabel(\"$k$\")\n",
    "plt.title(\"Training time (seconds)\")\n",
    "plt.axis([1, max_k, 0, 4])\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encontrar el número óptimo de conglomerados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Y si el número de clusters se fija en un valor inferior o superior a 5?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kmeans_k3 = KMeans(n_clusters=3, n_init=10, random_state=42)\n",
    "kmeans_k8 = KMeans(n_clusters=8, n_init=10, random_state=42)\n",
    "\n",
    "plot_clusterer_comparison(kmeans_k3, kmeans_k8, X, \"$k=3$\", \"$k=8$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos dos modelos no se ven muy bien. ¿Y sus inercias?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_k3.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_k8.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, no podemos tomar simplemente el valor de $k$ que minimiza la inercia, ya que ésta sigue disminuyendo a medida que aumentamos $k$. De hecho, cuantos más conglomerados haya, más cerca estará cada instancia de su centroide más cercano y, por tanto, menor será la inercia. Sin embargo, podemos representar la inercia en función de $k$ y analizar la curva resultante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kmeans_per_k = [KMeans(n_clusters=k, n_init=10, random_state=42).fit(X)\n",
    "                for k in range(1, 10)]\n",
    "inertias = [model.inertia_ for model in kmeans_per_k]\n",
    "\n",
    "plt.figure(figsize=(8, 3.5))\n",
    "plt.plot(range(1, 10), inertias, \"bo-\")\n",
    "plt.xlabel(\"$k$\")\n",
    "plt.ylabel(\"Inertia\")\n",
    "plt.annotate(\"\", xy=(4, inertias[3]), xytext=(4.45, 650),\n",
    "             arrowprops=dict(facecolor='black', shrink=0.1))\n",
    "plt.text(4.5, 650, \"Elbow\", horizontalalignment=\"center\")\n",
    "plt.axis([1, 8.5, 0, 1300])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver, hay un codo en $k=4$, lo que significa que menos agrupaciones que ésa serían malas, y más agrupaciones no ayudarían mucho y podrían reducir las agrupaciones a la mitad. Así que $k=4$ es una buena elección. Por supuesto, en este ejemplo no es perfecto, ya que significa que las dos manchas de la parte inferior izquierda se considerarán como un solo conglomerado, pero no deja de ser una agrupación bastante buena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundaries(kmeans_per_k[4 - 1], X)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otro método consiste en considerar la _puntuación de silueta_, que es el _coeficiente de silueta_ medio de todas las instancias. El coeficiente de silueta de una instancia es igual a (_b_ - _a_) / max(_a_, _b_), donde _a_ es la distancia media a las demás instancias del mismo clúster (es la _distancia media intraclúster_), y _b_ es la _distancia media al clúster más cercano_, es decir, la distancia media a las instancias del clúster más cercano (definido como el que minimiza _b_, excluyendo el clúster de la propia instancia). El coeficiente de silueta puede variar entre -1 y +1: un coeficiente cercano a +1 significa que la instancia está bien dentro de su propio cluster y lejos de otros clusters, mientras que un coeficiente cercano a 0 significa que está cerca del límite de un cluster, y finalmente un coeficiente cercano a -1 significa que la instancia puede haber sido asignada al cluster equivocado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a trazar la puntuación de la silueta en función de $k$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_score(X, kmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "silhouette_scores = [silhouette_score(X, model.labels_)\n",
    "                     for model in kmeans_per_k[1:]]\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(range(2, 10), silhouette_scores, \"bo-\")\n",
    "plt.xlabel(\"$k$\")\n",
    "plt.ylabel(\"Silhouette score\")\n",
    "plt.axis([1.8, 8.5, 0.55, 0.7])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como puede ver, esta visualización es mucho más mejor que la anterior: en particular, aunque confirma que $k=4$ es una muy buena elección, también subraya el hecho de que $k=5$ es bastante buena también."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El coeficiente de silueta de cada instancia, ordenado por el cluster al que está asignado y por el valor del coeficiente, ofrece una visualización aún más informativa. Esto se denomina _diagrama de siluetas_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import silhouette_samples\n",
    "from matplotlib.ticker import FixedLocator, FixedFormatter\n",
    "\n",
    "plt.figure(figsize=(11, 9))\n",
    "\n",
    "for k in (3, 4, 5, 6):\n",
    "    plt.subplot(2, 2, k - 2)\n",
    "    \n",
    "    y_pred = kmeans_per_k[k - 1].labels_\n",
    "    silhouette_coefficients = silhouette_samples(X, y_pred)\n",
    "\n",
    "    padding = len(X) // 30\n",
    "    pos = padding\n",
    "    ticks = []\n",
    "    for i in range(k):\n",
    "        coeffs = silhouette_coefficients[y_pred == i]\n",
    "        coeffs.sort()\n",
    "\n",
    "        color = plt.cm.Spectral(i / k)\n",
    "        plt.fill_betweenx(np.arange(pos, pos + len(coeffs)), 0, coeffs,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "        ticks.append(pos + len(coeffs) // 2)\n",
    "        pos += len(coeffs) + padding\n",
    "\n",
    "    plt.gca().yaxis.set_major_locator(FixedLocator(ticks))\n",
    "    plt.gca().yaxis.set_major_formatter(FixedFormatter(range(k)))\n",
    "    if k in (3, 5):\n",
    "        plt.ylabel(\"Cluster\")\n",
    "    \n",
    "    if k in (5, 6):\n",
    "        plt.gca().set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "        plt.xlabel(\"Silhouette Coefficient\")\n",
    "    else:\n",
    "        plt.tick_params(labelbottom=False)\n",
    "\n",
    "    plt.axvline(x=silhouette_scores[k - 2], color=\"red\", linestyle=\"--\")\n",
    "    plt.title(f\"$k={k}$\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como puede ver, $k=5$ parece la mejor opción aquí, ya que todos los conglomerados tienen aproximadamente el mismo tamaño, y todos cruzan la línea discontinua, que representa la puntuación media de la silueta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Límites de K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generemos un conjunto de datos más difícil, con manchas alargadas y densidades variables, y demostremos que K-Means tiene dificultades para agruparlo correctamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X1, y1 = make_blobs(n_samples=1000, centers=((4, -4), (0, 0)), random_state=42)\n",
    "X1 = X1.dot(np.array([[0.374, 0.95], [0.732, 0.598]]))\n",
    "X2, y2 = make_blobs(n_samples=250, centers=1, random_state=42)\n",
    "X2 = X2 + [6, -8]\n",
    "X = np.r_[X1, X2]\n",
    "y = np.r_[y1, y2]\n",
    "\n",
    "kmeans_good = KMeans(n_clusters=3,\n",
    "                     init=np.array([[-1.5, 2.5], [0.5, 0], [4, 0]]),\n",
    "                     n_init=1, random_state=42)\n",
    "kmeans_bad = KMeans(n_clusters=3, n_init=10, random_state=42)\n",
    "kmeans_good.fit(X)\n",
    "kmeans_bad.fit(X)\n",
    "\n",
    "plt.figure(figsize=(10, 3.2))\n",
    "\n",
    "plt.subplot(121)\n",
    "plot_decision_boundaries(kmeans_good, X)\n",
    "plt.title(f\"Inertia = {kmeans_good.inertia_:.1f}\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plot_decision_boundaries(kmeans_bad, X, show_ylabels=False)\n",
    "plt.title(f\"Inertia = {kmeans_bad.inertia_:.1f}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uso de la agrupación para la segmentación de imágenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descarga la imagen de la mariquita:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "homl3_root = \"https://github.com/ageron/handson-ml3/raw/main/\"\n",
    "filename = \"ladybug.png\"\n",
    "directory = Path('img')\n",
    "filepath = directory / filename\n",
    "\n",
    "if not filepath.is_file():\n",
    "    print(\"Downloading\", filename)\n",
    "    url = f\"{homl3_root}/images/unsupervised_learning/{filename}\"\n",
    "    directory.mkdir(parents=True, exist_ok=True)  # Asegura que el directorio existe\n",
    "    urllib.request.urlretrieve(url, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "\n",
    "image = np.asarray(PIL.Image.open(filepath))\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = image.reshape(-1, 3)\n",
    "kmeans = KMeans(n_clusters=8, n_init=10, random_state=42).fit(X)\n",
    "segmented_img = kmeans.cluster_centers_[kmeans.labels_]\n",
    "segmented_img = segmented_img.reshape(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "segmented_imgs = []\n",
    "n_colors = (10, 8, 6, 4, 2)\n",
    "for n_clusters in n_colors:\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42).fit(X)\n",
    "    segmented_img = kmeans.cluster_centers_[kmeans.labels_]\n",
    "    segmented_imgs.append(segmented_img.reshape(image.shape))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplots_adjust(wspace=0.05, hspace=0.1)\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.imshow(image)\n",
    "plt.title(\"Original image\")\n",
    "plt.axis('off')\n",
    "\n",
    "for idx, n_clusters in enumerate(n_colors):\n",
    "    plt.subplot(2, 3, 2 + idx)\n",
    "    plt.imshow(segmented_imgs[idx] / 255)\n",
    "    plt.title(f\"{n_clusters} colors\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uso de la agrupación para el aprendizaje semisupervisado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otro caso de uso de la agrupación es el aprendizaje semisupervisado, cuando tenemos muchas instancias sin etiquetar y muy pocas etiquetadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a abordar el conjunto de datos _digits_, que es un conjunto de datos sencillo similar al MNIST que contiene 1.797 imágenes de 8×8 en escala de grises que representan los dígitos del 0 al 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "X_digits, y_digits = load_digits(return_X_y=True)\n",
    "X_train, y_train = X_digits[:1400], y_digits[:1400]\n",
    "X_test, y_test = X_digits[1400:], y_digits[1400:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos el rendimiento de un modelo de regresión logística cuando sólo tenemos 50 instancias etiquetadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "n_labeled = 50\n",
    "log_reg = LogisticRegression(max_iter=10_000)\n",
    "log_reg.fit(X_train[:n_labeled], y_train[:n_labeled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# medir la precisión cuando utilizamos todo el conjunto de entrenamiento\n",
    "log_reg_full = LogisticRegression(max_iter=10_000)\n",
    "log_reg_full.fit(X_train, y_train)\n",
    "log_reg_full.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es mucho menos que antes, por supuesto. Veamos cómo podemos hacerlo mejor. En primer lugar, agrupemos el conjunto de entrenamiento en 50 clusters y, a continuación, busquemos en cada cluster la imagen más cercana al centroide. A estas imágenes las llamaremos imágenes representativas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 50\n",
    "kmeans = KMeans(n_clusters=k, n_init=10, random_state=42)\n",
    "X_digits_dist = kmeans.fit_transform(X_train)\n",
    "representative_digit_idx = X_digits_dist.argmin(axis=0)\n",
    "X_representative_digits = X_train[representative_digit_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a trazar estas imágenes representativas y a etiquetarlas manualmente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(8, 2))\n",
    "for index, X_representative_digit in enumerate(X_representative_digits):\n",
    "    plt.subplot(k // 10, 10, index + 1)\n",
    "    plt.imshow(X_representative_digit.reshape(8, 8), cmap=\"binary\",\n",
    "               interpolation=\"bilinear\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_representative_digits = np.array([\n",
    "    1, 3, 6, 0, 7, 9, 2, 4, 8, 9,\n",
    "    5, 4, 7, 1, 2, 6, 1, 2, 5, 1,\n",
    "    4, 1, 3, 3, 8, 8, 2, 5, 6, 9,\n",
    "    1, 4, 0, 6, 8, 3, 4, 6, 7, 2,\n",
    "    4, 1, 0, 7, 5, 1, 9, 9, 3, 7\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora tenemos un conjunto de datos con sólo 50 instancias etiquetadas, pero en lugar de ser instancias completamente aleatorias, cada una de ellas es una imagen representativa de su clúster. Veamos si el rendimiento es mejor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(max_iter=10_000)\n",
    "log_reg.fit(X_representative_digits, y_representative_digits)\n",
    "log_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vaya. Hemos pasado de una precisión del 74,8% al 84,9%, aunque todavía sólo estamos entrenando el modelo con 50 instancias. Dado que a menudo es costoso y doloroso etiquetar instancias, especialmente cuando lo tienen que hacer manualmente expertos, es una buena idea hacer que etiqueten instancias representativas en lugar de instancias aleatorias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero quizá podamos ir un paso más allá: ¿y si propagamos las etiquetas a todas las demás instancias del mismo clúster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_propagated = np.empty(len(X_train), dtype=np.int64)\n",
    "for i in range(k):\n",
    "    y_train_propagated[kmeans.labels_ == i] = y_representative_digits[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(max_iter=10_000)\n",
    "log_reg.fit(X_train, y_train_propagated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos obtenido otro aumento significativo de la precisión. Veamos si podemos hacerlo aún mejor ignorando el 1% de instancias que están más lejos del centro de su clúster: esto debería eliminar algunos valores atípicos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_closest = 99\n",
    "\n",
    "X_cluster_dist = X_digits_dist[np.arange(len(X_train)), kmeans.labels_]\n",
    "for i in range(k):\n",
    "    in_cluster = (kmeans.labels_ == i)\n",
    "    cluster_dist = X_cluster_dist[in_cluster]\n",
    "    cutoff_distance = np.percentile(cluster_dist, percentile_closest)\n",
    "    above_cutoff = (X_cluster_dist > cutoff_distance)\n",
    "    X_cluster_dist[in_cluster & above_cutoff] = -1\n",
    "\n",
    "partially_propagated = (X_cluster_dist != -1)\n",
    "X_train_partially_propagated = X_train[partially_propagated]\n",
    "y_train_partially_propagated = y_train_propagated[partially_propagated]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(max_iter=10_000)\n",
    "log_reg.fit(X_train_partially_propagated, y_train_partially_propagated)\n",
    "log_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vaya, ¡otro aumento de la precisión! Incluso hemos superado ligeramente el rendimiento que obtuvimos entrenando con el conjunto de entrenamiento totalmente etiquetado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestras etiquetas propagadas son bastante buenas: su precisión ronda el 97,6%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_train_partially_propagated == y_train[partially_propagated]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podrías hacer algunas iteraciones de *aprendizaje activo*:\n",
    "1. Etiquetar manualmente las instancias de las que el clasificador está menos seguro, si es posible, agrupándolas en clusters distintos.\n",
    "2. Entrenar un nuevo modelo con estas etiquetas adicionales."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

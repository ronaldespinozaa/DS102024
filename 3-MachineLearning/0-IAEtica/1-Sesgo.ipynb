{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81ad362a",
   "metadata": {
    "papermill": {
     "duration": 0.003092,
     "end_time": "2023-04-20T17:46:24.680246",
     "exception": false,
     "start_time": "2023-04-20T17:46:24.677154",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Introducción\n",
    "\n",
    "El aprendizaje automático (AM) tiene el potencial de mejorar vidas, pero también puede ser una fuente de perjuicios. Las aplicaciones de ML han discriminado a personas por motivos de raza, sexo, religión, estatus socioeconómico y otras categorías.\n",
    "\n",
    "En este notebook, aprenderás sobre el **bias**, que se refiere a las consecuencias negativas y no deseadas de las aplicaciones de ML, especialmente si las consecuencias afectan desproporcionadamente a ciertos grupos.\n",
    "\n",
    "Cubriremos seis tipos diferentes de sesgo que pueden afectar a cualquier aplicación de ML.\n",
    "\n",
    "# El sesgo es complejo\n",
    "\n",
    "Muchos profesionales del ML están familiarizados con los \"datos sesgados\" y el concepto de \"basura dentro, basura fuera\". Por ejemplo, si estás entrenando a un chatbot utilizando un conjunto de datos que contiene conversaciones antisemitas en línea (\"garbage in\"), es probable que el chatbot haga comentarios antisemitas (\"garbage out\").  Este ejemplo detalla un importante tipo de sesgo (llamado **sesgo histórico**, como se verá más adelante) que debe reconocerse y abordarse.\n",
    "\n",
    "Esta no es la única forma en que los prejuicios pueden arruinar las aplicaciones de ML.\n",
    "\n",
    "El sesgo en los datos es complejo.  Los datos defectuosos también pueden dar lugar a un **sesgo de representación**, si un grupo está infrarrepresentado en los datos de entrenamiento.  Por ejemplo, al entrenar un sistema de detección facial, si los datos de entrenamiento contienen mayoritariamente individuos con tonos de piel más claros, no funcionará bien para usuarios con tonos de piel más oscuros.  Un tercer tipo de sesgo que puede surgir de los datos de entrenamiento se denomina **sesgo de medición**, sobre el que aprenderás más adelante.\n",
    "\n",
    "Y no son sólo los datos sesgados los que pueden dar lugar a aplicaciones de ML injustas, el sesgo también puede resultar de la forma en que se define el modelo de ML, de la forma en que el modelo se compara con otros modelos y de la forma en que los usuarios cotidianos interpretan los resultados finales del modelo.  El daño puede venir de cualquier parte del proceso de ML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97407de9",
   "metadata": {
    "papermill": {
     "duration": 0.001593,
     "end_time": "2023-04-20T17:46:24.684060",
     "exception": false,
     "start_time": "2023-04-20T17:46:24.682467",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "1. Sesgo Histórico\n",
    "    \n",
    "    Definición: Ocurre cuando los datos reflejan un estado del mundo que ya está sesgado o es incorrecto.\n",
    "\n",
    "    Ejemplo: En 2020, solo el 7.4% de los CEO de Fortune 500 eran mujeres, a pesar de que investigaciones muestran que las empresas lideradas por mujeres suelen ser más rentables. Si entrenamos un modelo con estos datos sesgados, replicará las mismas desigualdades en futuras contrataciones.\n",
    "\n",
    "2. Sesgo de Representación\n",
    "\n",
    "    Definición: Se produce cuando los datos no representan adecuadamente a todos los grupos a los que el modelo servirá.\n",
    "\n",
    "    Ejemplo: Los datos recolectados mediante aplicaciones de smartphones tienden a excluir a personas mayores de 65 años, quienes suelen usar menos esta tecnología. Si se usa este conjunto de datos para diseñar un sistema de transporte, podría no satisfacer las necesidades de este grupo etario.\n",
    "\n",
    "3. Sesgo de Medición\n",
    "\n",
    "    Definición: Ocurre cuando la precisión de los datos varía entre diferentes grupos.\n",
    "\n",
    "    Ejemplo: Un hospital utiliza un modelo para predecir riesgos de salud basado en costos médicos. Sin embargo, los pacientes negros suelen tener menores costos debido a barreras en el acceso a la atención. Como resultado, el modelo podría subestimar los riesgos de salud en este grupo.\n",
    "\n",
    "4. Sesgo de Agregación\n",
    "\n",
    "    Definición: Aparece cuando se combinan datos de diferentes grupos de manera inadecuada, lo que puede hacer que el modelo funcione bien solo para el grupo mayoritario.\n",
    "\n",
    "    Ejemplo: Si se construye una IA para diagnosticar diabetes sin considerar que los hispanos tienen tasas más altas de complicaciones relacionadas con la diabetes, el modelo podría no ser tan efectivo para este grupo.\n",
    "\n",
    "5. Sesgo de Evaluación\n",
    "\n",
    "    Definición: Se presenta cuando los datos utilizados para evaluar un modelo no representan correctamente a la población objetivo.\n",
    "\n",
    "    Ejemplo: Estudios han demostrado que los conjuntos de datos de referencia para análisis facial contienen principalmente personas de piel clara. Como resultado, las IA desarrolladas para clasificar el género funcionan peor en personas de color.\n",
    "\n",
    "6. Sesgo de Despliegue\n",
    "\n",
    "    Definición: Sucede cuando el modelo se usa de una manera diferente a la prevista originalmente, lo que puede llevar a resultados inesperados o inexactos.\n",
    "    \n",
    "    Ejemplo: En el sistema judicial, se utilizan herramientas para predecir la reincidencia de criminales. Sin embargo, estas predicciones no deberían usarse para decidir sentencias, ya que no están diseñadas para ese propósito."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3145dcf",
   "metadata": {
    "papermill": {
     "duration": 0.001586,
     "end_time": "2023-04-20T17:46:24.687523",
     "exception": false,
     "start_time": "2023-04-20T17:46:24.685937",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Podemos representar visualmente estos diferentes tipos de sesgo, que se producen en diferentes etapas del flujo de trabajo de ML:\n",
    "\n",
    "[![representación visual de los tipos de sesgo](https://storage.googleapis.com/kaggle-media/learn/images/bvArGRY.png)](https://arxiv.org/pdf/1901.10002.pdf)\n",
    "\n",
    "Nótese que *no son mutuamente excluyentes*: es decir, una aplicación de ML puede sufrir fácilmente más de un tipo de sesgo.  Por ejemplo, las aplicaciones de ML en dispositivos de fitness portátiles pueden sufrir de:\n",
    "\n",
    "- **Sesgo de representación** (si el conjunto de datos utilizado para entrenar los modelos excluye los tonos de piel más oscuros),\n",
    "\n",
    "- **Sesgo de medición** (si el aparato de medición muestra un rendimiento reducido con tonos de piel oscuros), y\n",
    "\n",
    "- **Sesgo de evaluación** (si el conjunto de datos utilizado para evaluar el modelo excluye los tonos de piel oscuros)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11.620416,
   "end_time": "2023-04-20T17:46:25.318551",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-04-20T17:46:13.698135",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

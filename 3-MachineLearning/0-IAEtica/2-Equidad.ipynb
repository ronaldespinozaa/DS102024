{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7352597",
   "metadata": {
    "papermill": {
     "duration": 0.003216,
     "end_time": "2023-04-20T17:46:32.317698",
     "exception": false,
     "start_time": "2023-04-20T17:46:32.314482",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Introducción\n",
    "\n",
    "Hay muchas formas diferentes de definir lo que podríamos buscar en un modelo de aprendizaje automático (ML) justo.  Por ejemplo, supongamos que estamos trabajando con un modelo que aprueba (o deniega) solicitudes de tarjetas de crédito.  ¿Es\n",
    "- justo si la tasa de aprobación es igual para todos los sexos, o es\n",
    "- mejor si el género se elimina del conjunto de datos y se oculta del modelo?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7293b97f",
   "metadata": {
    "papermill": {
     "duration": 0.001731,
     "end_time": "2023-04-20T17:46:32.321775",
     "exception": false,
     "start_time": "2023-04-20T17:46:32.320044",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Cuatro Criterios de Equidad en Aprendizaje Automático\n",
    "\n",
    "1. **Paridad Demográfica (o Estadística)**\n",
    "\n",
    "    Definición: Un modelo cumple con la paridad demográfica si selecciona a personas en proporción a la composición de los grupos en la población original de solicitantes.\n",
    "\n",
    "    Ejemplo: Si una conferencia internacional recibe 20,000 inscripciones y el 50% de los asistentes son mujeres, un modelo que selecciona 100 posibles oradores debe incluir a 50 mujeres para cumplir con este criterio.\n",
    "\n",
    "2. **Igualdad de Oportunidades**\n",
    "\n",
    "    Definición: Este criterio garantiza que la tasa de verdaderos positivos (TPR), o la sensibilidad del modelo, sea la misma para cada grupo. Esto significa que el modelo debe ser igualmente efectivo en identificar correctamente a las personas que deberían ser seleccionadas en cada grupo.\n",
    "\n",
    "    Ejemplo: Un médico usa una herramienta para identificar pacientes que necesitan cuidados adicionales. La herramienta debe ser igual de efectiva para todos los grupos demográficos para asegurar que todos reciban las mismas oportunidades de ser identificados correctamente.\n",
    "\n",
    "3. **Igual Precisión**\n",
    "\n",
    "    Definición: Un modelo tiene igual precisión si el porcentaje de decisiones correctas es el mismo para cada grupo. Esto significa que el modelo no favorece a un grupo sobre otro en términos de precisión en sus predicciones.\n",
    "\n",
    "    Ejemplo: Un banco utiliza un modelo para aprobar préstamos. Si el modelo es 98% preciso en sus decisiones para un grupo, debe mantener ese nivel de precisión para todos los grupos, evitando así decisiones incorrectas que perjudiquen tanto al banco como a los solicitantes.\n",
    "\n",
    "4. **Imparcialidad Sin Conocimiento de Grupo (\"Equidad a través del Desconocimiento\")**\n",
    "    Definición: Este criterio implica eliminar toda la información relacionada con la pertenencia a un grupo, como género, raza o edad, del conjunto de datos para intentar que el modelo sea neutral.\n",
    "    \n",
    "    Ejemplo: Si eliminamos los datos de raza de un conjunto de datos, debemos asegurarnos de también eliminar cualquier otro dato que pueda inferir la raza, como el código postal en ciudades con segregación racial. De lo contrario, el modelo podría seguir siendo sesgado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8639ad",
   "metadata": {
    "papermill": {
     "duration": 0.001642,
     "end_time": "2023-04-20T17:46:32.325359",
     "exception": false,
     "start_time": "2023-04-20T17:46:32.323717",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Ejemplo\n",
    "\n",
    "Trabajaremos con un pequeño ejemplo para ilustrar las diferencias entre los cuatro tipos de equidad.  Utilizaremos una **matriz de confusión**, que es una herramienta común utilizada para entender el rendimiento de un modelo ML.  Esta herramienta se representa en el siguiente ejemplo, que muestra un modelo con una precisión del 80% (ya que 8/10 personas se clasificaron correctamente) y tiene una tasa de verdaderos positivos del 83% (ya que 5/6 \"positivos\" se clasificaron correctamente).\n",
    "\n",
    "![](https://storage.googleapis.com/kaggle-media/learn/images/xFZG5fF.png)\n",
    "\n",
    "Para entender cómo varía el rendimiento de un modelo entre grupos, podemos construir una matriz de confusión diferente para cada grupo. En este pequeño ejemplo, supondremos que tenemos datos de sólo 20 personas, divididos a partes iguales entre dos grupos (10 del Grupo A y 10 del Grupo B).  \n",
    "\n",
    "La siguiente imagen muestra cómo podrían ser las matrices de confusión, si el modelo satisface la equidad de **paridad demográfica**. El modelo tiene en cuenta a 10 personas de cada grupo (50% del grupo A y 50% del grupo B). 14 personas, también repartidas a partes iguales entre los grupos (50% del Grupo A y 50% del Grupo B) fueron aprobadas por el modelo.\n",
    "\n",
    "![](https://storage.googleapis.com/kaggle-media/learn/images/e32gcDh.png)\n",
    "\n",
    "Para que **igualdad de oportunidades** sea justo, el TPR de cada grupo debe ser el mismo; en el ejemplo siguiente, es del 66% en cada caso.\n",
    "\n",
    "![](https://storage.googleapis.com/kaggle-media/learn/images/aInWboA.png)\n",
    "\n",
    "A continuación, podemos ver cómo serían las matrices de confusión para **equal accuracy** fairness. Para cada grupo, el modelo tuvo una precisión del 80%.\n",
    "\n",
    "![](https://storage.googleapis.com/kaggle-media/learn/images/fIOJovc.png)\n",
    "\n",
    "Obsérvese que **group unaware** fairness no puede detectarse a partir de la matriz de confusión, y se refiere más bien a la eliminación de la información de pertenencia a un grupo del conjunto de datos.\n",
    "\n",
    "Cuando se trabaja con un proyecto real, los datos serán mucho, mucho mayores.  En este caso, las matrices de confusión siguen siendo una herramienta útil para analizar el rendimiento del modelo.  Sin embargo, hay que tener en cuenta que no se puede esperar que los modelos del mundo real satisfagan a la perfección ninguna definición de equidad.  Por ejemplo, si se elige la \"paridad demográfica\" como criterio de equidad y el objetivo es que el modelo seleccione un 50% de hombres, puede ocurrir que el modelo final seleccione un porcentaje cercano, pero no exacto, al 50% (como un 48% o un 53%)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10.631669,
   "end_time": "2023-04-20T17:46:32.958095",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-04-20T17:46:22.326426",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

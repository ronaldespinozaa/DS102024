{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e84c3c71",
   "metadata": {},
   "source": [
    "# **Árboles de Decisión**\n",
    "\n",
    "**Resumen:** En este capítulo veremos cómo entrenar y visualizar árboles de decisión para clasificación y regresión, así como explorar algunos aspectos avanzados como la sensibilidad a la orientación de los ejes y la varianza alta de estos modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe51006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a2eed1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "IMAGES_PATH = Path() / \"img\"\n",
    "IMAGES_PATH.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106dbece",
   "metadata": {},
   "source": [
    "# Entrenamiento y visualización de un Árbol de Decisión\n",
    "\n",
    "En esta sección entrenamos un árbol de decisión para clasificar el conjunto de datos Iris utilizando únicamente dos características (la longitud y anchura del pétalo) y lo visualizamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275aed78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Cargamos el dataset Iris en formato DataFrame (as_frame=True)\n",
    "iris = load_iris(as_frame=True)\n",
    "# Seleccionamos dos características para simplificar la visualización:\n",
    "X_iris = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\n",
    "y_iris = iris.target\n",
    "\n",
    "# Creamos y entrenamos un árbol de decisión con profundidad máxima 2\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "tree_clf.fit(X_iris, y_iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9410beaa",
   "metadata": {},
   "source": [
    "**Árbol de Decisión para el dataset Iris.**\n",
    "\n",
    "**Resumen:** Se entrena un árbol simple que utiliza dos características para clasificar las flores del dataset Iris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e349b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "export_graphviz(\n",
    "    tree_clf,\n",
    "    out_file=str(IMAGES_PATH / \"iris_tree.dot\"),  # La ruta puede variar respecto al libro\n",
    "    feature_names=[\"petal length (cm)\", \"petal width (cm)\"],\n",
    "    class_names=iris.target_names,\n",
    "    rounded=True,\n",
    "    filled=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd22509b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Source\n",
    "\n",
    "# Visualizamos el archivo .dot generado para ver la estructura del árbol\n",
    "Source.from_file(IMAGES_PATH / \"iris_tree.dot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee22d05",
   "metadata": {},
   "source": [
    "Graphviz también provee la herramienta de línea de comandos `dot` para convertir archivos `.dot` a diversos formatos. El siguiente comando convierte el archivo .dot en una imagen PNG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbf0512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Código extra: conversión del archivo .dot a PNG\n",
    "!dot -Tpng {IMAGES_PATH / \"iris_tree.dot\"} -o {IMAGES_PATH / \"iris_tree.png\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addcddeb",
   "metadata": {},
   "source": [
    "# Realizando Predicciones\n",
    "\n",
    "En esta parte se visualizan las fronteras de decisión del árbol entrenado y se muestra cómo predecir la clase para nuevas instancias.\n",
    "\n",
    "**Resumen:** Se crea una malla de puntos en el espacio de características, se predicen las clases con el árbol y se grafican las fronteras junto con los datos originales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911b5b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#  detalles de formateo para la visualización:\n",
    "from matplotlib.colors import ListedColormap\n",
    "custom_cmap = ListedColormap(['#fafab0', '#9898ff', '#a0faa0'])\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "# Creamos una malla de puntos en el rango adecuado para las características\n",
    "lengths, widths = np.meshgrid(np.linspace(0, 7.2, 100), np.linspace(0, 3, 100))\n",
    "X_iris_all = np.c_[lengths.ravel(), widths.ravel()]\n",
    "# Predecimos la clase para cada punto de la malla\n",
    "y_pred = tree_clf.predict(X_iris_all).reshape(lengths.shape)\n",
    "plt.contourf(lengths, widths, y_pred, alpha=0.3, cmap=custom_cmap)\n",
    "\n",
    "# Graficamos los datos originales\n",
    "for idx, (name, style) in enumerate(zip(iris.target_names, (\"yo\", \"bs\", \"g^\"))):\n",
    "    plt.plot(X_iris[:, 0][y_iris == idx], X_iris[:, 1][y_iris == idx],\n",
    "             style, label=f\"Iris {name}\")\n",
    "\n",
    "#  se entrena un árbol más profundo (max_depth=3) para mostrar fronteras adicionales\n",
    "tree_clf_deeper = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "tree_clf_deeper.fit(X_iris, y_iris)\n",
    "# Obtenemos algunos umbrales del árbol para dibujar líneas que indiquen las divisiones\n",
    "th0, th1, th2a, th2b = tree_clf_deeper.tree_.threshold[[0, 2, 3, 6]]\n",
    "plt.xlabel(\"Longitud del pétalo (cm)\")\n",
    "plt.ylabel(\"Anchura del pétalo (cm)\")\n",
    "plt.plot([th0, th0], [0, 3], \"k-\", linewidth=2)\n",
    "plt.plot([th0, 7.2], [th1, th1], \"k--\", linewidth=2)\n",
    "plt.plot([th2a, th2a], [0, th1], \"k:\", linewidth=2)\n",
    "plt.plot([th2b, th2b], [th1, 3], \"k:\", linewidth=2)\n",
    "plt.text(th0 - 0.05, 1.0, \"Profundidad=0\", horizontalalignment=\"right\", fontsize=15)\n",
    "plt.text(3.2, th1 + 0.02, \"Profundidad=1\", verticalalignment=\"bottom\", fontsize=13)\n",
    "plt.text(th2a + 0.05, 0.5, \"(Profundidad=2)\", fontsize=11)\n",
    "plt.axis([0, 7.2, 0, 3])\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc8b76a",
   "metadata": {},
   "source": [
    "Puedes acceder a la estructura interna del árbol a través del atributo `tree_`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a056a4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf.tree_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db9b6e3",
   "metadata": {},
   "source": [
    "Para obtener más información, consulta la documentación de la clase:\n",
    "\n",
    "```python\n",
    "help(sklearn.tree._tree.Tree)\n",
    "```\n",
    "\n",
    "También puedes revisar el material extra que aparece a continuación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8648687d",
   "metadata": {},
   "source": [
    "# Estimación de Probabilidades de Clase\n",
    "\n",
    "Los árboles de decisión también pueden predecir las probabilidades de que una instancia pertenezca a cada clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e58954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predice las probabilidades para la instancia [5, 1.5]\n",
    "tree_clf.predict_proba([[5, 1.5]]).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c7b7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predice la clase para la misma instancia\n",
    "tree_clf.predict([[5, 1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea56b92",
   "metadata": {},
   "source": [
    "# Hiperparámetros de Regularización\n",
    "\n",
    "En esta sección se explora el efecto de ciertos hiperparámetros (como `min_samples_leaf`) sobre el comportamiento del árbol. Utilizaremos el conjunto de datos _moons_ para este ejemplo.\n",
    "\n",
    "**Resumen:** Se comparan árboles sin restricciones y con restricciones (mínimo número de muestras por hoja) para ver cómo afecta la complejidad del modelo y la precisión en datos de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcac069",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X_moons, y_moons = make_moons(n_samples=150, noise=0.2, random_state=42)\n",
    "\n",
    "# Árbol sin restricciones\n",
    "tree_clf1 = DecisionTreeClassifier(random_state=42)\n",
    "# Árbol con restricción: mínimo 5 muestras por hoja\n",
    "tree_clf2 = DecisionTreeClassifier(min_samples_leaf=5, random_state=42)\n",
    "tree_clf1.fit(X_moons, y_moons)\n",
    "tree_clf2.fit(X_moons, y_moons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41862aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# función para graficar las fronteras de decisión\n",
    "def plot_decision_boundary(clf, X, y, axes, cmap):\n",
    "    \"\"\"\n",
    "    Dibuja las fronteras de decisión de un clasificador.\n",
    "    \n",
    "    Parámetros:\n",
    "      - clf: el clasificador entrenado.\n",
    "      - X, y: datos de entrada y etiquetas.\n",
    "      - axes: límites del gráfico.\n",
    "      - cmap: mapa de colores a utilizar.\n",
    "    \"\"\"\n",
    "    x1, x2 = np.meshgrid(np.linspace(axes[0], axes[1], 100),\n",
    "                         np.linspace(axes[2], axes[3], 100))\n",
    "    X_new = np.c_[x1.ravel(), x2.ravel()]\n",
    "    y_pred = clf.predict(X_new).reshape(x1.shape)\n",
    "    \n",
    "    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=cmap)\n",
    "    plt.contour(x1, x2, y_pred, cmap=\"Greys\", alpha=0.8)\n",
    "    colors = {\"Wistia\": [\"#78785c\", \"#c47b27\"], \"Pastel1\": [\"red\", \"blue\"]}\n",
    "    markers = (\"o\", \"^\")\n",
    "    for idx in (0, 1):\n",
    "        plt.plot(X[:, 0][y == idx], X[:, 1][y == idx],\n",
    "                 color=colors[cmap][idx], marker=markers[idx], linestyle=\"none\")\n",
    "    plt.axis(axes)\n",
    "    plt.xlabel(r\"$x_1$\")\n",
    "    plt.ylabel(r\"$x_2$\", rotation=0)\n",
    "\n",
    "# Graficamos las fronteras de ambos modelos en subplots\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\n",
    "plt.sca(axes[0])\n",
    "plot_decision_boundary(tree_clf1, X_moons, y_moons,\n",
    "                       axes=[-1.5, 2.4, -1, 1.5], cmap=\"Wistia\")\n",
    "plt.title(\"Sin restricciones\")\n",
    "plt.sca(axes[1])\n",
    "plot_decision_boundary(tree_clf2, X_moons, y_moons,\n",
    "                       axes=[-1.5, 2.4, -1, 1.5], cmap=\"Wistia\")\n",
    "plt.title(f\"min_samples_leaf = {tree_clf2.min_samples_leaf}\")\n",
    "plt.ylabel(\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a16cb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos un conjunto de prueba para evaluar la precisión de ambos modelos\n",
    "X_moons_test, y_moons_test = make_moons(n_samples=1000, noise=0.2,\n",
    "                                        random_state=43)\n",
    "print(\"Precisión árbol sin restricciones:\", tree_clf1.score(X_moons_test, y_moons_test))\n",
    "print(\"Precisión árbol con min_samples_leaf=5:\", tree_clf2.score(X_moons_test, y_moons_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae78dee2",
   "metadata": {},
   "source": [
    "# Regresión con Árboles de Decisión\n",
    "\n",
    "Ahora veremos cómo utilizar árboles de decisión para problemas de regresión. Comenzamos preparando un conjunto de entrenamiento sencillo basado en una función cuadrática.\n",
    "\n",
    "**Resumen:** Se entrenan árboles de regresión con diferentes profundidades y se visualizan las predicciones junto con los puntos de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ae11b2",
   "metadata": {},
   "source": [
    "Vamos a preparar un conjunto de entrenamiento cuadrático sencillo:\n",
    "\n",
    "**Ejemplo de código:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759dbbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "np.random.seed(42)\n",
    "# Generamos 200 puntos en el intervalo aproximadamente [-0.5, 0.5]\n",
    "X_quad = np.random.rand(200, 1) - 0.5  # única característica de entrada\n",
    "# La variable objetivo es una función cuadrática con algo de ruido\n",
    "y_quad = X_quad ** 2 + 0.025 * np.random.randn(200, 1)\n",
    "\n",
    "# Entrenamos un árbol de regresión con profundidad máxima 2\n",
    "tree_reg = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg.fit(X_quad, y_quad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293a61c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  exportamos la estructura del árbol a un archivo .dot para visualizarlo\n",
    "export_graphviz(\n",
    "    tree_reg,\n",
    "    out_file=str(IMAGES_PATH / \"regression_tree.dot\"),\n",
    "    feature_names=[\"x1\"],\n",
    "    rounded=True,\n",
    "    filled=True\n",
    ")\n",
    "Source.from_file(IMAGES_PATH / \"regression_tree.dot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e2b702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamos otro árbol de regresión con una mayor profundidad (max_depth=3)\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=3, random_state=42)\n",
    "tree_reg2.fit(X_quad, y_quad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ebd084",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Mostramos los umbrales de los nodos en cada árbol para comparar\n",
    "print(\"Umbrales árbol (max_depth=2):\", tree_reg.tree_.threshold)\n",
    "print(\"Umbrales árbol (max_depth=3):\", tree_reg2.tree_.threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca11d05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  función para graficar las predicciones de un árbol de regresión\n",
    "def plot_regression_predictions(tree_reg, X, y, axes=[-0.5, 0.5, -0.05, 0.25]):\n",
    "    \"\"\"\n",
    "    Dibuja las predicciones de un árbol de regresión junto con los datos de entrenamiento.\n",
    "    \"\"\"\n",
    "    x1 = np.linspace(axes[0], axes[1], 500).reshape(-1, 1)\n",
    "    y_pred = tree_reg.predict(x1)\n",
    "    plt.axis(axes)\n",
    "    plt.xlabel(\"$x_1$\")\n",
    "    plt.plot(X, y, \"b.\")\n",
    "    plt.plot(x1, y_pred, \"r.-\", linewidth=2, label=r\"$\\hat{y}$\")\n",
    "\n",
    "# Se crean dos subplots para comparar los árboles con diferentes profundidades\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\n",
    "plt.sca(axes[0])\n",
    "plot_regression_predictions(tree_reg, X_quad, y_quad)\n",
    "\n",
    "# Dibujamos las líneas que indican los umbrales en cada nodo\n",
    "th0, th1a, th1b = tree_reg.tree_.threshold[[0, 1, 4]]\n",
    "for split, style in ((th0, \"k-\"), (th1a, \"k--\"), (th1b, \"k--\")):\n",
    "    plt.plot([split, split], [-0.05, 0.25], style, linewidth=2)\n",
    "plt.text(th0, 0.16, \"Profundidad=0\", fontsize=15)\n",
    "plt.text(th1a + 0.01, -0.01, \"Profundidad=1\", horizontalalignment=\"center\", fontsize=13)\n",
    "plt.text(th1b + 0.01, -0.01, \"Profundidad=1\", fontsize=13)\n",
    "plt.ylabel(\"$y$\", rotation=0)\n",
    "plt.legend(loc=\"upper center\", fontsize=16)\n",
    "plt.title(\"max_depth=2\")\n",
    "\n",
    "plt.sca(axes[1])\n",
    "th2s = tree_reg2.tree_.threshold[[2, 5, 9, 12]]\n",
    "plot_regression_predictions(tree_reg2, X_quad, y_quad)\n",
    "for split, style in ((th0, \"k-\"), (th1a, \"k--\"), (th1b, \"k--\")):\n",
    "    plt.plot([split, split], [-0.05, 0.25], style, linewidth=2)\n",
    "for split in th2s:\n",
    "    plt.plot([split, split], [-0.05, 0.25], \"k:\", linewidth=1)\n",
    "plt.text(th2s[2] + 0.01, 0.15, \"Profundidad=2\", fontsize=13)\n",
    "plt.title(\"max_depth=3\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3c36c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  exploración de la regularización en árboles de regresión\n",
    "tree_reg1 = DecisionTreeRegressor(random_state=42)\n",
    "tree_reg2 = DecisionTreeRegressor(random_state=42, min_samples_leaf=10)\n",
    "tree_reg1.fit(X_quad, y_quad)\n",
    "tree_reg2.fit(X_quad, y_quad)\n",
    "\n",
    "x1 = np.linspace(-0.5, 0.5, 500).reshape(-1, 1)\n",
    "y_pred1 = tree_reg1.predict(x1)\n",
    "y_pred2 = tree_reg2.predict(x1)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\n",
    "\n",
    "plt.sca(axes[0])\n",
    "plt.plot(X_quad, y_quad, \"b.\")\n",
    "plt.plot(x1, y_pred1, \"r.-\", linewidth=2, label=r\"$\\hat{y}$\")\n",
    "plt.axis([-0.5, 0.5, -0.05, 0.25])\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$y$\", rotation=0)\n",
    "plt.legend(loc=\"upper center\")\n",
    "plt.title(\"Sin restricciones\")\n",
    "\n",
    "plt.sca(axes[1])\n",
    "plt.plot(X_quad, y_quad, \"b.\")\n",
    "plt.plot(x1, y_pred2, \"r.-\", linewidth=2, label=r\"$\\hat{y}$\")\n",
    "plt.axis([-0.5, 0.5, -0.05, 0.25])\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.title(f\"min_samples_leaf={tree_reg2.min_samples_leaf}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9aed03",
   "metadata": {},
   "source": [
    "# Sensibilidad a la Orientación de los Ejes\n",
    "\n",
    "Rotar el conjunto de datos puede producir fronteras de decisión completamente diferentes.\n",
    "\n",
    "**Resumen:** Se ilustra cómo una rotación (en este caso, 45°) del conjunto de datos cambia la forma en que el árbol de decisión divide el espacio, demostrando que la orientación de los ejes puede influir notablemente en la partición generada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee33d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(6)\n",
    "# Generamos 100 puntos en un cuadrado centrado en el origen\n",
    "X_square = np.random.rand(100, 2) - 0.5\n",
    "# Definimos la etiqueta en función de si la primera característica es positiva\n",
    "y_square = (X_square[:, 0] > 0).astype(np.int64)\n",
    "\n",
    "# Rotamos el conjunto de datos 45° (π/4 radianes)\n",
    "angle = np.pi / 4  # 45 grados\n",
    "rotation_matrix = np.array([[np.cos(angle), -np.sin(angle)],\n",
    "                            [np.sin(angle), np.cos(angle)]])\n",
    "X_rotated_square = X_square.dot(rotation_matrix)\n",
    "\n",
    "# Entrenamos un árbol en el conjunto original y otro en el conjunto rotado\n",
    "tree_clf_square = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf_square.fit(X_square, y_square)\n",
    "tree_clf_rotated_square = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf_rotated_square.fit(X_rotated_square, y_square)\n",
    "\n",
    "# Graficamos las fronteras de decisión para ambos casos\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\n",
    "plt.sca(axes[0])\n",
    "plot_decision_boundary(tree_clf_square, X_square, y_square,\n",
    "                       axes=[-0.7, 0.7, -0.7, 0.7], cmap=\"Pastel1\")\n",
    "plt.sca(axes[1])\n",
    "plot_decision_boundary(tree_clf_rotated_square, X_rotated_square, y_square,\n",
    "                       axes=[-0.7, 0.7, -0.7, 0.7], cmap=\"Pastel1\")\n",
    "plt.ylabel(\"\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaf229e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Aplicamos PCA para ver la influencia de la rotación en un espacio de componentes principales\n",
    "pca_pipeline = make_pipeline(StandardScaler(), PCA())\n",
    "X_iris_rotated = pca_pipeline.fit_transform(X_iris)\n",
    "tree_clf_pca = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "tree_clf_pca.fit(X_iris_rotated, y_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b15dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "axes = [-2.2, 2.4, -0.6, 0.7]\n",
    "z0s, z1s = np.meshgrid(np.linspace(axes[0], axes[1], 100),\n",
    "                       np.linspace(axes[2], axes[3], 100))\n",
    "X_iris_pca_all = np.c_[z0s.ravel(), z1s.ravel()]\n",
    "y_pred = tree_clf_pca.predict(X_iris_pca_all).reshape(z0s.shape)\n",
    "\n",
    "plt.contourf(z0s, z1s, y_pred, alpha=0.3, cmap=custom_cmap)\n",
    "for idx, (name, style) in enumerate(zip(iris.target_names, (\"yo\", \"bs\", \"g^\"))):\n",
    "    plt.plot(X_iris_rotated[:, 0][y_iris == idx],\n",
    "             X_iris_rotated[:, 1][y_iris == idx],\n",
    "             style, label=f\"Iris {name}\")\n",
    "\n",
    "plt.xlabel(\"$z_1$\")\n",
    "plt.ylabel(\"$z_2$\", rotation=0)\n",
    "# Dibujamos líneas en función de los umbrales del árbol en el espacio PCA\n",
    "th1, th2 = tree_clf_pca.tree_.threshold[[0, 2]]\n",
    "plt.plot([th1, th1], axes[2:], \"k-\", linewidth=2)\n",
    "plt.plot([th2, th2], axes[2:], \"k--\", linewidth=2)\n",
    "plt.text(th1 - 0.01, axes[2] + 0.05, \"Profundidad=0\",\n",
    "         horizontalalignment=\"right\", fontsize=15)\n",
    "plt.text(th2 - 0.01, axes[2] + 0.05, \"Profundidad=1\",\n",
    "         horizontalalignment=\"right\", fontsize=13)\n",
    "plt.axis(axes)\n",
    "plt.legend(loc=(0.32, 0.67))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069a57fd",
   "metadata": {},
   "source": [
    "# Los Árboles de Decisión Tienen Alta Varianza\n",
    "\n",
    "Hemos visto que pequeños cambios en el conjunto de datos (como una rotación) pueden producir un árbol muy distinto. Ahora demostraremos que entrenar el mismo modelo con los mismos datos puede generar modelos diferentes en cada ejecución, ya que el algoritmo CART (utilizado por Scikit-Learn) es estocástico.\n",
    "\n",
    "**Resumen:** Se entrena un árbol con un `random_state` distinto para evidenciar que la estructura del árbol puede variar y, por ende, las fronteras de decisión también."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599de288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamos un árbol con un valor distinto de random_state\n",
    "tree_clf_tweaked = DecisionTreeClassifier(max_depth=2, random_state=40)\n",
    "tree_clf_tweaked.fit(X_iris, y_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfb85f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "y_pred = tree_clf_tweaked.predict(X_iris_all).reshape(lengths.shape)\n",
    "plt.contourf(lengths, widths, y_pred, alpha=0.3, cmap=custom_cmap)\n",
    "\n",
    "for idx, (name, style) in enumerate(zip(iris.target_names, (\"yo\", \"bs\", \"g^\"))):\n",
    "    plt.plot(X_iris[:, 0][y_iris == idx], X_iris[:, 1][y_iris == idx],\n",
    "             style, label=f\"Iris {name}\")\n",
    "\n",
    "# Dibujamos líneas en función de los umbrales obtenidos\n",
    "th0, th1 = tree_clf_tweaked.tree_.threshold[[0, 2]]\n",
    "plt.plot([0, 7.2], [th0, th0], \"k-\", linewidth=2)\n",
    "plt.plot([0, 7.2], [th1, th1], \"k--\", linewidth=2)\n",
    "plt.text(1.8, th0 + 0.05, \"Profundidad=0\", verticalalignment=\"bottom\", fontsize=15)\n",
    "plt.text(2.3, th1 + 0.05, \"Profundidad=1\", verticalalignment=\"bottom\", fontsize=13)\n",
    "plt.xlabel(\"Longitud del pétalo (cm)\")\n",
    "plt.ylabel(\"Anchura del pétalo (cm)\")\n",
    "plt.axis([0, 7.2, 0, 3])\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0df5f54",
   "metadata": {},
   "source": [
    "# Material Extra – Accediendo a la Estructura del Árbol\n",
    "\n",
    "Un `DecisionTreeClassifier` entrenado posee un atributo `tree_` que almacena la estructura interna del árbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e59c41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = tree_clf.tree_\n",
    "tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49835dcf",
   "metadata": {},
   "source": [
    "Puedes obtener el número total de nodos del árbol:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3861e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.node_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7ccbe9",
   "metadata": {},
   "source": [
    "Además, se encuentran disponibles otros atributos autoexplicativos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8d7026",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Profundidad máxima:\", tree.max_depth)\n",
    "print(\"Número máximo de clases:\", tree.max_n_classes)\n",
    "print(\"Número de características:\", tree.n_features)\n",
    "print(\"Número de salidas:\", tree.n_outputs)\n",
    "print(\"Número de hojas:\", tree.n_leaves)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af21a2e5",
   "metadata": {},
   "source": [
    "Toda la información sobre los nodos se almacena en arrays de NumPy. Por ejemplo, la impureza de cada nodo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fe2d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4f54f6",
   "metadata": {},
   "source": [
    "El nodo raíz se encuentra en el índice 0. Los hijos izquierdo y derecho del nodo _i_ son `tree.children_left[i]` y `tree.children_right[i]`, respectivamente. Por ejemplo, los hijos del nodo raíz son:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae30caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.children_left[0], tree.children_right[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3d152d",
   "metadata": {},
   "source": [
    "Cuando los hijos izquierdo y derecho son iguales, significa que el nodo es hoja (los identificadores en este caso son arbitrarios):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a365181",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.children_left[3], tree.children_right[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b615ee4",
   "metadata": {},
   "source": [
    "Para obtener los identificadores de los nodos hoja:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f12fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_leaf = (tree.children_left == tree.children_right)\n",
    "np.arange(tree.node_count)[is_leaf]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bf1d73",
   "metadata": {},
   "source": [
    "Los nodos que no son hoja se denominan _nodos de división_. La característica que utilizan para dividir el espacio está en el array `feature` (para los nodos hoja, estos valores deben ignorarse):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fdf476",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eae43c3",
   "metadata": {},
   "source": [
    "Y los umbrales correspondientes son:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a336f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc0cbfd",
   "metadata": {},
   "source": [
    "También se encuentra disponible el número de instancias por clase que alcanzaron cada nodo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2bef23",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "print(\"Valor (conteo por clase) en cada nodo:\\n\", tree.value)\n",
    "print(\"Número de muestras en cada nodo:\\n\", tree.n_node_samples)\n",
    "print(\"Verificación (la suma de valores por nodo coincide con n_node_samples):\",\n",
    "      np.all(tree.value.sum(axis=(1, 2)) == tree.n_node_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc96e2b",
   "metadata": {},
   "source": [
    "**Calcular la profundidad de cada nodo:**  \n",
    "El siguiente código recorre el árbol (usando una pila) para asignar la profundidad a cada nodo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4893380e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_depth(tree_clf):\n",
    "    \"\"\"\n",
    "    Calcula la profundidad de cada nodo en un árbol de decisión entrenado.\n",
    "    \n",
    "    Devuelve:\n",
    "      - depth: un array con la profundidad de cada nodo.\n",
    "    \"\"\"\n",
    "    tree = tree_clf.tree_\n",
    "    depth = np.zeros(tree.node_count)\n",
    "    stack = [(0, 0)]  # (nodo, profundidad)\n",
    "    while stack:\n",
    "        node, node_depth = stack.pop()\n",
    "        depth[node] = node_depth\n",
    "        # Si el nodo no es hoja, añadimos sus hijos a la pila\n",
    "        if tree.children_left[node] != tree.children_right[node]:\n",
    "            stack.append((tree.children_left[node], node_depth + 1))\n",
    "            stack.append((tree.children_right[node], node_depth + 1))\n",
    "    return depth\n",
    "\n",
    "depth = compute_depth(tree_clf)\n",
    "print(\"Profundidad de cada nodo:\", depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a10db2",
   "metadata": {},
   "source": [
    "**Obtener los umbrales de todos los nodos de división a profundidad 1:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326bffb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_depth1 = tree_clf.tree_.feature[(depth == 1) & (~is_leaf)]\n",
    "thresholds_depth1 = tree_clf.tree_.threshold[(depth == 1) & (~is_leaf)]\n",
    "print(\"Características en nodos a profundidad 1:\", features_depth1)\n",
    "print(\"Umbrales en nodos a profundidad 1:\", thresholds_depth1)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b58ebfbf",
   "metadata": {},
   "source": [
    "# **Máquinas de Vectores de Soporte (SVM)**\n",
    "\n",
    "En este notebook exploramos, paso a paso, los conceptos y ejemplos prácticos de las Máquinas de Vectores de Soporte para clasificación y regresión. Se incluyen además ejercicios y soluciones para profundizar en el tema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5a167a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e4653c",
   "metadata": {},
   "source": [
    "# Clasificación con SVM Lineal\n",
    "\n",
    "En esta sección veremos cómo entrenar un SVM lineal para clasificación. El libro comienza mostrando unas figuras ilustrativas antes del primer ejemplo de código, por lo que las siguientes celdas generan y guardan dichas figuras. Puedes omitirlas si lo deseas, pero son útiles para entender el comportamiento del clasificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a789855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Código adicional – Esta celda genera y guarda la Figura 5–1.\n",
    "# \n",
    "# Explicación:\n",
    "# - Se carga el dataset Iris y se seleccionan las características \"longitud del pétalo\" y \"ancho del pétalo\".\n",
    "# - Se filtran únicamente las clases Iris setosa e Iris versicolor.\n",
    "# - Se entrena un clasificador SVM lineal con un valor muy alto de C para forzar el margen máximo.\n",
    "# - Se definen tres “modelos malos” (líneas de predicción arbitrarias) para compararlos visualmente.\n",
    "# - Se dibuja la frontera de decisión y los márgenes junto con los vectores de soporte.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import datasets\n",
    "\n",
    "# Cargamos el dataset Iris\n",
    "iris = datasets.load_iris(as_frame=True)\n",
    "# Seleccionamos las características \"longitud del pétalo\" y \"ancho del pétalo\"\n",
    "X = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\n",
    "y = iris.target\n",
    "\n",
    "# Filtramos para quedarnos solo con Iris setosa (0) e Iris versicolor (1)\n",
    "setosa_or_versicolor = (y == 0) | (y == 1)\n",
    "X = X[setosa_or_versicolor]\n",
    "y = y[setosa_or_versicolor]\n",
    "\n",
    "# Entrenamos un clasificador SVM lineal con C muy alto (efectivamente sin regularización)\n",
    "svm_clf = SVC(kernel=\"linear\", C=1e100)\n",
    "svm_clf.fit(X, y)\n",
    "\n",
    "# Modelos \"malos\" para comparación visual (líneas arbitrarias)\n",
    "x0 = np.linspace(0, 5.5, 200)\n",
    "pred_1 = 5 * x0 - 20\n",
    "pred_2 = x0 - 1.8\n",
    "pred_3 = 0.1 * x0 + 0.5\n",
    "\n",
    "def plot_svc_decision_boundary(svm_clf, xmin, xmax):\n",
    "    \"\"\"\n",
    "    Dibuja la frontera de decisión del clasificador SVM, junto con los márgenes y los vectores de soporte.\n",
    "    \"\"\"\n",
    "    w = svm_clf.coef_[0]\n",
    "    b = svm_clf.intercept_[0]\n",
    "\n",
    "    # En la frontera de decisión: w0*x0 + w1*x1 + b = 0  =>  x1 = -w0/w1 * x0 - b/w1\n",
    "    x0_vals = np.linspace(xmin, xmax, 200)\n",
    "    decision_boundary = -w[0] / w[1] * x0_vals - b / w[1]\n",
    "\n",
    "    # La distancia al margen es 1/|w1|\n",
    "    margin = 1 / abs(w[1])\n",
    "    gutter_up = decision_boundary + margin\n",
    "    gutter_down = decision_boundary - margin\n",
    "    svs = svm_clf.support_vectors_\n",
    "\n",
    "    plt.plot(x0_vals, decision_boundary, \"k-\", linewidth=2, zorder=-2)\n",
    "    plt.plot(x0_vals, gutter_up, \"k--\", linewidth=2, zorder=-2)\n",
    "    plt.plot(x0_vals, gutter_down, \"k--\", linewidth=2, zorder=-2)\n",
    "    plt.scatter(svs[:, 0], svs[:, 1], s=180, facecolors='#AAA', zorder=-1)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10, 2.7), sharey=True)\n",
    "\n",
    "# Primer subplot: muestra los modelos malos y los puntos\n",
    "plt.sca(axes[0])\n",
    "plt.plot(x0, pred_1, \"g--\", linewidth=2, label=\"Modelo 1\")\n",
    "plt.plot(x0, pred_2, \"m-\", linewidth=2, label=\"Modelo 2\")\n",
    "plt.plot(x0, pred_3, \"r-\", linewidth=2, label=\"Modelo 3\")\n",
    "plt.plot(X[:, 0][y == 1], X[:, 1][y == 1], \"bs\", label=\"Iris versicolor\")\n",
    "plt.plot(X[:, 0][y == 0], X[:, 1][y == 0], \"yo\", label=\"Iris setosa\")\n",
    "plt.xlabel(\"Longitud del pétalo\")\n",
    "plt.ylabel(\"Ancho del pétalo\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.axis([0, 5.5, 0, 2])\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.grid()\n",
    "\n",
    "# Segundo subplot: muestra la frontera de decisión del SVM y sus márgenes\n",
    "plt.sca(axes[1])\n",
    "plot_svc_decision_boundary(svm_clf, 0, 5.5)\n",
    "plt.plot(X[:, 0][y == 1], X[:, 1][y == 1], \"bs\")\n",
    "plt.plot(X[:, 0][y == 0], X[:, 1][y == 0], \"yo\")\n",
    "plt.xlabel(\"Longitud del pétalo\")\n",
    "plt.axis([0, 5.5, 0, 2])\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94daa04e",
   "metadata": {},
   "source": [
    "A continuación, generamos otra figura que muestra la sensibilidad de un SVM a la escala de las características."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ba2152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Código adicional – Esta celda genera y guarda la Figura 5–2.\n",
    "#\n",
    "# Explicación:\n",
    "# - Se crea un conjunto de datos pequeño (Xs, ys) con dos características de magnitudes muy distintas.\n",
    "# - Se entrena un SVM lineal tanto en los datos originales como en los escalados (usando StandardScaler).\n",
    "# - Se comparan las fronteras de decisión para ver el efecto de la escala de las características.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Datos de ejemplo con escalas muy diferentes\n",
    "Xs = np.array([[1, 50], [5, 20], [3, 80], [5, 60]], dtype=np.float64)\n",
    "ys = np.array([0, 0, 1, 1])\n",
    "svm_clf = SVC(kernel=\"linear\", C=100).fit(Xs, ys)\n",
    "\n",
    "# Escalamos los datos\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(Xs)\n",
    "svm_clf_scaled = SVC(kernel=\"linear\", C=100).fit(X_scaled, ys)\n",
    "\n",
    "plt.figure(figsize=(9, 2.7))\n",
    "\n",
    "# Primer subplot: datos sin escalar\n",
    "plt.subplot(121)\n",
    "plt.plot(Xs[:, 0][ys == 1], Xs[:, 1][ys == 1], \"bo\")\n",
    "plt.plot(Xs[:, 0][ys == 0], Xs[:, 1][ys == 0], \"ms\")\n",
    "plot_svc_decision_boundary(svm_clf, 0, 6)\n",
    "plt.xlabel(\"$x_0$\")\n",
    "plt.ylabel(\"$x_1$    \", rotation=0)\n",
    "plt.title(\"Sin escalar\")\n",
    "plt.axis([0, 6, 0, 90])\n",
    "plt.grid()\n",
    "\n",
    "# Segundo subplot: datos escalados\n",
    "plt.subplot(122)\n",
    "plt.plot(X_scaled[:, 0][ys == 1], X_scaled[:, 1][ys == 1], \"bo\")\n",
    "plt.plot(X_scaled[:, 0][ys == 0], X_scaled[:, 1][ys == 0], \"ms\")\n",
    "plot_svc_decision_boundary(svm_clf_scaled, -2, 2)\n",
    "plt.xlabel(\"$x'_0$\")\n",
    "plt.ylabel(\"$x'_1$  \", rotation=0)\n",
    "plt.title(\"Escalado\")\n",
    "plt.axis([-2, 2, -2, 2])\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2caedeed",
   "metadata": {},
   "source": [
    "## Clasificación con margen suave\n",
    "\n",
    "Ahora veremos cómo afectan los outliers (valores atípicos) al modelo SVM y cómo el uso de un margen suave permite cierta tolerancia en la clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de43ecfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Código adicional – Esta celda genera y guarda la Figura 5–3.\n",
    "#\n",
    "# Explicación:\n",
    "# - Se añaden outliers al conjunto original.\n",
    "# - Se crean dos conjuntos de datos: uno con el primer outlier y otro con el segundo.\n",
    "# - Se entrena un SVM con C muy alto para forzar una clasificación estricta.\n",
    "# - Se ilustran, en dos subplots, el efecto de los outliers en la separación y el margen.\n",
    "\n",
    "# Creamos algunos outliers\n",
    "X_outliers = np.array([[3.4, 1.3], [3.2, 0.8]])\n",
    "y_outliers = np.array([0, 0])\n",
    "Xo1 = np.concatenate([X, X_outliers[:1]], axis=0)\n",
    "yo1 = np.concatenate([y, y_outliers[:1]], axis=0)\n",
    "Xo2 = np.concatenate([X, X_outliers[1:]], axis=0)\n",
    "yo2 = np.concatenate([y, y_outliers[1:]], axis=0)\n",
    "\n",
    "# Entrenamos un SVM en el conjunto con el segundo outlier\n",
    "svm_clf2 = SVC(kernel=\"linear\", C=10**9)\n",
    "svm_clf2.fit(Xo2, yo2)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10, 2.7), sharey=True)\n",
    "\n",
    "# Primer subplot: muestra el conjunto con el primer outlier (la separación es imposible)\n",
    "plt.sca(axes[0])\n",
    "plt.plot(Xo1[:, 0][yo1 == 1], Xo1[:, 1][yo1 == 1], \"bs\")\n",
    "plt.plot(Xo1[:, 0][yo1 == 0], Xo1[:, 1][yo1 == 0], \"yo\")\n",
    "plt.text(0.3, 1.0, \"¡Imposible!\", color=\"red\", fontsize=18)\n",
    "plt.xlabel(\"Longitud del pétalo\")\n",
    "plt.ylabel(\"Ancho del pétalo\")\n",
    "plt.annotate(\n",
    "    \"Outlier\",\n",
    "    xy=(X_outliers[0][0], X_outliers[0][1]),\n",
    "    xytext=(2.5, 1.7),\n",
    "    ha=\"center\",\n",
    "    arrowprops=dict(facecolor='black', shrink=0.1),\n",
    ")\n",
    "plt.axis([0, 5.5, 0, 2])\n",
    "plt.grid()\n",
    "\n",
    "# Segundo subplot: muestra el conjunto con el segundo outlier y la frontera de decisión ajustada\n",
    "plt.sca(axes[1])\n",
    "plt.plot(Xo2[:, 0][yo2 == 1], Xo2[:, 1][yo2 == 1], \"bs\")\n",
    "plt.plot(Xo2[:, 0][yo2 == 0], Xo2[:, 1][yo2 == 0], \"yo\")\n",
    "plot_svc_decision_boundary(svm_clf2, 0, 5.5)\n",
    "plt.xlabel(\"Longitud del pétalo\")\n",
    "plt.annotate(\n",
    "    \"Outlier\",\n",
    "    xy=(X_outliers[1][0], X_outliers[1][1]),\n",
    "    xytext=(3.2, 0.08),\n",
    "    ha=\"center\",\n",
    "    arrowprops=dict(facecolor='black', shrink=0.1),\n",
    ")\n",
    "plt.axis([0, 5.5, 0, 2])\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da9a391",
   "metadata": {},
   "source": [
    "**Nota:**  \n",
    "El valor predeterminado del hiperparámetro `dual` en los estimadores `LinearSVC` y `LinearSVR` cambiará de `True` a `\"auto\"` en Scikit-Learn 1.4. Aquí se establece `dual=True` en todo el notebook para que la salida sea consistente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d66eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A continuación usamos el dataset Iris para clasificar la clase Iris virginica (positiva) contra las demás (negativa).\n",
    "# Se utiliza un pipeline que incluye escalado y el clasificador LinearSVC.\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "iris = load_iris(as_frame=True)\n",
    "X = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\n",
    "# Definimos la etiqueta: True para Iris virginica, False para las otras\n",
    "y = (iris.target == 2)\n",
    "\n",
    "svm_clf = make_pipeline(StandardScaler(),\n",
    "                        LinearSVC(C=1, dual=True, random_state=42))\n",
    "svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9d00f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizamos predicciones para nuevos ejemplos\n",
    "X_new = [[5.5, 1.7], [5.0, 1.5]]\n",
    "print(\"Predicciones:\", svm_clf.predict(X_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807fcc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos las puntuaciones de decisión (distancia a la frontera de decisión)\n",
    "print(\"Función de decisión:\", svm_clf.decision_function(X_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8049e23f",
   "metadata": {},
   "source": [
    "A continuación se muestra otra figura comparativa que ilustra el efecto de modificar el parámetro de regularización C en el clasificador SVM lineal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa48af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Código adicional – Esta celda genera y guarda la Figura 5–4.\n",
    "#\n",
    "# Explicación:\n",
    "# - Se entrenan dos modelos LinearSVC con diferentes valores de C (1 y 100).\n",
    "# - Se escalan los datos para que el entrenamiento sea más estable.\n",
    "# - Se “convierte” manualmente a parámetros no escalados para dibujar correctamente la frontera de decisión.\n",
    "# - Se calculan los vectores de soporte y se dibujan las fronteras de decisión en dos subplots para comparar el efecto de la regularización.\n",
    "\n",
    "scaler = StandardScaler()\n",
    "svm_clf1 = LinearSVC(C=1, max_iter=10_000, dual=True, random_state=42)\n",
    "svm_clf2 = LinearSVC(C=100, max_iter=10_000, dual=True, random_state=42)\n",
    "\n",
    "scaled_svm_clf1 = make_pipeline(scaler, svm_clf1)\n",
    "scaled_svm_clf2 = make_pipeline(scaler, svm_clf2)\n",
    "\n",
    "scaled_svm_clf1.fit(X, y)\n",
    "scaled_svm_clf2.fit(X, y)\n",
    "\n",
    "# Convertir los parámetros a la escala original\n",
    "b1 = svm_clf1.decision_function([-scaler.mean_ / scaler.scale_])\n",
    "b2 = svm_clf2.decision_function([-scaler.mean_ / scaler.scale_])\n",
    "w1 = svm_clf1.coef_[0] / scaler.scale_\n",
    "w2 = svm_clf2.coef_[0] / scaler.scale_\n",
    "svm_clf1.intercept_ = np.array([b1])\n",
    "svm_clf2.intercept_ = np.array([b2])\n",
    "svm_clf1.coef_ = np.array([w1])\n",
    "svm_clf2.coef_ = np.array([w2])\n",
    "\n",
    "# Calcular los vectores de soporte (LinearSVC no lo hace automáticamente)\n",
    "t = y * 2 - 1\n",
    "support_vectors_idx1 = (t * (X.dot(w1) + b1) < 1).ravel()\n",
    "support_vectors_idx2 = (t * (X.dot(w2) + b2) < 1).ravel()\n",
    "svm_clf1.support_vectors_ = X[support_vectors_idx1]\n",
    "svm_clf2.support_vectors_ = X[support_vectors_idx2]\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10, 2.7), sharey=True)\n",
    "\n",
    "plt.sca(axes[0])\n",
    "plt.plot(X[:, 0][y == 1], X[:, 1][y == 1], \"g^\", label=\"Iris virginica\")\n",
    "plt.plot(X[:, 0][y == 0], X[:, 1][y == 0], \"bs\", label=\"Iris versicolor\")\n",
    "plot_svc_decision_boundary(svm_clf1, 4, 5.9)\n",
    "plt.xlabel(\"Longitud del pétalo\")\n",
    "plt.ylabel(\"Ancho del pétalo\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.title(f\"$C = {svm_clf1.C}$\")\n",
    "plt.axis([4, 5.9, 0.8, 2.8])\n",
    "plt.grid()\n",
    "\n",
    "plt.sca(axes[1])\n",
    "plt.plot(X[:, 0][y == 1], X[:, 1][y == 1], \"g^\")\n",
    "plt.plot(X[:, 0][y == 0], X[:, 1][y == 0], \"bs\")\n",
    "plot_svc_decision_boundary(svm_clf2, 4, 5.99)\n",
    "plt.xlabel(\"Longitud del pétalo\")\n",
    "plt.title(f\"$C = {svm_clf2.C}$\")\n",
    "plt.axis([4, 5.9, 0.8, 2.8])\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c1632b",
   "metadata": {},
   "source": [
    "# Clasificación con SVM no lineal\n",
    "\n",
    "Ahora exploraremos cómo aplicar SVM en problemas no lineales mediante la transformación de los datos a espacios de mayor dimensión o utilizando núcleos (kernels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7548706e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Código adicional – Esta celda genera y guarda la Figura 5–5.\n",
    "#\n",
    "# Explicación:\n",
    "# - Se genera un conjunto de datos 1D y se extiende a 2D usando una transformación cuadrática.\n",
    "# - Se muestran las representaciones en 1D (con puntos en una línea) y en 2D.\n",
    "# - Se dibuja una línea (en rojo) para ilustrar cómo una frontera lineal en el espacio transformado equivale a una frontera no lineal en el espacio original.\n",
    "\n",
    "X1D = np.linspace(-4, 4, 9).reshape(-1, 1)\n",
    "X2D = np.c_[X1D, X1D**2]\n",
    "y = np.array([0, 0, 1, 1, 1, 1, 1, 0, 0])\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.grid(True)\n",
    "plt.axhline(y=0, color='k')\n",
    "plt.plot(X1D[:, 0][y == 0], np.zeros(4), \"bs\")\n",
    "plt.plot(X1D[:, 0][y == 1], np.zeros(5), \"g^\")\n",
    "plt.gca().get_yaxis().set_ticks([])\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.axis([-4.5, 4.5, -0.2, 0.2])\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.grid(True)\n",
    "plt.axhline(y=0, color='k')\n",
    "plt.axvline(x=0, color='k')\n",
    "plt.plot(X2D[:, 0][y == 0], X2D[:, 1][y == 0], \"bs\")\n",
    "plt.plot(X2D[:, 0][y == 1], X2D[:, 1][y == 1], \"g^\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$  \", rotation=0)\n",
    "plt.gca().get_yaxis().set_ticks([0, 4, 8, 12, 16])\n",
    "plt.plot([-4.5, 4.5], [6.5, 6.5], \"r--\", linewidth=3)\n",
    "plt.axis([-4.5, 4.5, -1, 17])\n",
    "\n",
    "plt.subplots_adjust(right=1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a611a780",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Ahora trabajamos con el dataset \"moons\", que es un clásico ejemplo no lineal.\n",
    "# Se utiliza un pipeline que incluye:\n",
    "# - Transformación polinómica (grado 3)\n",
    "# - Escalado\n",
    "# - Un clasificador lineal (LinearSVC) con pérdida \"hinge\"\n",
    "#\n",
    "# De esta forma, se logra una clasificación no lineal mediante la transformación de las características.\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "X, y = make_moons(n_samples=100, noise=0.15, random_state=42)\n",
    "\n",
    "polynomial_svm_clf = make_pipeline(\n",
    "    PolynomialFeatures(degree=3),\n",
    "    StandardScaler(),\n",
    "    LinearSVC(C=10, max_iter=10_000, dual=True, random_state=42)\n",
    ")\n",
    "polynomial_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdde8754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Código adicional – Esta celda genera y guarda la Figura 5–6.\n",
    "#\n",
    "# Se definen dos funciones auxiliares:\n",
    "# - plot_dataset: para dibujar los puntos del dataset.\n",
    "# - plot_predictions: para dibujar las predicciones y la función de decisión sobre una malla.\n",
    "#\n",
    "# Se utiliza para visualizar la frontera de decisión resultante del clasificador polinómico.\n",
    "\n",
    "def plot_dataset(X, y, axes):\n",
    "    plt.plot(X[:, 0][y == 0], X[:, 1][y == 0], \"bs\")\n",
    "    plt.plot(X[:, 0][y == 1], X[:, 1][y == 1], \"g^\")\n",
    "    plt.axis(axes)\n",
    "    plt.grid(True)\n",
    "    plt.xlabel(\"$x_1$\")\n",
    "    plt.ylabel(\"$x_2$\", rotation=0)\n",
    "\n",
    "def plot_predictions(clf, axes):\n",
    "    x0s = np.linspace(axes[0], axes[1], 100)\n",
    "    x1s = np.linspace(axes[2], axes[3], 100)\n",
    "    x0, x1 = np.meshgrid(x0s, x1s)\n",
    "    X_grid = np.c_[x0.ravel(), x1.ravel()]\n",
    "    y_pred = clf.predict(X_grid).reshape(x0.shape)\n",
    "    y_decision = clf.decision_function(X_grid).reshape(x0.shape)\n",
    "    plt.contourf(x0, x1, y_pred, cmap=plt.cm.brg, alpha=0.2)\n",
    "    plt.contourf(x0, x1, y_decision, cmap=plt.cm.brg, alpha=0.1)\n",
    "\n",
    "plot_predictions(polynomial_svm_clf, [-1.5, 2.5, -1, 1.5])\n",
    "plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66897b41",
   "metadata": {},
   "source": [
    "## Núcleo polinómico\n",
    "\n",
    "Ahora usaremos el núcleo polinómico directamente con el clasificador SVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e547456",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Entrenamos un SVC con kernel polinómico (grado 3)\n",
    "poly_kernel_svm_clf = make_pipeline(StandardScaler(),\n",
    "                                    SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\n",
    "poly_kernel_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37d8181",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Código adicional – Esta celda genera y guarda la Figura 5–7.\n",
    "#\n",
    "# Se entrena un segundo modelo SVC con kernel polinómico, pero con parámetros:\n",
    "# grado 10, coef0=100 y C=5.\n",
    "# Se dibujan las fronteras de decisión de ambos modelos para compararlos.\n",
    "\n",
    "poly100_kernel_svm_clf = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    SVC(kernel=\"poly\", degree=10, coef0=100, C=5)\n",
    ")\n",
    "poly100_kernel_svm_clf.fit(X, y)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10.5, 4), sharey=True)\n",
    "\n",
    "plt.sca(axes[0])\n",
    "plot_predictions(poly_kernel_svm_clf, [-1.5, 2.45, -1, 1.5])\n",
    "plot_dataset(X, y, [-1.5, 2.4, -1, 1.5])\n",
    "plt.title(\"grado=3, coef0=1, C=5\")\n",
    "\n",
    "plt.sca(axes[1])\n",
    "plot_predictions(poly100_kernel_svm_clf, [-1.5, 2.45, -1, 1.5])\n",
    "plot_dataset(X, y, [-1.5, 2.4, -1, 1.5])\n",
    "plt.title(\"grado=10, coef0=100, C=5\")\n",
    "plt.ylabel(\"\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3aa54b",
   "metadata": {},
   "source": [
    "## Funciones de similitud\n",
    "\n",
    "En esta sección se muestra cómo se pueden crear nuevas características que miden la similitud entre una instancia y ciertos “landmarks” (puntos de referencia). Esto es la base de muchos métodos kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b1cb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Código adicional – Esta celda genera y guarda la Figura 5–8.\n",
    "#\n",
    "# Se define la función gaussiana RBF y se calcula la similitud de los puntos respecto a dos landmarks.\n",
    "# Se ilustra la transformación de los datos originales a este nuevo espacio de similitud.\n",
    "\n",
    "def gaussian_rbf(x, landmark, gamma):\n",
    "    \"\"\"\n",
    "    Calcula la similitud RBF gaussiana entre cada punto en x y un landmark.\n",
    "    \"\"\"\n",
    "    return np.exp(-gamma * np.linalg.norm(x - landmark, axis=1)**2)\n",
    "\n",
    "gamma = 0.3\n",
    "\n",
    "x1s = np.linspace(-4.5, 4.5, 200).reshape(-1, 1)\n",
    "x2s = gaussian_rbf(x1s, -2, gamma)\n",
    "x3s = gaussian_rbf(x1s, 1, gamma)\n",
    "\n",
    "# Transformamos los datos 1D usando la función RBF para dos landmarks: -2 y 1.\n",
    "XK = np.c_[gaussian_rbf(X1D, -2, gamma), gaussian_rbf(X1D, 1, gamma)]\n",
    "yk = np.array([0, 0, 1, 1, 1, 1, 1, 0, 0])\n",
    "\n",
    "plt.figure(figsize=(10.5, 4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.grid(True)\n",
    "plt.axhline(y=0, color='k')\n",
    "plt.scatter(x=[-2, 1], y=[0, 0], s=150, alpha=0.5, c=\"red\")\n",
    "plt.plot(X1D[:, 0][yk == 0], np.zeros(4), \"bs\")\n",
    "plt.plot(X1D[:, 0][yk == 1], np.zeros(5), \"g^\")\n",
    "plt.plot(x1s, x2s, \"g--\")\n",
    "plt.plot(x1s, x3s, \"b:\")\n",
    "plt.gca().get_yaxis().set_ticks([0, 0.25, 0.5, 0.75, 1])\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"Similitud\")\n",
    "plt.annotate(\n",
    "    r'$\\mathbf{x}$',\n",
    "    xy=(X1D[3, 0], 0),\n",
    "    xytext=(-0.5, 0.20),\n",
    "    ha=\"center\",\n",
    "    arrowprops=dict(facecolor='black', shrink=0.1),\n",
    "    fontsize=16,\n",
    ")\n",
    "plt.text(-2, 0.9, \"$x_2$\", ha=\"center\", fontsize=15)\n",
    "plt.text(1, 0.9, \"$x_3$\", ha=\"center\", fontsize=15)\n",
    "plt.axis([-4.5, 4.5, -0.1, 1.1])\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.grid(True)\n",
    "plt.axhline(y=0, color='k')\n",
    "plt.axvline(x=0, color='k')\n",
    "plt.plot(XK[:, 0][yk == 0], XK[:, 1][yk == 0], \"bs\")\n",
    "plt.plot(XK[:, 0][yk == 1], XK[:, 1][yk == 1], \"g^\")\n",
    "plt.xlabel(\"$x_2$\")\n",
    "plt.ylabel(\"$x_3$  \", rotation=0)\n",
    "plt.annotate(\n",
    "    r'$\\phi\\left(\\mathbf{x}\\right)$',\n",
    "    xy=(XK[3, 0], XK[3, 1]),\n",
    "    xytext=(0.65, 0.50),\n",
    "    ha=\"center\",\n",
    "    arrowprops=dict(facecolor='black', shrink=0.1),\n",
    "    fontsize=16,\n",
    ")\n",
    "plt.plot([-0.1, 1.1], [0.57, -0.1], \"r--\", linewidth=3)\n",
    "plt.axis([-0.1, 1.1, -0.1, 1.1])\n",
    "    \n",
    "plt.subplots_adjust(right=1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1379ec7b",
   "metadata": {},
   "source": [
    "## Núcleo RBF Gaussiano\n",
    "\n",
    "Se entrena un SVC utilizando el núcleo RBF (función gaussiana) para abordar problemas no lineales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554b525c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_kernel_svm_clf = make_pipeline(StandardScaler(),\n",
    "                                   SVC(kernel=\"rbf\", gamma=5, C=0.001))\n",
    "rbf_kernel_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffa2163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Código adicional – Esta celda genera y guarda la Figura 5–9.\n",
    "#\n",
    "# Se entrenan varios modelos SVC con diferentes combinaciones de hiperparámetros (gamma y C)\n",
    "# y se dibujan sus fronteras de decisión en un arreglo de 2x2 subplots para comparar su comportamiento.\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "gamma1, gamma2 = 0.1, 5\n",
    "C1, C2 = 0.001, 1000\n",
    "hyperparams = (gamma1, C1), (gamma1, C2), (gamma2, C1), (gamma2, C2)\n",
    "\n",
    "svm_clfs = []\n",
    "for gamma, C in hyperparams:\n",
    "    rbf_kernel_svm_clf = make_pipeline(\n",
    "        StandardScaler(),\n",
    "        SVC(kernel=\"rbf\", gamma=gamma, C=C)\n",
    "    )\n",
    "    rbf_kernel_svm_clf.fit(X, y)\n",
    "    svm_clfs.append(rbf_kernel_svm_clf)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10.5, 7), sharex=True, sharey=True)\n",
    "\n",
    "for i, svm_clf in enumerate(svm_clfs):\n",
    "    plt.sca(axes[i // 2, i % 2])\n",
    "    plot_predictions(svm_clf, [-1.5, 2.45, -1, 1.5])\n",
    "    plot_dataset(X, y, [-1.5, 2.45, -1, 1.5])\n",
    "    gamma_val, C_val = hyperparams[i]\n",
    "    plt.title(f\"gamma={gamma_val}, C={C_val}\")\n",
    "    if i in (0, 1):\n",
    "        plt.xlabel(\"\")\n",
    "    if i in (1, 3):\n",
    "        plt.ylabel(\"\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000bf7d9",
   "metadata": {},
   "source": [
    "# Regresión con SVM\n",
    "\n",
    "Además de clasificación, las SVM se pueden utilizar para regresión. En esta sección veremos dos ejemplos:\n",
    "- Regresión lineal con LinearSVR.\n",
    "- Regresión con núcleo polinómico usando SVR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1e9720",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "# Código adicional – Estas 3 líneas generan un conjunto de datos lineal simple\n",
    "np.random.seed(42)\n",
    "X = 2 * np.random.rand(50, 1)\n",
    "y = 4 + 3 * X[:, 0] + np.random.randn(50)\n",
    "\n",
    "# Entrenamos un modelo de regresión SVM lineal (LinearSVR)\n",
    "svm_reg = make_pipeline(StandardScaler(),\n",
    "                        LinearSVR(epsilon=0.5, dual=True, random_state=42))\n",
    "svm_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3162db8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Código adicional – Esta celda genera y guarda la Figura 5–10.\n",
    "#\n",
    "# Se definen dos funciones:\n",
    "# - find_support_vectors: encuentra los índices de las instancias que están fuera del margen ε.\n",
    "# - plot_svm_regression: dibuja la predicción del modelo, los márgenes y los vectores de soporte.\n",
    "#\n",
    "# Se comparan dos modelos de regresión SVM con diferentes valores de epsilon.\n",
    "\n",
    "def find_support_vectors(svm_reg, X, y):\n",
    "    y_pred = svm_reg.predict(X)\n",
    "    epsilon = svm_reg[-1].epsilon\n",
    "    off_margin = np.abs(y - y_pred) >= epsilon\n",
    "    return np.argwhere(off_margin)\n",
    "\n",
    "def plot_svm_regression(svm_reg, X, y, axes):\n",
    "    x1s = np.linspace(axes[0], axes[1], 100).reshape(100, 1)\n",
    "    y_pred = svm_reg.predict(x1s)\n",
    "    epsilon = svm_reg[-1].epsilon\n",
    "    plt.plot(x1s, y_pred, \"k-\", linewidth=2, label=r\"$\\hat{y}$\", zorder=-2)\n",
    "    plt.plot(x1s, y_pred + epsilon, \"k--\", zorder=-2)\n",
    "    plt.plot(x1s, y_pred - epsilon, \"k--\", zorder=-2)\n",
    "    plt.scatter(X[svm_reg._support], y[svm_reg._support], s=180,\n",
    "                facecolors='#AAA', zorder=-1)\n",
    "    plt.plot(X, y, \"bo\")\n",
    "    plt.xlabel(\"$x_1$\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.axis(axes)\n",
    "\n",
    "# Entrenamos un segundo modelo con un epsilon mayor para ver el efecto\n",
    "svm_reg2 = make_pipeline(StandardScaler(),\n",
    "                         LinearSVR(epsilon=1.2, dual=True, random_state=42))\n",
    "svm_reg2.fit(X, y)\n",
    "\n",
    "# Encontramos los vectores de soporte para cada modelo\n",
    "svm_reg._support = find_support_vectors(svm_reg, X, y)\n",
    "svm_reg2._support = find_support_vectors(svm_reg2, X, y)\n",
    "\n",
    "eps_x1 = 1\n",
    "eps_y_pred = svm_reg2.predict([[eps_x1]])\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(9, 4), sharey=True)\n",
    "plt.sca(axes[0])\n",
    "plot_svm_regression(svm_reg, X, y, [0, 2, 3, 11])\n",
    "plt.title(f\"epsilon={svm_reg[-1].epsilon}\")\n",
    "plt.ylabel(\"$y$\", rotation=0)\n",
    "plt.grid()\n",
    "plt.sca(axes[1])\n",
    "plot_svm_regression(svm_reg2, X, y, [0, 2, 3, 11])\n",
    "plt.title(f\"epsilon={svm_reg2[-1].epsilon}\")\n",
    "plt.annotate(\n",
    "    '', xy=(eps_x1, eps_y_pred), xycoords='data',\n",
    "    xytext=(eps_x1, eps_y_pred - svm_reg2[-1].epsilon),\n",
    "    textcoords='data', arrowprops={'arrowstyle': '<->', 'linewidth': 1.5}\n",
    ")\n",
    "plt.text(0.90, 5.4, r\"$\\epsilon$\", fontsize=16)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c41e0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "# Código adicional – Estas 3 líneas generan un conjunto de datos cuadrático simple\n",
    "np.random.seed(42)\n",
    "X = 2 * np.random.rand(50, 1) - 1\n",
    "y = 0.2 + 0.1 * X[:, 0] + 0.5 * X[:, 0] ** 2 + np.random.randn(50) / 10\n",
    "\n",
    "# Entrenamos un SVR con núcleo polinómico (grado 2)\n",
    "svm_poly_reg = make_pipeline(StandardScaler(),\n",
    "                             SVR(kernel=\"poly\", degree=2, C=0.01, epsilon=0.1))\n",
    "svm_poly_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d27934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Código adicional – Esta celda genera y guarda la Figura 5–11.\n",
    "#\n",
    "# Se entrena un segundo modelo de SVR con parámetros distintos para comparar.\n",
    "# Se dibujan las predicciones y se identifican los vectores de soporte.\n",
    "\n",
    "svm_poly_reg2 = make_pipeline(StandardScaler(),\n",
    "                             SVR(kernel=\"poly\", degree=2, C=100))\n",
    "svm_poly_reg2.fit(X, y)\n",
    "\n",
    "svm_poly_reg._support = find_support_vectors(svm_poly_reg, X, y)\n",
    "svm_poly_reg2._support = find_support_vectors(svm_poly_reg2, X, y)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(9, 4), sharey=True)\n",
    "plt.sca(axes[0])\n",
    "plot_svm_regression(svm_poly_reg, X, y, [-1, 1, 0, 1])\n",
    "plt.title(f\"grado={svm_poly_reg[-1].degree}, C={svm_poly_reg[-1].C}, ε={svm_poly_reg[-1].epsilon}\")\n",
    "plt.ylabel(\"$y$\", rotation=0)\n",
    "plt.grid()\n",
    "\n",
    "plt.sca(axes[1])\n",
    "plot_svm_regression(svm_poly_reg2, X, y, [-1, 1, 0, 1])\n",
    "plt.title(f\"grado={svm_poly_reg2[-1].degree}, C={svm_poly_reg2[-1].C}, ε={svm_poly_reg2[-1].epsilon}\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa77306",
   "metadata": {},
   "source": [
    "# Bajo el Capó\n",
    "\n",
    "En esta sección se muestran detalles internos de las SVM, como la relación entre el parámetro de peso y el margen, y se visualizan las funciones de pérdida (hinge loss) y su versión cuadrática."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d01d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Código adicional – Esta celda genera y guarda la Figura 5–12.\n",
    "#\n",
    "# Explicación:\n",
    "# - Se define una función para graficar la función de decisión 2D para diferentes valores de w (peso) y el margen correspondiente.\n",
    "# - Se dibujan dos subplots para comparar el efecto de un peso mayor o menor en la anchura del margen.\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def plot_2D_decision_function(w, b, ylabel=True, x1_lim=[-3, 3]):\n",
    "    x1 = np.linspace(x1_lim[0], x1_lim[1], 200)\n",
    "    y_vals = w * x1 + b\n",
    "    half_margin = 1 / abs(w)\n",
    "\n",
    "    plt.plot(x1, y_vals, \"b-\", linewidth=2, label=r\"$s = w_1 x_1$\")\n",
    "    plt.axhline(y=0, color='k', linewidth=1)\n",
    "    plt.axvline(x=0, color='k', linewidth=1)\n",
    "    rect = patches.Rectangle((-half_margin, -2), 2 * half_margin, 4,\n",
    "                             edgecolor='none', facecolor='gray', alpha=0.2)\n",
    "    plt.gca().add_patch(rect)\n",
    "    plt.plot([-3, 3], [1, 1], \"k--\", linewidth=1)\n",
    "    plt.plot([-3, 3], [-1, -1], \"k--\", linewidth=1)\n",
    "    plt.plot(half_margin, 1, \"k.\")\n",
    "    plt.plot(-half_margin, -1, \"k.\")\n",
    "    plt.axis(x1_lim + [-2, 2])\n",
    "    plt.xlabel(\"$x_1$\")\n",
    "    if ylabel:\n",
    "        plt.ylabel(\"$s$\", rotation=0, labelpad=5)\n",
    "        plt.legend()\n",
    "        plt.text(1.02, -1.6, \"Margen\", ha=\"left\", va=\"center\", color=\"k\")\n",
    "\n",
    "    plt.annotate(\n",
    "        '', xy=(-half_margin, -1.6), xytext=(half_margin, -1.6),\n",
    "        arrowprops={'ec': 'k', 'arrowstyle': '<->', 'linewidth': 1.5}\n",
    "    )\n",
    "    plt.title(f\"$w_1 = {w}$\")\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(9, 3.2), sharey=True)\n",
    "plt.sca(axes[0])\n",
    "plot_2D_decision_function(1, 0)\n",
    "plt.grid()\n",
    "plt.sca(axes[1])\n",
    "plot_2D_decision_function(0.5, 0, ylabel=False)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce3d51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Código adicional – Esta celda genera y guarda la Figura 5–13.\n",
    "#\n",
    "# Se grafican la función hinge loss (pérdida de bisagra) y la versión cuadrática de la misma para las etiquetas t=1 y t=-1.\n",
    "# Esto ilustra cómo varía la penalización según la distancia a la frontera de decisión.\n",
    "\n",
    "s = np.linspace(-2.5, 2.5, 200)\n",
    "hinge_pos = np.where(1 - s < 0, 0, 1 - s)  # max(0, 1 - s)\n",
    "hinge_neg = np.where(1 + s < 0, 0, 1 + s)  # max(0, 1 + s)\n",
    "\n",
    "titles = (r\"Pérdida hinge = $max(0, 1 - s\\,t)$\", \"Pérdida hinge cuadrática\")\n",
    "\n",
    "fix, axs = plt.subplots(1, 2, sharey=True, figsize=(8.2, 3))\n",
    "\n",
    "for ax, loss_pos, loss_neg, title in zip(\n",
    "        axs, (hinge_pos, hinge_pos ** 2), (hinge_neg, hinge_neg ** 2), titles):\n",
    "    ax.plot(s, loss_pos, \"g-\", linewidth=2, zorder=10, label=\"$t=1$\")\n",
    "    ax.plot(s, loss_neg, \"r--\", linewidth=2, zorder=10, label=\"$t=-1$\")\n",
    "    ax.grid(True)\n",
    "    ax.axhline(y=0, color='k')\n",
    "    ax.axvline(x=0, color='k')\n",
    "    ax.set_xlabel(r\"$s = \\mathbf{w}^\\intercal \\mathbf{x} + b$\")\n",
    "    ax.axis([-2.5, 2.5, -0.5, 2.5])\n",
    "    ax.legend(loc=\"center right\")\n",
    "    ax.set_title(title)\n",
    "    ax.set_yticks(np.arange(0, 2.5, 1))\n",
    "    ax.set_aspect(\"equal\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec0507a",
   "metadata": {},
   "source": [
    "# Material Extra\n",
    "\n",
    "A continuación se muestra una implementación personalizada de un clasificador SVM lineal utilizando descenso de gradiente por lotes. Esto es útil para comprender en detalle cómo funciona el algoritmo de optimización."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075dc9de",
   "metadata": {},
   "source": [
    "## Implementación de un clasificador SVM lineal usando descenso de gradiente por lotes\n",
    "\n",
    "Se define la clase `MyLinearSVC` que:\n",
    "- Inicializa los parámetros del modelo de forma aleatoria.\n",
    "- Utiliza una tasa de aprendizaje que disminuye con el número de épocas.\n",
    "- Realiza el entrenamiento mediante descenso de gradiente sobre el conjunto de datos.\n",
    "- Calcula la función de decisión y realiza predicciones.\n",
    "\n",
    "Se comparará el rendimiento de este modelo con el de los estimadores de Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff98f52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\n",
    "y = (iris.target == 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f212ba8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class MyLinearSVC(BaseEstimator):\n",
    "    def __init__(self, C=1, eta0=1, eta_d=10000, n_epochs=1000, random_state=None):\n",
    "        self.C = C\n",
    "        self.eta0 = eta0\n",
    "        self.n_epochs = n_epochs\n",
    "        self.random_state = random_state\n",
    "        self.eta_d = eta_d\n",
    "\n",
    "    def eta(self, epoch):\n",
    "        # Tasa de aprendizaje que decrece con el número de épocas\n",
    "        return self.eta0 / (epoch + self.eta_d)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Inicialización aleatoria\n",
    "        if self.random_state:\n",
    "            np.random.seed(self.random_state)\n",
    "        w = np.random.randn(X.shape[1], 1)  # Pesos para cada característica\n",
    "        b = 0\n",
    "\n",
    "        # Convertimos las etiquetas a -1 y 1\n",
    "        t = np.array(y, dtype=np.float64).reshape(-1, 1) * 2 - 1\n",
    "        X_t = X * t  # Multiplicamos cada fila por su etiqueta\n",
    "        self.Js = []  # Lista para almacenar la función de costo en cada época\n",
    "\n",
    "        # Entrenamiento mediante descenso de gradiente\n",
    "        for epoch in range(self.n_epochs):\n",
    "            # Identificamos los vectores de soporte (donde la condición  t*(w^T x + b) < 1 se cumple)\n",
    "            support_vectors_idx = (X_t.dot(w) + t * b < 1).ravel()\n",
    "            X_t_sv = X_t[support_vectors_idx]\n",
    "            t_sv = t[support_vectors_idx]\n",
    "\n",
    "            # Calculamos la función de costo (pérdida + término de regularización)\n",
    "            J = 0.5 * (w * w).sum() + self.C * ((1 - X_t_sv.dot(w)).sum() - b * t_sv.sum())\n",
    "            self.Js.append(J)\n",
    "\n",
    "            # Derivada de la función de costo con respecto a w y b\n",
    "            w_gradient_vector = w - self.C * X_t_sv.sum(axis=0).reshape(-1, 1)\n",
    "            b_derivative = -self.C * t_sv.sum()\n",
    "                \n",
    "            # Actualización de los parámetros\n",
    "            w = w - self.eta(epoch) * w_gradient_vector\n",
    "            b = b - self.eta(epoch) * b_derivative\n",
    "            \n",
    "        self.intercept_ = np.array([b])\n",
    "        self.coef_ = np.array([w])\n",
    "        # Recalculamos los vectores de soporte con los parámetros finales\n",
    "        support_vectors_idx = (X_t.dot(w) + t * b < 1).ravel()\n",
    "        self.support_vectors_ = X[support_vectors_idx]\n",
    "        return self\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        return X.dot(self.coef_[0]) + self.intercept_[0]\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.decision_function(X) >= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e02d404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamos nuestro clasificador SVM lineal personalizado con unos hiperparámetros ajustados.\n",
    "C = 2\n",
    "svm_clf = MyLinearSVC(C=C, eta0=10, eta_d=1000, n_epochs=60000, random_state=2)\n",
    "svm_clf.fit(X, y)\n",
    "print(\"Predicciones MyLinearSVC:\", svm_clf.predict(np.array([[5, 2], [4, 1]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813b9570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos la evolución de la función de costo a lo largo de las épocas.\n",
    "plt.plot(range(svm_clf.n_epochs), svm_clf.Js)\n",
    "plt.axis([0, svm_clf.n_epochs, 0, 100])\n",
    "plt.xlabel(\"Épocas\")\n",
    "plt.ylabel(\"Pérdida\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1313af4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Parámetros MyLinearSVC:\", svm_clf.intercept_, svm_clf.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f950b3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparación con el clasificador SVC de Scikit-Learn\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_clf2 = SVC(kernel=\"linear\", C=C)\n",
    "svm_clf2.fit(X, y.ravel())\n",
    "print(\"Parámetros SVC:\", svm_clf2.intercept_, svm_clf2.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07b7a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizamos y comparamos las fronteras de decisión de MyLinearSVC y SVC\n",
    "yr = y.ravel()\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(11, 3.2), sharey=True)\n",
    "plt.sca(axes[0])\n",
    "plt.plot(X[:, 0][yr == 1], X[:, 1][yr == 1], \"g^\", label=\"Iris virginica\")\n",
    "plt.plot(X[:, 0][yr == 0], X[:, 1][yr == 0], \"bs\", label=\"No Iris virginica\")\n",
    "plot_svc_decision_boundary(svm_clf, 4, 6)\n",
    "plt.xlabel(\"Longitud del pétalo\")\n",
    "plt.ylabel(\"Ancho del pétalo\")\n",
    "plt.title(\"MyLinearSVC\")\n",
    "plt.axis([4, 6, 0.8, 2.8])\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.grid()\n",
    "\n",
    "plt.sca(axes[1])\n",
    "plt.plot(X[:, 0][yr == 1], X[:, 1][yr == 1], \"g^\")\n",
    "plt.plot(X[:, 0][yr == 0], X[:, 1][yr == 0], \"bs\")\n",
    "plot_svc_decision_boundary(svm_clf2, 4, 6)\n",
    "plt.xlabel(\"Longitud del pétalo\")\n",
    "plt.title(\"SVC\")\n",
    "plt.axis([4, 6, 0.8, 2.8])\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18090a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalmente, entrenamos un clasificador SVM usando SGD (descenso de gradiente estocástico)\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_clf = SGDClassifier(loss=\"hinge\", alpha=0.017, max_iter=1000, tol=1e-3, random_state=42)\n",
    "sgd_clf.fit(X, y)\n",
    "\n",
    "m = len(X)\n",
    "t = np.array(y).reshape(-1, 1) * 2 - 1  # Convertimos etiquetas a -1 y +1\n",
    "X_b = np.c_[np.ones((m, 1)), X]  # Añadimos la entrada del sesgo (x0 = 1)\n",
    "X_b_t = X_b * t\n",
    "sgd_theta = np.r_[sgd_clf.intercept_[0], sgd_clf.coef_[0]]\n",
    "print(\"Parámetros SGDClassifier:\", sgd_theta)\n",
    "support_vectors_idx = (X_b_t.dot(sgd_theta) < 1).ravel()\n",
    "sgd_clf.support_vectors_ = X[support_vectors_idx]\n",
    "sgd_clf.C = C\n",
    "\n",
    "plt.figure(figsize=(5.5, 3.2))\n",
    "plt.plot(X[:, 0][yr == 1], X[:, 1][yr == 1], \"g^\")\n",
    "plt.plot(X[:, 0][yr == 0], X[:, 1][yr == 0], \"bs\")\n",
    "plot_svc_decision_boundary(sgd_clf, 4, 6)\n",
    "plt.xlabel(\"Longitud del pétalo\")\n",
    "plt.ylabel(\"Ancho del pétalo\")\n",
    "plt.title(\"SGDClassifier\")\n",
    "plt.axis([4, 6, 0.8, 2.8])\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

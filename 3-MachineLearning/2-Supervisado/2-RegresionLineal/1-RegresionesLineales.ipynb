{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Regresiones Lineales**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comenzaremos con el an√°lisis del modelo de regresi√≥n lineal, uno de los m√°s simples que existen. Exploraremos dos m√©todos diferentes para entrenarlo:\n",
    "\n",
    "- Ecuaci√≥n de \"forma cerrada\": Este m√©todo calcula directamente los par√°metros del modelo que mejor se ajustan al conjunto de entrenamiento, minimizando la funci√≥n de coste.\n",
    "\n",
    "- Descenso de gradiente (DG): Un m√©todo de optimizaci√≥n iterativo que ajusta gradualmente los par√°metros del modelo para minimizar la funci√≥n de coste, convergiendo finalmente a los mismos par√°metros que el primer m√©todo.\n",
    "\n",
    "Luego, estudiaremos la regresi√≥n polin√≥mica, un modelo m√°s complejo que puede ajustarse a conjuntos de datos no lineales. Dado que este modelo tiene m√°s par√°metros que la regresi√≥n lineal, es m√°s propenso al sobreajuste. Exploraremos c√≥mo detectar el sobreajuste utilizando curvas de aprendizaje y luego veremos varias t√©cnicas de regularizaci√≥n para reducir este riesgo.\n",
    "\n",
    "Finalmente, examinaremos otros dos modelos com√∫nmente utilizados para tareas de clasificaci√≥n: la regresi√≥n log√≠stica y la regresi√≥n softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresi√≥n lineal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede expresar de manera m√°s concisa usando una forma vectorizada:\n",
    "\n",
    "$$\\hat{y} = ‚Ñé_{Œ∏}(x) = Œ∏ ¬∑ x$$\n",
    "\n",
    "En esta ecuaci√≥n:\n",
    "\n",
    "- $h_{Œ∏}$ es la funci√≥n de hip√≥tesis usando los par√°metros del modelo $Œ∏$.\n",
    "- $Œ∏$ es el vector de par√°metros, que incluye el sesgo $Œ∏‚ÇÄ$ y los pesos $Œ∏‚ÇÅ$ a $Œ∏_n$.\n",
    "- $x$ es el vector de caracter√≠sticas, que incluye $x‚ÇÄ$ a $x_n$, con $x‚ÇÄ = 1$.\n",
    "- $Œ∏ \\cdot x$ es el producto punto de los vectores $Œ∏$ y $x$, lo que equivale a $Œ∏‚ÇÄx‚ÇÄ + Œ∏‚ÇÅx‚ÇÅ + ... + Œ∏_nx_n$.\n",
    "\n",
    "En Machine Learning, los vectores suelen representarse como vectores columna. Si $Œ∏$ y $x$ son vectores columna, la predicci√≥n es:\n",
    "\n",
    "$$\\hat{ùë¶}=ùúÉ^‚ä∫\\timesùë•$$\n",
    "\n",
    "Aqu√≠, $Œ∏^‚ä∫$ es la transpuesta de $Œ∏$ (vector fila), y $Œ∏^‚ä∫ \\times x$ es la multiplicaci√≥n de matrices, que da el mismo resultado que el producto punto, pero como una matriz de una sola celda en lugar de un escalar.\n",
    "\n",
    "Entrenamiento del modelo: Para entrenar un modelo de regresi√≥n lineal, necesitamos ajustar sus par√°metros para que el modelo se ajuste mejor al conjunto de entrenamiento. Esto se logra minimizando el error cuadr√°tico medio (MSE):\n",
    "\n",
    "$$ùëÄùëÜùê∏(ùúÉ)=\\frac{1}{ùëö}\\sum_{i=1}{ùëö}(ùúÉ‚ä∫ùë•ùëñ‚àíùë¶ùëñ)2$$\n",
    " \n",
    "\n",
    "Donde $m$ es el n√∫mero de instancias en el conjunto de entrenamiento, $x_{i}$ es el vector de caracter√≠sticas de la $i$-√©sima instancia, y $y_{i}$ es el valor objetivo correspondiente.\n",
    "\n",
    "Para simplificar, escribimos $MSE(Œ∏)$ en lugar de $MSE(X, h_{Œ∏})$, ya que la funci√≥n est√° parametrizada por $Œ∏$.\n",
    "\n",
    "Este enfoque es com√∫n porque minimizar el MSE es m√°s sencillo que minimizar el RMSE y conduce al mismo resultado.\n",
    "\n",
    "Nota: A menudo, en Machine Learning, la funci√≥n de p√©rdida optimizada durante el entrenamiento es diferente de la m√©trica de rendimiento final. Esto se hace porque la funci√≥n de p√©rdida es m√°s f√°cil de optimizar y puede incluir t√©rminos adicionales para regularizaci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La Ecuaci√≥n Normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para encontrar el valor de $Œ∏$ que minimiza el $MSE$, existe una soluci√≥n de forma cerrada, es decir, una ecuaci√≥n matem√°tica que da el resultado directamente. Esto se conoce como la ecuaci√≥n normal.\n",
    "\n",
    "$$\\hat{Œ∏} = (X^‚ä∫X)^{‚àí1}X^‚ä∫y$$\n",
    "\n",
    "En esta ecuaci√≥n:\n",
    "\n",
    "$\\hat{Œ∏}$ es el valor de $Œ∏$ que minimiza la funci√≥n de costo.\n",
    "$y$ es el vector de valores objetivo que contiene $y^{(1)}$ a $y^{(m)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerias\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.random.seed(42)\n",
    "m = 100 # n√∫mero de instancias\n",
    "X = 2 * np.random.rand(m, 1)  \n",
    "y = 4 + 3 * X + np.random.randn(m, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$y$\", rotation=0)\n",
    "plt.axis([0, 2, 0, 15])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a calcular $Œ∏$ utilizando la ecuaci√≥n Normal. Utilizaremos la funci√≥n inv() del m√≥dulo √°lgebra lineal de NumPy (np.linalg) para calcular la inversa de una matriz, y el m√©todo dot() para la multiplicaci√≥n de matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import add_dummy_feature\n",
    "\n",
    "X_b = add_dummy_feature(X)  # a√±ade x0 = 1 a cada instancia\n",
    "theta_best = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El operador @ realiza la multiplicaci√≥n de matrices. Si A y B son matrices NumPy, entonces A @ B es equivalente a np.matmul(A, B). Muchas otras bibliotecas, como TensorFlow, PyTorch y JAX, tambi√©n soportan el operador @. Sin embargo, no se puede utilizar @ en arrays Python puros (es decir, listas de listas)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La funci√≥n que hemos utilizado para generar los datos es $y = 4 + 3x1 + ruido \\space gaussiano$. Veamos lo que encontr√≥ la ecuaci√≥n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Habr√≠amos esperado $Œ∏_0 = 4$ y $Œ∏_1 = 3$ en lugar de $Œ∏_0 = 4,215$ y $Œ∏_1 = 2,770$. Se acercaba bastante, pero el ruido hac√≠a imposible recuperar los par√°metros exactos de la funci√≥n original. Cuanto m√°s peque√±o y ruidoso es el conjunto de datos, m√°s dif√≠cil resulta.\n",
    "Ahora podemos hacer predicciones utilizando $Œ∏$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.array([[0], [2]])\n",
    "X_new_b = add_dummy_feature(X_new)  # a√±ade x0 = 1 a cada instancia\n",
    "y_predict = X_new_b @ theta_best\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(6, 4))  \n",
    "plt.plot(X_new, y_predict, \"r-\", label=\"Predictions\")\n",
    "plt.plot(X, y, \"b.\")\n",
    "\n",
    "\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$y$\", rotation=0)\n",
    "plt.axis([0, 2, 0, 15])\n",
    "plt.grid()\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizar una regresi√≥n lineal con Scikit-Learn es relativamente sencillo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La clase `LinearRegression` se basa en la funci√≥n `scipy.linalg.lstsq()` (el nombre significa \"m√≠nimos cuadrados\"), que podr√≠as llamar directamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)\n",
    "theta_best_svd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta funci√≥n calcula $\\mathbf{X}^+\\mathbf{y}$, donde $\\mathbf{X}^{+}$ es la _pseudoinversa_ de $\\mathbf{X}$ (concretamente la inversa de Moore-Penrose). Puede utilizar `np.linalg.pinv()` para calcular la pseudoinversa directamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.pinv(X_b) @ y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El descenso de gradiente ajusta los par√°metros de un modelo para minimizar la funci√≥n de coste, como bajar una monta√±a buscando la pendiente m√°s pronunciada. La tasa de aprendizaje controla el tama√±o de los pasos: si es muy baja, el algoritmo ser√° lento; si es alta, puede no encontrar el m√≠nimo.\n",
    "\n",
    "La funci√≥n de coste en regresi√≥n lineal es convexa, lo que garantiza un √∫nico m√≠nimo global que el descenso de gradiente puede alcanzar si se configura bien. Es importante escalar las caracter√≠sticas para que el algoritmo converja m√°s r√°pido.\n",
    "\n",
    "Entrenar un modelo significa encontrar los par√°metros que minimicen la funci√≥n de coste. En regresi√≥n lineal, esto es m√°s sencillo gracias a la convexidad de la funci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para implementar el descenso de gradiente, necesitas calcular el gradiente de la funci√≥n de costo respecto a cada par√°metro $Œ∏_j$. Esto te indica c√≥mo cambia la funci√≥n de costo cuando ajustas $Œ∏_j$ ligeramente. La f√≥rmula para la derivada parcial del $MSE$ respecto a $Œ∏_j$ es:\n",
    "\n",
    "$$\\frac{‚àÇ}{‚àÇŒ∏_j} MSE(Œ∏) = \\frac{2}{m} \\sum_{i=1}^{m} (Œ∏^‚ä∫x^i - y^i) x_{j}^i$$\n",
    "\n",
    "El vector gradiente (que contiene todas las derivadas parciales) se calcula como:\n",
    "\n",
    "$$‚àá_Œ∏MSE(Œ∏) = \\begin{bmatrix} \\frac{‚àÇ}{‚àÇŒ∏_0} MSE(Œ∏) \\\\ \\frac{‚àÇ}{‚àÇŒ∏_1} MSE(Œ∏) \\\\ ‚ãÆ \\\\ \\frac{‚àÇ}{‚àÇŒ∏_n} MSE(Œ∏) \\end{bmatrix} = \\frac{2}{m} X^‚ä∫(XŒ∏ - y) $$\n",
    "\n",
    "ste c√°lculo se realiza sobre todo el conjunto de entrenamiento en cada paso, por eso se llama descenso de gradiente por lotes. Es m√°s lento en grandes conjuntos de datos, pero escala bien con muchas caracter√≠sticas.\n",
    "\n",
    "Para actualizar $Œ∏$ y moverte cuesta abajo, restas el gradiente multiplicado por la tasa de aprendizaje $Œ∑$:\n",
    "\n",
    "$$Œ∏^{(next step)} = Œ∏ ‚àí Œ∑‚àá_Œ∏ MSE(Œ∏)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.1  # ratio de aprendizaje\n",
    "n_epochs = 1000\n",
    "m = len(X_b)  # n√∫mero de instancias\n",
    "\n",
    "np.random.seed(42)\n",
    "theta = np.random.randn(2, 1)  # par√°metros del modelo inicializados aleatoriamente\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    gradients = 2 / m * X_b.T @ (X_b @ theta - y)\n",
    "    theta = theta - eta * gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No ha sido tan dif√≠cil. Cada iteraci√≥n sobre el conjunto de entrenamiento se llama epoch. Veamos el theta resultante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¬°Eso es exactamente lo que encontr√≥ la ecuaci√≥n Normal! El descenso del Gradiente ha funcionado perfectamente. Pero, ¬øy si hubieras utilizado una tasa de aprendizaje (eta) diferente? La figura muestra los 20 primeros pasos del descenso del Gradiente utilizando tres tasas de aprendizaje diferentes. La l√≠nea de la parte inferior de cada gr√°fico representa el punto de partida aleatorio y, a continuaci√≥n, cada √©poca est√° representada por una l√≠nea cada vez m√°s oscura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "def plot_gradient_descent(theta, eta):\n",
    "    m = len(X_b)\n",
    "    plt.plot(X, y, \"b.\")\n",
    "    n_epochs = 1000\n",
    "    n_shown = 20\n",
    "    theta_path = []\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch < n_shown:\n",
    "            y_predict = X_new_b @ theta\n",
    "            color = mpl.colors.rgb2hex(plt.cm.OrRd(epoch / n_shown + 0.15))\n",
    "            plt.plot(X_new, y_predict, linestyle=\"solid\", color=color)\n",
    "        gradients = 2 / m * X_b.T @ (X_b @ theta - y)\n",
    "        theta = theta - eta * gradients\n",
    "        theta_path.append(theta)\n",
    "    plt.xlabel(\"$x_1$\")\n",
    "    plt.axis([0, 2, 0, 15])\n",
    "    plt.grid()\n",
    "    plt.title(fr\"$\\eta = {eta}$\")\n",
    "    return theta_path\n",
    "\n",
    "np.random.seed(42)\n",
    "theta = np.random.randn(2, 1) # inicializaci√≥n aleatoria\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(131)\n",
    "plot_gradient_descent(theta, eta=0.02)\n",
    "plt.ylabel(\"$y$\", rotation=0)\n",
    "plt.subplot(132)\n",
    "theta_path_bgd = plot_gradient_descent(theta, eta=0.1)\n",
    "plt.gca().axes.yaxis.set_ticklabels([])\n",
    "plt.subplot(133)\n",
    "plt.gca().axes.yaxis.set_ticklabels([])\n",
    "plot_gradient_descent(theta, eta=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A la izquierda, la tasa de aprendizaje es demasiado baja: el algoritmo eventualmente llegar√° a la soluci√≥n, pero tardar√° mucho. En el centro, la tasa de aprendizaje es adecuada: converge r√°pidamente en pocas √©pocas. A la derecha, la tasa es demasiado alta: el algoritmo diverge, saltando y alej√°ndose de la soluci√≥n en cada paso.\n",
    "\n",
    "Para encontrar una buena tasa de aprendizaje, puedes usar gridsearch, pero limita el n√∫mero de √©pocas para evitar que los modelos que tardan mucho en converger sean seleccionados. El n√∫mero de √©pocas debe ser lo suficientemente alto para que el algoritmo se acerque a la soluci√≥n, pero no tanto como para perder tiempo cuando ya no hay cambios significativos.\n",
    "\n",
    "Una soluci√≥n sencilla es establecer un n√∫mero grande de √©pocas y detener el algoritmo cuando el vector gradiente sea peque√±o, es decir, cuando su norma sea menor que un peque√±o n√∫mero $œµ$ (tolerancia), lo que indica que el algoritmo ha casi alcanzado el m√≠nimo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El descenso de gradiente por lotes utiliza todo el conjunto de entrenamiento para calcular los gradientes en cada paso, lo que lo hace lento con grandes conjuntos de datos. En contraste, el descenso de gradiente estoc√°stico elige una instancia aleatoria por paso, calculando los gradientes r√°pidamente con esa √∫nica instancia. Esto lo hace mucho m√°s r√°pido y permite entrenar con conjuntos de datos enormes.\n",
    "\n",
    "Sin embargo, debido a su aleatoriedad, el descenso de gradiente estoc√°stico es menos regular: en lugar de bajar suavemente hacia el m√≠nimo, la funci√≥n de coste oscila, acerc√°ndose al m√≠nimo pero sin asentarse completamente. Esto puede ser ventajoso para escapar de m√≠nimos locales y acercarse al m√≠nimo global.\n",
    "\n",
    "Para resolver el problema de la aleatoriedad, se puede reducir gradualmente la tasa de aprendizaje: comenzando con pasos grandes para un progreso r√°pido y luego haciendo los pasos m√°s peque√±os para que el algoritmo se asiente en el m√≠nimo global. Este enfoque es similar al recocido simulado, donde la tasa de aprendizaje (llamada programa de aprendizaje) se reduce lentamente. Si la tasa se reduce demasiado r√°pido, podr√≠as quedar atrapado en un m√≠nimo local; si se reduce demasiado lento, podr√≠as rebotar alrededor del m√≠nimo por mucho tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_path_sgd = []  # necesitamos almacenar la trayectoria de theta en el\n",
    "                     # espacio de par√°metros para trazar la siguiente figura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "t0, t1 = 5, 50  # hiperpar√°metros del programa de aprendizaje\n",
    "\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + t1)\n",
    "\n",
    "np.random.seed(42)\n",
    "theta = np.random.randn(2, 1)  \n",
    "\n",
    "n_shown = 20  \n",
    "plt.figure(figsize=(6, 4)) \n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for iteration in range(m):\n",
    "\n",
    "        if epoch == 0 and iteration < n_shown:\n",
    "            y_predict = X_new_b @ theta\n",
    "            color = mpl.colors.rgb2hex(plt.cm.OrRd(iteration / n_shown + 0.15))\n",
    "            plt.plot(X_new, y_predict, color=color)\n",
    "\n",
    "        random_index = np.random.randint(m)\n",
    "        xi = X_b[random_index : random_index + 1]\n",
    "        yi = y[random_index : random_index + 1]\n",
    "        gradients = 2 * xi.T @ (xi @ theta - yi)  # para SGD, no dividir por m\n",
    "        eta = learning_schedule(epoch * m + iteration)\n",
    "        theta = theta - eta * gradients\n",
    "        theta_path_sgd.append(theta) \n",
    "\n",
    "\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$y$\", rotation=0)\n",
    "plt.axis([0, 2, 0, 15])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por convenci√≥n, iteramos por rondas de $m$ iteraciones; cada ronda se denomina √©poca, como antes. Mientras que el c√≥digo del descenso del Gradiente por lotes itera 1.000 veces a trav√©s de todo el conjunto de entrenamiento, este c√≥digo lo hace s√≥lo 50 veces y alcanza una soluci√≥n bastante buena:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el descenso de gradiente estoc√°stico (SGD), las instancias se eligen aleatoriamente en cada √©poca, lo que significa que algunas pueden ser seleccionadas varias veces, mientras que otras podr√≠an no ser elegidas. Para asegurarte de que el algoritmo pasa por todas las instancias en cada √©poca, podr√≠as barajar el conjunto de entrenamiento antes de cada iteraci√≥n. Sin embargo, este enfoque es m√°s complejo y no suele mejorar significativamente los resultados.\n",
    "\n",
    "Para realizar una regresi√≥n lineal con SGD en Scikit-Learn, puedes usar la clase SGDRegressor, que optimiza la funci√≥n de coste MSE por defecto. Puedes configurarlo para ejecutarse hasta un m√°ximo de 1,000 √©pocas (max_iter) o hasta que la p√©rdida disminuya menos de $10^{-5}$ (tol) durante 100 √©pocas. Comienza con una tasa de aprendizaje de 0.01 (eta0), utilizando el programa de aprendizaje predeterminado, y sin regularizaci√≥n (penalty=None).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "sgd_reg = SGDRegressor(max_iter=1000, tol=1e-5, penalty=None, eta0=0.01,\n",
    "                       n_iter_no_change=100, random_state=42)\n",
    "sgd_reg.fit(X, y.ravel())  # y.ravel() because fit() expects 1D targets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez m√°s, se encuentra una soluci√≥n bastante cercana a la que devuelve la ecuaci√≥n Normal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_reg.intercept_, sgd_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todos los estimadores de Scikit-Learn pueden entrenarse usando el m√©todo fit(), pero algunos tambi√©n tienen un m√©todo partial_fit(), que permite entrenar el modelo en una o m√°s instancias de forma incremental. Al llamar repetidamente a partial_fit(), se entrena gradualmente el modelo, lo cual es √∫til para tener m√°s control sobre el proceso.\n",
    "\n",
    "Algunos modelos tambi√©n tienen el hiperpar√°metro warm_start. Si warm_start=True, llamar a fit() en un modelo entrenado continuar√° el entrenamiento desde donde se dej√≥, en lugar de reiniciarlo. Esto respeta hiperpar√°metros como max_iter y tol. Sin embargo, ten en cuenta que fit() reinicia el contador de iteraciones del programa de aprendizaje, mientras que partial_fit() no lo hace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-batch gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El descenso de gradiente por mini lotes es un punto intermedio entre el descenso de gradiente por lotes y el estoc√°stico. En lugar de calcular los gradientes usando todo el conjunto de entrenamiento (como en el descenso por lotes) o solo una instancia (como en el estoc√°stico), el descenso por mini lotes usa peque√±os conjuntos aleatorios de instancias, llamados minilotes.\n",
    "\n",
    "Ventajas:\n",
    "\n",
    "- Mejora el rendimiento gracias a la optimizaci√≥n por hardware, especialmente con GPU.\n",
    "- El progreso es menos err√°tico que con el descenso estoc√°stico, lo que lo acerca m√°s al m√≠nimo.\n",
    "\n",
    "Desventajas:\n",
    "\n",
    "- Puede ser m√°s dif√≠cil escapar de m√≠nimos locales en problemas complejos.\n",
    "\n",
    "En resumen, el descenso por mini lotes combina la estabilidad del descenso por lotes con la velocidad del estoc√°stico, aunque requiere un buen programa de aprendizaje para acercarse al m√≠nimo global."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from math import ceil\n",
    "\n",
    "n_epochs = 50\n",
    "minibatch_size = 20\n",
    "n_batches_per_epoch = ceil(m / minibatch_size)\n",
    "\n",
    "np.random.seed(42)\n",
    "theta = np.random.randn(2, 1)  # inicializaci√≥n aleatoria\n",
    "\n",
    "t0, t1 = 200, 1000  # hiperpar√°metros del programa de aprendizaje\n",
    "\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + t1)\n",
    "\n",
    "theta_path_mgd = []\n",
    "for epoch in range(n_epochs):\n",
    "    shuffled_indices = np.random.permutation(m)\n",
    "    X_b_shuffled = X_b[shuffled_indices]\n",
    "    y_shuffled = y[shuffled_indices]\n",
    "    for iteration in range(0, n_batches_per_epoch):\n",
    "        idx = iteration * minibatch_size\n",
    "        xi = X_b_shuffled[idx : idx + minibatch_size]\n",
    "        yi = y_shuffled[idx : idx + minibatch_size]\n",
    "        gradients = 2 / minibatch_size * xi.T @ (xi @ theta - yi)\n",
    "        eta = learning_schedule(iteration)\n",
    "        theta = theta - eta * gradients\n",
    "        theta_path_mgd.append(theta)\n",
    "\n",
    "theta_path_bgd = np.array(theta_path_bgd)\n",
    "theta_path_sgd = np.array(theta_path_sgd)\n",
    "theta_path_mgd = np.array(theta_path_mgd)\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(theta_path_sgd[:, 0], theta_path_sgd[:, 1], \"r-s\", linewidth=1,\n",
    "         label=\"Stochastic\")\n",
    "plt.plot(theta_path_mgd[:, 0], theta_path_mgd[:, 1], \"g-+\", linewidth=2,\n",
    "         label=\"Mini-batch\")\n",
    "plt.plot(theta_path_bgd[:, 0], theta_path_bgd[:, 1], \"b-o\", linewidth=3,\n",
    "         label=\"Batch\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(r\"$\\theta_0$\")\n",
    "plt.ylabel(r\"$\\theta_1$   \", rotation=0)\n",
    "plt.axis([2.6, 4.6, 2.3, 3.4])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compara los algoritmos que hemos analizado hasta ahora para la regresi√≥n lineal (recuerde que m es el n√∫mero de instancias de entrenamiento y n es el n√∫mero de caracter√≠sticas)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Algoritmo| Gran m |Soporte fuera de n√∫cleo| Gran n| Hiperpar√°metros| Escalado necesario| Scikit-Learn|\n",
    "|---|---|---|---|---|---|---|\n",
    "|Ecuaci√≥n normal| R√°pido| No| Lento| 0 |No| N/A|\n",
    "|SVD |R√°pido |No |Lento| 0 |No |LinearRegression|\n",
    "|Batch GD| Lento| No| R√°pido| 2| S√≠| N/A|\n",
    "|Estoc√°stico GD| R√°pido| S√≠| R√°pido| ‚â•2| S√≠| SGDRegressor|\n",
    "|Mini-batch GD| R√°pido| S√≠| R√°pido| ‚â•2| S√≠| N/A||"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Casi no hay diferencias despu√©s del entrenamiento: todos estos algoritmos acaban con modelos muy similares y hacen predicciones exactamente de la misma manera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresi√≥n polin√≥mica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¬øY si los datos son m√°s complejos que una l√≠nea recta? Sorprendentemente, puede utilizar un modelo lineal para ajustar datos no lineales. Una forma sencilla de hacerlo es a√±adir potencias de cada caracter√≠stica como nuevas caracter√≠sticas y, a continuaci√≥n, entrenar un modelo lineal en este conjunto ampliado de caracter√≠sticas. Esta t√©cnica se denomina regresi√≥n polin√≥mica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos un ejemplo. Primero, generaremos algunos datos no lineales, basados en una simple ecuaci√≥n cuadr√°tica, es decir, una ecuaci√≥n de la forma $ y = ax^2 + bx + c $, m√°s algo de ruido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "m = 100\n",
    "X = 6 * np.random.rand(m, 1) - 3\n",
    "y = 0.5 * X ** 2 + X + 2 + np.random.randn(m, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$y$\", rotation=0)\n",
    "plt.axis([-3, 3, 0, 10])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Est√° claro que una l√≠nea recta nunca se ajustar√° correctamente a estos datos. As√≠ que vamos a utilizar la clase PolynomialFeatures de Scikit-Learn para transformar nuestros datos de entrenamiento, a√±adiendo el cuadrado (polinomio de segundo grado) de cada caracter√≠stica en el conjunto de entrenamiento como una nueva caracter√≠stica (en este caso s√≥lo hay una caracter√≠stica):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_poly[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_poly contiene ahora la caracter√≠stica original de X m√°s el cuadrado de esta caracter√≠stica. Ahora podemos ajustar un modelo de regresi√≥n lineal a estos datos de entrenamiento ampliados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_poly, y)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_new = np.linspace(-3, 3, 100).reshape(100, 1)\n",
    "X_new_poly = poly_features.transform(X_new)\n",
    "y_new = lin_reg.predict(X_new_poly)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.plot(X_new, y_new, \"r-\", linewidth=2, label=\"Predictions\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$y$\", rotation=0)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.axis([-3, 3, 0, 10])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No est√° mal: el modelo estima $ y = 0.56x1^2 + 0.93x1 + 1.78 $ cuando de hecho la funci√≥n original era $ y = 0.5x1^2 + 1.0x1 + 2.0 $ m√°s ruido Gaussiano. \n",
    "\n",
    "Ten en cuenta que cuando hay m√∫ltiples caracter√≠sticas, la regresi√≥n polin√≥mica es capaz de encontrar relaciones entre caracter√≠sticas, lo cual es algo que un modelo de regresi√≥n lineal simple no puede hacer. Esto es posible gracias a que PolynomialFeatures tambi√©n agrega todas las combinaciones de caracter√≠sticas hasta el grado dado. Por ejemplo, si hubiera dos caracter√≠sticas a y b, PolynomialFeatures con degree=3 no solo agregar√≠a las caracter√≠sticas a¬≤, a¬≥, b¬≤ y b¬≥, sino tambi√©n las combinaciones ab, a¬≤b y ab¬≤.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¬°PolynomialFeatures(degree=d) transforma una matriz que contiene n caracter√≠sticas en una matriz que contiene $(n + d)! / d!n!$ es el factorial de n, igual a $1 √ó 2 √ó 3 √ó ‚ãØ √ó n$. ¬°Cuidado con la explosi√≥n combinatoria del n√∫mero de caracter√≠sticas!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curvas de aprendizaje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si realiza una regresi√≥n polin√≥mica de alto grado, probablemente ajustar√° los datos de entrenamiento mucho mejor que con una regresi√≥n lineal pura. Por ejemplo, la Figura aplica un modelo polin√≥mico de 300 grados a los datos de entrenamiento anteriores y compara el resultado con un modelo lineal puro y un modelo cuadr√°tico (polin√≥mico de segundo grado). Observe c√≥mo el modelo polin√≥mico de 300 grados se contonea para acercarse lo m√°s posible a las instancias de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "for style, width, degree in ((\"r-+\", 2, 1), (\"b--\", 2, 2), (\"g-\", 1, 300)):\n",
    "    polybig_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    std_scaler = StandardScaler()\n",
    "    lin_reg = LinearRegression()\n",
    "    polynomial_regression = make_pipeline(polybig_features, std_scaler, lin_reg)\n",
    "    polynomial_regression.fit(X, y)\n",
    "    y_newbig = polynomial_regression.predict(X_new)\n",
    "    label = f\"{degree} degree{'s' if degree > 1 else ''}\"\n",
    "    plt.plot(X_new, y_newbig, style, label=label, linewidth=width)\n",
    "\n",
    "plt.plot(X, y, \"b.\", linewidth=3)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$y$\", rotation=0)\n",
    "plt.axis([-3, 3, 0, 10])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo de regresi√≥n polin√≥mica de alto grado tiende a sobrefit (sobreajustar) los datos de entrenamiento, mientras que el modelo lineal tiende a underfit (subajustar). El modelo cuadr√°tico generalmente generaliza mejor en este caso, ya que los datos se generaron con una funci√≥n cuadr√°tica. Sin embargo, en la pr√°ctica, no siempre sabr√°s cu√°l es la funci√≥n generadora de los datos, por lo que necesitas herramientas para decidir la complejidad adecuada de tu modelo.\n",
    "\n",
    "Cross-Validation se utiliza para estimar el rendimiento de generalizaci√≥n de un modelo. Si el modelo rinde bien en los datos de entrenamiento pero mal en validaci√≥n, est√° sobreajustando. Si rinde mal en ambos, est√° subajustando.\n",
    "\n",
    "Otra herramienta √∫til son las curvas de aprendizaje, que muestran el error de entrenamiento y validaci√≥n a lo largo de las iteraciones de entrenamiento. Si el modelo no permite entrenamiento incremental, puedes entrenarlo varias veces en subconjuntos crecientes del conjunto de entrenamiento.\n",
    "\n",
    "Scikit-Learn ofrece la funci√≥n learning_curve(), que entrena y eval√∫a el modelo usando Cross-Validation, generando curvas de aprendizaje. Esta funci√≥n puede reentrenar el modelo en subconjuntos crecientes del conjunto de entrenamiento, o entrenarlo de manera incremental si el modelo lo permite, configurando exploit_incremental_learning=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "train_sizes, train_scores, valid_scores = learning_curve(\n",
    "    LinearRegression(), X, y, train_sizes=np.linspace(0.01, 1.0, 40), cv=5,\n",
    "    scoring=\"neg_root_mean_squared_error\")\n",
    "train_errors = -train_scores.mean(axis=1)\n",
    "valid_errors = -valid_scores.mean(axis=1)\n",
    "\n",
    "plt.figure(figsize=(6, 4))  # extra code ‚Äì not needed, just formatting\n",
    "plt.plot(train_sizes, train_errors, \"r-+\", linewidth=2, label=\"train\")\n",
    "plt.plot(train_sizes, valid_errors, \"b-\", linewidth=3, label=\"valid\")\n",
    "\n",
    "plt.xlabel(\"Training set size\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.grid()\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.axis([0, 80, 0, 2.5])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este modelo est√° subajustado. Veamos por qu√©:\n",
    "\n",
    "1. Error de entrenamiento: Cuando el conjunto de entrenamiento es peque√±o, el modelo puede ajustarse perfectamente a los pocos datos disponibles, por lo que la curva de error comienza en cero. A medida que se a√±aden m√°s datos, el modelo no puede ajustarse perfectamente debido al ruido y a la naturaleza no lineal de los datos, por lo que el error de entrenamiento aumenta y luego se estabiliza en una meseta.\n",
    "\n",
    "2. Error de validaci√≥n: Con pocas instancias de entrenamiento, el modelo no generaliza bien, lo que hace que el error de validaci√≥n sea alto inicialmente. A medida que se a√±aden m√°s ejemplos, el modelo mejora y el error de validaci√≥n disminuye, pero luego tambi√©n se estabiliza en una meseta, cerca del error de entrenamiento.\n",
    "\n",
    "Estas curvas de aprendizaje son t√≠picas de un modelo subajustado: ambas curvas est√°n altas, cercanas entre s√≠, y se han estabilizado. En este caso, agregar m√°s datos de entrenamiento no mejorar√° el rendimiento. La soluci√≥n es usar un modelo m√°s complejo o mejorar las caracter√≠sticas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos ahora las curvas de aprendizaje de un modelo polin√≥mico de 10¬∫ grado sobre los mismos datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "polynomial_regression = make_pipeline(\n",
    "    PolynomialFeatures(degree=10, include_bias=False),\n",
    "    LinearRegression())\n",
    "\n",
    "train_sizes, train_scores, valid_scores = learning_curve(\n",
    "    polynomial_regression, X, y, train_sizes=np.linspace(0.01, 1.0, 40), cv=5,\n",
    "    scoring=\"neg_root_mean_squared_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_errors = -train_scores.mean(axis=1)\n",
    "valid_errors = -valid_scores.mean(axis=1)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(train_sizes, train_errors, \"r-+\", linewidth=2, label=\"train\")\n",
    "plt.plot(train_sizes, valid_errors, \"b-\", linewidth=3, label=\"valid\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel(\"Training set size\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.grid()\n",
    "plt.axis([0, 80, 0, 2.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estas curvas de aprendizaje se parecen un poco a las anteriores, pero hay dos diferencias muy importantes:\n",
    "- El error en los datos de entrenamiento es mucho menor que antes.\n",
    "- Hay un hueco entre las curvas. Esto significa que el modelo funciona mucho mejor con los datos de entrenamiento que con los datos de validaci√≥n, lo que caracteriza a un modelo sobreajustado. Sin embargo, si se utilizara un conjunto de datos de entrenamiento mucho mayor, las dos curvas seguir√≠an acerc√°ndose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una forma de mejorar un modelo sobreajustado es introducir m√°s datos de entrenamiento hasta que el error de validaci√≥n alcance el error de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El equilibrio entre sesgo y varianza\n",
    "\n",
    "El error de generalizaci√≥n de un modelo se descompone en tres componentes:\n",
    "\n",
    "- Sesgo: Error causado por suposiciones incorrectas, como suponer que los datos son lineales cuando en realidad no lo son. Un modelo con alto sesgo no se ajusta bien a los datos de entrenamiento.\n",
    "\n",
    "- Varianza: Error debido a la sensibilidad excesiva del modelo a peque√±as variaciones en los datos. Un modelo muy complejo, como un polin√≥mico de alto grado, tiende a tener alta varianza y sobreajusta los datos de entrenamiento.\n",
    "\n",
    "- Error irreductible: Error inherente al ruido en los datos. La √∫nica manera de reducirlo es limpiando los datos.\n",
    "\n",
    "Aumentar la complejidad de un modelo generalmente reduce el sesgo pero aumenta la varianza, y viceversa. Este es el equilibrio entre sesgo y varianza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos lineales regularizados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una buena forma de reducir el sobreajuste es regularizar el modelo (es decir, restringirlo): cuantos menos grados de libertad tenga, m√°s dif√≠cil ser√° que sobreajuste los datos. Una forma sencilla de regularizar un modelo polin√≥mico es reducir el n√∫mero de grados polin√≥micos.\n",
    "\n",
    "Para un modelo lineal, la regularizaci√≥n se consigue normalmente restringiendo los pesos del modelo. Ahora veremos la regresi√≥n de Ridge, la regresi√≥n de Lasso y la regresi√≥n de ElasticNet, que implementan tres formas diferentes de restringir los pesos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresi√≥n Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es una versi√≥n regularizada de la regresi√≥n lineal. A√±ade un t√©rmino de regularizaci√≥n al MSE:\n",
    "$ J(Œ∏) = MSE(Œ∏) + \\frac{Œ±}{m} \\sum_{i=1}^{n} Œ∏_{i}^2 $\n",
    "\n",
    "Este t√©rmino de regularizaci√≥n obliga al modelo a mantener los pesos $Œ∏_i$ lo m√°s peque√±os posible, lo que ayuda a prevenir el sobreajuste. El hiperpar√°metro $Œ±$ controla la intensidad de esta regularizaci√≥n:\n",
    "\n",
    "- Si $Œ± = 0$, la regresi√≥n Ridge es equivalente a la regresi√≥n lineal.\n",
    "- Si $Œ±$ es grande, los pesos se acercan a cero, resultando en una l√≠nea casi plana.\n",
    "\n",
    "Importante: El t√©rmino de regularizaci√≥n solo se a√±ade durante el entrenamiento, no al evaluar el modelo.\n",
    "\n",
    "Para descenso de gradiente por lotes, solo se necesita a√±adir $\\frac{2Œ±w}{m}$ al gradiente de los pesos, sin modificar el gradiente del t√©rmino de sesgo $Œ∏_0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es importante escalar los datos (por ejemplo, utilizando un StandardScaler) antes de realizar la regresi√≥n ridge, ya que es sensible a la escala de las caracter√≠sticas de entrada. Esto ocurre con la mayor√≠a de los modelos regularizados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generemos un conjunto de datos lineales muy peque√±o y ruidoso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "m = 20\n",
    "X = 3 * np.random.rand(m, 1)\n",
    "y = 1 + 0.5 * X + np.random.randn(m, 1) / 1.5\n",
    "X_new = np.linspace(0, 3, 100).reshape(100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(X, y, \".\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$y$  \", rotation=0)\n",
    "plt.axis([0, 3, 0, 3.5])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge_reg = Ridge(alpha=0.1, solver=\"cholesky\")\n",
    "ridge_reg.fit(X, y)\n",
    "ridge_reg.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_model(model_class, polynomial, alphas, **model_kwargs):\n",
    "    plt.plot(X, y, \"b.\", linewidth=3)\n",
    "    for alpha, style in zip(alphas, (\"b:\", \"g--\", \"r-\")):\n",
    "        if alpha > 0:\n",
    "            model = model_class(alpha, **model_kwargs)\n",
    "        else:\n",
    "            model = LinearRegression()\n",
    "        if polynomial:\n",
    "            model = make_pipeline(\n",
    "                PolynomialFeatures(degree=10, include_bias=False),\n",
    "                StandardScaler(),\n",
    "                model)\n",
    "        model.fit(X, y)\n",
    "        y_new_regul = model.predict(X_new)\n",
    "        plt.plot(X_new, y_new_regul, style, linewidth=2,\n",
    "                 label=fr\"$\\alpha = {alpha}$\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.xlabel(\"$x_1$\")\n",
    "    plt.axis([0, 3, 0, 3.5])\n",
    "    plt.grid()\n",
    "\n",
    "plt.figure(figsize=(9, 3.5))\n",
    "plt.subplot(121)\n",
    "plot_model(Ridge, polynomial=False, alphas=(0, 10, 100), random_state=42)\n",
    "plt.ylabel(\"$y$  \", rotation=0)\n",
    "plt.subplot(122)\n",
    "plot_model(Ridge, polynomial=True, alphas=(0, 10**-5, 1), random_state=42)\n",
    "plt.gca().axes.yaxis.set_ticklabels([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La regresi√≥n Ridge ajusta modelos lineales para controlar el sobreajuste mediante regularizaci√≥n. En la figura mencionada:\n",
    "\n",
    "- Izquierda: Se usan modelos Ridge simples, lo que da predicciones lineales.\n",
    "\n",
    "- Derecha: Los datos se expanden a caracter√≠sticas polin√≥micas de grado 10, se escalan, y luego se aplica la regresi√≥n Ridge. A medida que el valor de $Œ±$ aumenta, las predicciones se vuelven m√°s planas, reduciendo la varianza pero aumentando el sesgo.\n",
    "\n",
    "Al igual que con la regresi√≥n lineal, la regresi√≥n Ridge puede resolverse de dos maneras:\n",
    "\n",
    "1. Ecuaci√≥n de forma cerrada: Proporciona una soluci√≥n exacta.\n",
    "\n",
    "2. Descenso de gradiente: Aproxima la soluci√≥n iterativamente.\n",
    "\n",
    "La soluci√≥n de forma cerrada para la regresi√≥n Ridge es:\n",
    "\n",
    "$ Œ∏ = (X^‚ä∫X + Œ±A)^{‚àí1}X^‚ä∫y $\n",
    "\n",
    "Donde $A$ es la matriz identidad con un 0 en la celda correspondiente al t√©rmino de sesgo.\n",
    "\n",
    "En Scikit-Learn, puedes realizar la regresi√≥n Ridge utilizando una variante de esta ecuaci√≥n con la t√©cnica de factorizaci√≥n de matrices de Cholesky.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostrar que obtenemos aproximadamente la misma soluci√≥n que antes cuando\n",
    "# utilizamos el promedio estoc√°stico GD (solver=\"sag\")\n",
    "ridge_reg = Ridge(alpha=0.1, solver=\"sag\", random_state=42)\n",
    "ridge_reg.fit(X, y)\n",
    "ridge_reg.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y utilizando el descenso de gradiente estoc√°stico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_reg = SGDRegressor(penalty=\"l2\", alpha=0.1 / m, tol=None,\n",
    "                       max_iter=1000, eta0=0.01, random_state=42)\n",
    "sgd_reg.fit(X, y.ravel())  # y.ravel() porque fit() espera objetivos 1D\n",
    "sgd_reg.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El hiperpar√°metro de penalizaci√≥n en SGD determina el tipo de regularizaci√≥n a aplicar. Especificar \"l2\" a√±ade un t√©rmino de regularizaci√≥n a la funci√≥n de coste MSE, igual a $Œ±$ veces el cuadrado de la norma $‚Ñì2$ del vector de pesos. Esto es similar a la regresi√≥n Ridge, pero sin la divisi√≥n por $m$. Por eso, se pasa alpha=0.1 / m para obtener un resultado equivalente a Ridge(alpha=0.1).\n",
    "\n",
    "La clase RidgeCV tambi√©n realiza regresi√≥n Ridge, pero ajusta autom√°ticamente el hiperpar√°metro $Œ±$ mediante Cross-Validation. Es similar a usar GridSearchCV, pero est√° optimizado para la regresi√≥n Ridge, lo que lo hace m√°s r√°pido. Otros estimadores lineales, como LassoCV y ElasticNetCV, tambi√©n tienen variantes con Cross-Validation eficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# muestra la soluci√≥n de forma cerrada de la regresi√≥n Ridge,\n",
    "# comparar con los par√°metros aprendidos del modelo Ridge\n",
    "alpha = 0.1\n",
    "A = np.array([[0., 0.], [0., 1.]])\n",
    "X_b = np.c_[np.ones(m), X]\n",
    "np.linalg.inv(X_b.T @ X_b + alpha * A) @ X_b.T @ y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_reg.intercept_, ridge_reg.coef_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresi√≥n Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La regresi√≥n Lasso (Least Absolute Shrinkage and Selection Operator) es otra versi√≥n regularizada de la regresi√≥n lineal. Similar a la regresi√≥n Ridge, a√±ade un t√©rmino de regularizaci√≥n a la funci√≥n de coste, pero utiliza la norma $‚Ñì1$ del vector de pesos en lugar del cuadrado de la norma $‚Ñì2$.\n",
    "\n",
    "En la regresi√≥n Lasso, la norma $‚Ñì1$ se multiplica por $2Œ±$, mientras que en la regresi√≥n Ridge, la norma $‚Ñì2$ se multiplica por $\\frac{Œ±}{m}$. Estos factores se eligen para asegurar que el valor √≥ptimo de $Œ±$ sea independiente del tama√±o del conjunto de entrenamiento, ya que diferentes normas requieren diferentes factores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La funci√≥n de coste de la regresi√≥n Lasso se define como:\n",
    "\n",
    "$$J(\\theta) = MSE(\\theta) + 2\\alpha \\sum_{i=1}^{n} |\\theta_i| $$\n",
    "\n",
    "Donde:\n",
    "\n",
    "- $J(\\theta)$ es la funci√≥n de coste total.\n",
    "- $\\text{MSE}(\\theta)$ es el error cuadr√°tico medio (Mean Squared Error).\n",
    "- $2\\alpha \\sum_{i=1}^{n} |\\theta_i|$ es el t√©rmino de regularizaci√≥n Lasso, donde $|\\theta_i|$ es la norma $‚Ñì1$ de los pesos.\n",
    "\n",
    "Este t√©rmino de regularizaci√≥n ayuda a reducir la magnitud de los pesos y puede llevar a que algunos de ellos sean exactamente cero, lo que efectivamente selecciona caracter√≠sticas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "plt.figure(figsize=(9, 3.5))\n",
    "plt.subplot(121)\n",
    "plot_model(Lasso, polynomial=False, alphas=(0, 0.1, 1), random_state=42)\n",
    "plt.ylabel(\"$y$  \", rotation=0)\n",
    "plt.subplot(122)\n",
    "plot_model(Lasso, polynomial=True, alphas=(0, 1e-2, 1), random_state=42)\n",
    "plt.gca().axes.yaxis.set_ticklabels([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tiene la notable capacidad de eliminar los pesos de las caracter√≠sticas menos importantes, es decir, los ajusta a cero. Esto significa que Lasso realiza autom√°ticamente la selecci√≥n de caracter√≠sticas, produciendo un modelo m√°s simple y con menos caracter√≠sticas relevantes.\n",
    "\n",
    "Por ejemplo, en el gr√°fico mencionado (Figura 4-18), con $Œ± = 0.01$, el modelo resultante es aproximadamente c√∫bico, ya que las caracter√≠sticas polin√≥micas de alto grado han sido eliminadas (sus pesos son cero).\n",
    "\n",
    "Esto ocurre debido a c√≥mo funciona la regularizaci√≥n $‚Ñì1$. En el gr√°fico superior izquierdo de la Figura 4-19, los contornos representan la p√©rdida $‚Ñì1$ ($|Œ∏1| + |Œ∏2|$), que decrece linealmente hacia los ejes. Cuando se aplica el descenso de gradiente, los par√°metros tienden a reducirse, y aquellos que est√°n m√°s cerca de cero llegan a cero primero, lo que crea un \"canal√≥n\" que luego lleva el modelo a eliminar ciertos par√°metros.\n",
    "\n",
    "En el gr√°fico superior derecho, los contornos representan la funci√≥n de coste de la regresi√≥n Lasso (MSE m√°s p√©rdida $‚Ñì1$). A medida que el descenso de gradiente ajusta los par√°metros, uno de ellos llega a cero r√°pidamente, y el algoritmo sigue un camino que lleva al √≥ptimo global, aunque con cierto \"rebote\". Si se aumenta $Œ±$, el √≥ptimo global se mueve hacia valores m√°s peque√±os de los par√°metros, eliminando m√°s caracter√≠sticas; si se disminuye $Œ±$, el modelo retiene m√°s caracter√≠sticas.\n",
    "\n",
    "En resumen, la regresi√≥n Lasso no solo ajusta el modelo a los datos, sino que tambi√©n simplifica el modelo al eliminar caracter√≠sticas innecesarias, logrando un equilibrio entre ajuste y simplicidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "t1a, t1b, t2a, t2b = -1, 3, -1.5, 1.5\n",
    "\n",
    "t1s = np.linspace(t1a, t1b, 500)\n",
    "t2s = np.linspace(t2a, t2b, 500)\n",
    "t1, t2 = np.meshgrid(t1s, t2s)\n",
    "T = np.c_[t1.ravel(), t2.ravel()]\n",
    "Xr = np.array([[1, 1], [1, -1], [1, 0.5]])\n",
    "yr = 2 * Xr[:, :1] + 0.5 * Xr[:, 1:]\n",
    "\n",
    "J = (1 / len(Xr) * ((T @ Xr.T - yr.T) ** 2).sum(axis=1)).reshape(t1.shape)\n",
    "\n",
    "N1 = np.linalg.norm(T, ord=1, axis=1).reshape(t1.shape)\n",
    "N2 = np.linalg.norm(T, ord=2, axis=1).reshape(t1.shape)\n",
    "\n",
    "t_min_idx = np.unravel_index(J.argmin(), J.shape)\n",
    "t1_min, t2_min = t1[t_min_idx], t2[t_min_idx]\n",
    "\n",
    "t_init = np.array([[0.25], [-1]])\n",
    "\n",
    "def bgd_path(theta, X, y, l1, l2, core=1, eta=0.05, n_iterations=200):\n",
    "    path = [theta]\n",
    "    for iteration in range(n_iterations):\n",
    "        gradients = (core * 2 / len(X) * X.T @ (X @ theta - y)\n",
    "                     + l1 * np.sign(theta) + l2 * theta)\n",
    "        theta = theta - eta * gradients\n",
    "        path.append(theta)\n",
    "    return np.array(path)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, sharex=True, sharey=True, figsize=(10.1, 8))\n",
    "\n",
    "for i, N, l1, l2, title in ((0, N1, 2.0, 0, \"Lasso\"), (1, N2, 0, 2.0, \"Ridge\")):\n",
    "    JR = J + l1 * N1 + l2 * 0.5 * N2 ** 2\n",
    "\n",
    "    tr_min_idx = np.unravel_index(JR.argmin(), JR.shape)\n",
    "    t1r_min, t2r_min = t1[tr_min_idx], t2[tr_min_idx]\n",
    "\n",
    "    levels = np.exp(np.linspace(0, 1, 20)) - 1\n",
    "    levelsJ = levels * (J.max() - J.min()) + J.min()\n",
    "    levelsJR = levels * (JR.max() - JR.min()) + JR.min()\n",
    "    levelsN = np.linspace(0, N.max(), 10)\n",
    "\n",
    "    path_J = bgd_path(t_init, Xr, yr, l1=0, l2=0)\n",
    "    path_JR = bgd_path(t_init, Xr, yr, l1, l2)\n",
    "    path_N = bgd_path(theta=np.array([[2.0], [0.5]]), X=Xr, y=yr,\n",
    "                      l1=np.sign(l1) / 3, l2=np.sign(l2), core=0)\n",
    "    ax = axes[i, 0]\n",
    "    ax.grid()\n",
    "    ax.axhline(y=0, color=\"k\")\n",
    "    ax.axvline(x=0, color=\"k\")\n",
    "    ax.contourf(t1, t2, N / 2.0, levels=levelsN)\n",
    "    ax.plot(path_N[:, 0], path_N[:, 1], \"y--\")\n",
    "    ax.plot(0, 0, \"ys\")\n",
    "    ax.plot(t1_min, t2_min, \"ys\")\n",
    "    ax.set_title(fr\"$\\ell_{i + 1}$ penalty\")\n",
    "    ax.axis([t1a, t1b, t2a, t2b])\n",
    "    if i == 1:\n",
    "        ax.set_xlabel(r\"$\\theta_1$\")\n",
    "    ax.set_ylabel(r\"$\\theta_2$\", rotation=0)\n",
    "\n",
    "    ax = axes[i, 1]\n",
    "    ax.grid()\n",
    "    ax.axhline(y=0, color=\"k\")\n",
    "    ax.axvline(x=0, color=\"k\")\n",
    "    ax.contourf(t1, t2, JR, levels=levelsJR, alpha=0.9)\n",
    "    ax.plot(path_JR[:, 0], path_JR[:, 1], \"w-o\")\n",
    "    ax.plot(path_N[:, 0], path_N[:, 1], \"y--\")\n",
    "    ax.plot(0, 0, \"ys\")\n",
    "    ax.plot(t1_min, t2_min, \"ys\")\n",
    "    ax.plot(t1r_min, t2r_min, \"rs\")\n",
    "    ax.set_title(title)\n",
    "    ax.axis([t1a, t1b, t2a, t2b])\n",
    "    if i == 1:\n",
    "        ax.set_xlabel(r\"$\\theta_1$\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ilustran las diferencias entre la regresi√≥n Lasso (filas superiores) y la regresi√≥n Ridge (filas inferiores) en funci√≥n de los contornos de las funciones de p√©rdida.\n",
    "\n",
    "- Parte superior izquierda (Lasso, solo $‚Ñì1$):\n",
    "\n",
    "    - Los contornos en forma de diamante muestran c√≥mo la penalizaci√≥n $‚Ñì1$ afecta a la funci√≥n de p√©rdida, disminuyendo linealmente hacia los ejes.\n",
    "    - El descenso de gradiente sigue un camino discontinuo (l√≠nea amarilla), lo que lleva a uno de los par√°metros a 0 primero. Esto representa la capacidad de Lasso para eliminar caracter√≠sticas (dejar sus pesos en 0).\n",
    "\n",
    "- Parte superior derecha (Lasso, MSE + $‚Ñì1$):\n",
    "\n",
    "    - Aqu√≠ se combinan el MSE con la penalizaci√≥n $‚Ñì1$. Los peque√±os c√≠rculos blancos muestran el recorrido del descenso de gradiente, que lleva r√°pidamente uno de los par√°metros a 0, luego avanza hacia el √≥ptimo global (cuadrado rojo).\n",
    "    - El rebote alrededor del √≥ptimo es notable porque los gradientes no se acercan a cero, lo que puede requerir una reducci√≥n de la tasa de aprendizaje para estabilizar la convergencia.\n",
    "\n",
    "- Parte inferior izquierda (Ridge, solo $‚Ñì2$):\n",
    "\n",
    "    - Los contornos circulares muestran c√≥mo la penalizaci√≥n $‚Ñì2$ suaviza la funci√≥n de p√©rdida.\n",
    "    - El descenso de gradiente sigue un camino recto hacia el origen, disminuyendo los par√°metros gradualmente pero sin llevar ninguno a cero.\n",
    "\n",
    "- Parte inferior derecha (Ridge, MSE + $‚Ñì2$):\n",
    "\n",
    "    - Aqu√≠, el MSE se combina con la penalizaci√≥n $‚Ñì2$. El recorrido del descenso de gradiente es m√°s suave y directo hacia el √≥ptimo global, con una reducci√≥n gradual de los gradientes a medida que los par√°metros se acercan al m√≠nimo.\n",
    "    - Esto demuestra que Ridge no elimina completamente los par√°metros, sino que los reduce de manera uniforme.\n",
    "\n",
    "Conclusi√≥n:\n",
    "- Lasso es √∫til para la selecci√≥n de caracter√≠sticas, ya que puede llevar algunos par√°metros a cero.\n",
    "- Ridge reduce todos los par√°metros de manera uniforme, evitando que alguno llegue a cero, y tiende a converger m√°s suavemente al √≥ptimo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lasso_reg = Lasso(alpha=0.1)\n",
    "lasso_reg.fit(X, y)\n",
    "lasso_reg.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenga en cuenta que podr√≠a utilizar SGDRegressor(penalty=\"l1\", alpha=0.1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combina lo mejor de Ridge y Lasso. Su t√©rmino de regularizaci√≥n es una mezcla ponderada de los t√©rminos de regularizaci√≥n de Ridge y Lasso, controlada por un par√°metro de mezcla $r$:\n",
    "\n",
    "- Cuando $r = 0$, la red el√°stica es equivalente a Ridge.\n",
    "- Cuando $r = 1$, es equivalente a Lasso.\n",
    "\n",
    "La funci√≥n de coste de la red el√°stica es:\n",
    "\n",
    "$$ ùêΩ(ùúÉ) = ùëÄùëÜùê∏(ùúÉ) + ùëü‚ãÖùõº \\sum_{ùëñ=1}^{ùëõ}‚à£ùúÉùëñ‚à£+\\frac{1‚àíùëü}{2}‚ãÖ\\frac{ùõº}{ùëö}\\sum_{ùëñ=1}^{ùëõ}ùúÉ_{ùëñ}^2 $$\n",
    "\n",
    "¬øCu√°ndo usar cada tipo de regresi√≥n?\n",
    "\n",
    "- Regresi√≥n Lineal Simple: En general, se debe evitar, ya que es mejor tener algo de regularizaci√≥n.\n",
    "- Ridge: Buena opci√≥n por defecto, adecuada cuando no se tiene certeza sobre la relevancia de las caracter√≠sticas.\n",
    "- Lasso: Preferible si se sospecha que solo unas pocas caracter√≠sticas son √∫tiles, ya que puede reducir a cero los pesos de las caracter√≠sticas menos relevantes.\n",
    "- ElasticNet: Generalmente preferido sobre Lasso cuando el n√∫mero de caracter√≠sticas es mayor que el n√∫mero de instancias de entrenamiento o cuando hay correlaci√≥n fuerte entre caracter√≠sticas, ya que es m√°s estable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "elastic_net.fit(X, y)\n",
    "elastic_net.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es una t√©cnica de regularizaci√≥n para algoritmos de aprendizaje iterativos, como el descenso de gradiente. En lugar de entrenar hasta completar todas las √©pocas, se detiene el entrenamiento cuando el error de validaci√≥n alcanza su m√≠nimo y comienza a aumentar, se√±alando el inicio del sobreajuste.\n",
    "\n",
    "Explicaci√≥n:\n",
    "- En el caso de un modelo complejo, como la regresi√≥n polin√≥mica de alto grado, el error de predicci√≥n (RMSE) en los conjuntos de entrenamiento y validaci√≥n disminuye inicialmente a medida que avanza el entrenamiento.\n",
    "- Sin embargo, tras un cierto n√∫mero de √©pocas, el error de validaci√≥n comienza a aumentar, indicando que el modelo ha comenzado a sobreajustar.\n",
    "- Con parada temprana, el entrenamiento se detiene en el punto en que el error de validaci√≥n es m√≠nimo.\n",
    "\n",
    "Geoffrey Hinton la calific√≥ como un \"hermoso almuerzo gratis\" por su sencillez y eficacia.\n",
    "\n",
    "Desaf√≠o con Descenso Estoc√°stico y Mini-batch:\n",
    "\n",
    "- Con descenso estoc√°stico o mini-batch, las curvas de error son menos suaves, lo que dificulta identificar cu√°ndo se ha alcanzado el m√≠nimo.\n",
    "- La soluci√≥n es detenerse solo despu√©s de que el error de validaci√≥n haya superado el m√≠nimo durante un tiempo, y luego retroceder los par√°metros del modelo hasta el punto en que el error de validaci√≥n era m√≠nimo.\n",
    "\n",
    "Conclusi√≥n:\n",
    " \n",
    "La parada temprana es una t√©cnica simple pero poderosa para evitar el sobreajuste durante el entrenamiento, especialmente efectiva cuando se aplica correctamente, incluso en escenarios m√°s ruidosos como el descenso estoc√°stico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# c√≥digo extra - crea el mismo conjunto de datos cuadr√°ticos que antes y lo divide\n",
    "np.random.seed(42)\n",
    "m = 100\n",
    "X = 6 * np.random.rand(m, 1) - 3\n",
    "y = 0.5 * X ** 2 + X + 2 + np.random.randn(m, 1)\n",
    "X_train, y_train = X[: m // 2], y[: m // 2, 0]\n",
    "X_valid, y_valid = X[m // 2 :], y[m // 2 :, 0]\n",
    "\n",
    "preprocessing = make_pipeline(PolynomialFeatures(degree=90, include_bias=False),\n",
    "                              StandardScaler())\n",
    "X_train_prep = preprocessing.fit_transform(X_train)\n",
    "X_valid_prep = preprocessing.transform(X_valid)\n",
    "sgd_reg = SGDRegressor(penalty=None, eta0=0.002, random_state=42)\n",
    "n_epochs = 500\n",
    "best_valid_rmse = float('inf')\n",
    "train_errors, val_errors = [], []  # c√≥digo extra - es para la figura de abajo\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    sgd_reg.partial_fit(X_train_prep, y_train)\n",
    "    y_valid_predict = sgd_reg.predict(X_valid_prep)\n",
    "    val_error = mean_squared_error(y_valid, y_valid_predict, squared=False)\n",
    "    if val_error < best_valid_rmse:\n",
    "        best_valid_rmse = val_error\n",
    "        best_model = deepcopy(sgd_reg)\n",
    "\n",
    "    # c√≥digo extra - evaluamos el error del tren y lo guardamos para la figura\n",
    "    y_train_predict = sgd_reg.predict(X_train_prep)\n",
    "    train_error = mean_squared_error(y_train, y_train_predict, squared=False)\n",
    "    val_errors.append(val_error)\n",
    "    train_errors.append(train_error)\n",
    "\n",
    "best_epoch = np.argmin(val_errors)\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.annotate('Best model',\n",
    "             xy=(best_epoch, best_valid_rmse),\n",
    "             xytext=(best_epoch, best_valid_rmse + 0.5),\n",
    "             ha=\"center\",\n",
    "             arrowprops=dict(facecolor='black', shrink=0.05))\n",
    "plt.plot([0, n_epochs], [best_valid_rmse, best_valid_rmse], \"k:\", linewidth=2)\n",
    "plt.plot(val_errors, \"b-\", linewidth=3, label=\"Validation set\")\n",
    "plt.plot(best_epoch, best_valid_rmse, \"bo\")\n",
    "plt.plot(train_errors, \"r--\", linewidth=2, label=\"Training set\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.axis([0, n_epochs, 0, 3.5])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocesamiento:\n",
    "\n",
    "- Expansi√≥n polin√≥mica: Se a√±aden caracter√≠sticas polin√≥micas.\n",
    "- Escalado: Se escalan las caracter√≠sticas tanto del conjunto de entrenamiento como del conjunto de validaci√≥n.\n",
    "\n",
    "Modelo:\n",
    "\n",
    "- Se crea un SGDRegressor sin regularizaci√≥n y con una tasa de aprendizaje peque√±a.\n",
    "- En lugar de usar fit(), se usa partial_fit() en un bucle para realizar el aprendizaje incremental.\n",
    "\n",
    "Evaluaci√≥n durante el entrenamiento:\n",
    "\n",
    "- En cada √©poca, se mide el RMSE en el conjunto de validaci√≥n.\n",
    "- Si el RMSE actual es menor que el m√°s bajo observado hasta el momento, se guarda una copia del modelo en la variable best_model.\n",
    "\n",
    "Copia del modelo:\n",
    "\n",
    "- Se usa copy.deepcopy() para guardar el mejor modelo, lo que asegura que tanto los hiperpar√°metros como los par√°metros aprendidos se copian.\n",
    "- sklearn.base.clone() solo copiar√≠a los hiperpar√°metros, por lo que no es adecuado para esta tarea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunos algoritmos de regresi√≥n pueden utilizarse para la clasificaci√≥n (y viceversa). La regresi√≥n log√≠stica (tambi√©n llamada regresi√≥n logit) suele utilizarse para estimar la probabilidad de que un caso pertenezca a una clase determinada (por ejemplo, ¬øcu√°l es la probabilidad de que este correo electr√≥nico sea spam?). Si la probabilidad estimada es superior a un umbral determinado (normalmente el 50%), el modelo predice que la instancia pertenece a esa clase (denominada clase positiva, etiquetada como \"1\") y, en caso contrario, predice que no (es decir, que pertenece a la clase negativa, etiquetada como \"0\"). Se trata, por tanto, de un clasificador binario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimaci√≥n de probabilidades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La regresi√≥n log√≠stica funciona de manera similar a la regresi√≥n lineal, pero con una diferencia clave. Mientras que el modelo de regresi√≥n lineal calcula una suma ponderada de las caracter√≠sticas de entrada (m√°s un t√©rmino de sesgo) y devuelve ese resultado directamente, el modelo de regresi√≥n log√≠stica aplica la funci√≥n log√≠stica (o sigmoide) a este resultado para obtener una probabilidad.\n",
    "\n",
    "La probabilidad estimada por el modelo se expresa en forma vectorizada como:\n",
    "\n",
    "$$\n",
    "p = h_\\theta(x) = \\sigma(\\theta^\\top x)\n",
    "$$\n",
    "\n",
    "La funci√≥n log√≠stica, denotada como ùúé(‚ãÖ), es una funci√≥n sigmoide que convierte el resultado en un valor entre 0 y 1, seg√∫n la siguiente f√≥rmula:\n",
    "\n",
    "$$\n",
    "\\sigma(t) = \\frac{1}{1 + \\exp(-t)}\n",
    "$$\n",
    "\n",
    "Esta funci√≥n tiene forma de \"S\" y es utilizada para modelar probabilidades en regresi√≥n log√≠stica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lim = 6\n",
    "t = np.linspace(-lim, lim, 100)\n",
    "sig = 1 / (1 + np.exp(-t))\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot([-lim, lim], [0, 0], \"k-\")\n",
    "plt.plot([-lim, lim], [0.5, 0.5], \"k:\")\n",
    "plt.plot([-lim, lim], [1, 1], \"k:\")\n",
    "plt.plot([0, 0], [-1.1, 1.1], \"k-\")\n",
    "plt.plot(t, sig, \"b-\", linewidth=2, label=r\"$\\sigma(t) = \\dfrac{1}{1 + e^{-t}}$\")\n",
    "plt.xlabel(\"t\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.axis([-lim, lim, -0.1, 1.1])\n",
    "plt.gca().set_yticks([0, 0.25, 0.5, 0.75, 1])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En un modelo de regresi√≥n log√≠stica, la predicci√≥n se realiza comparando la probabilidad estimada (ùëù) con un umbral del 50%. Si la probabilidad es mayor o igual a 0.5, se predice 1; en caso contrario, se predice 0. La funci√≥n sigmoide asegura que esto ocurra seg√∫n el signo de \n",
    "$ùúÉ^‚ä§ùë•$\n",
    "\n",
    "$$\n",
    "y = \n",
    "\\begin{cases} \n",
    "0 & \\text{si } ùëù < 0.5 \\\\\n",
    "1 & \\text{si } ùëù \\geq 0.5 \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- ùëù: Probabilidad de que la instancia ùë• pertenezca a la clase positiva.\n",
    "- $ùúÉ^‚ä§ùë•$: Producto escalar entre el vector de par√°metros del modelo (ùúÉ) y el vector de caracter√≠sticas de la instancia (ùë•)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El score ùë° es com√∫nmente llamado logit. Este nombre proviene del hecho de que la funci√≥n logit, definida como\n",
    "\n",
    "$$\n",
    "\\text{logit}(p) = \\log\\left(\\frac{p}{1 - p}\\right)\n",
    "$$\n",
    "\n",
    "es la inversa de la funci√≥n log√≠stica. Si calculas el logit de la probabilidad estimada \n",
    "ùëù, obtendr√°s como resultado ùë°. El logit tambi√©n se conoce como log-odds o logaritmo de las probabilidades, ya que es el logaritmo de la raz√≥n entre la probabilidad estimada para la clase positiva y la probabilidad estimada para la clase negativa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funci√≥n de formaci√≥n y costes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entrenar un modelo de regresi√≥n log√≠stica, el objetivo es ajustar el vector de par√°metros \n",
    "ùúÉ de manera que el modelo estime altas probabilidades para las instancias positivas (ùë¶=1) y bajas probabilidades para las instancias negativas (ùë¶=0). Esto se logra minimizando una funci√≥n de costo.\n",
    "\n",
    "La funci√≥n de costo para una sola instancia de entrenamiento ùë• se define de la siguiente manera:\n",
    "\n",
    "$$\n",
    "\\text{Costo}(\\theta) = \n",
    "\\begin{cases} \n",
    "- \\log(p) & \\text{si } y = 1 \\\\\n",
    "- \\log(1 - p) & \\text{si } y = 0 \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Esta funci√≥n tiene sentido porque \n",
    "‚àílog(ùë°) crece mucho cuando ùë° se aproxima a 0, lo que significa que el costo ser√° alto si el modelo estima una probabilidad cercana a 0 para una instancia positiva, o si estima una probabilidad cercana a 1 para una instancia negativa. Por el contrario, \n",
    "‚àílog‚Å°(ùë°) es cercano a 0 cuando ùë° est√° cerca de 1, lo que implica un costo bajo si la probabilidad estimada es correcta (cerca de 0 para una instancia negativa o cerca de 1 para una instancia positiva).\n",
    "\n",
    "La funci√≥n de costo para todo el conjunto de entrenamiento se calcula como el promedio de los costos de todas las instancias. Esta expresi√≥n se conoce como log loss o p√©rdida logar√≠tmica:\n",
    "\n",
    "$$\n",
    "J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) \\right]\n",
    "$$\n",
    "\n",
    "- p: Probabilidad estimada de que una instancia pertenezca a la clase positiva.\n",
    "- ùë¶: Valor real de la clase para la instancia (1 para positiva, 0 para negativa).\n",
    "- ùëö: N√∫mero total de instancias en el conjunto de entrenamiento.\n",
    "- ùêΩ(ùúÉ): Funci√≥n de costo total o log loss.\n",
    "\n",
    "Este enfoque garantiza que el modelo minimice los errores de clasificaci√≥n al ajustar ùúÉ para que las predicciones sean lo m√°s precisas posibles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L√≠mites de la decisi√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos utilizar el conjunto de datos iris para ilustrar la regresi√≥n log√≠stica. Este es un famoso conjunto de datos que contiene las longitudes y anchuras de los s√©palos y p√©talos de 150 flores de iris de tres especies diferentes: Iris setosa, Iris versicolor y Iris virginica.\n",
    "\n",
    "Este conjunto de datos es com√∫nmente utilizado en el aprendizaje autom√°tico para problemas de clasificaci√≥n, donde se intenta predecir la especie de una flor en funci√≥n de sus caracter√≠sticas morfol√≥gicas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intentemos construir un clasificador para detectar el tipo de Iris virginica bas√°ndonos √∫nicamente en la caracter√≠stica de anchura de los p√©talos. El primer paso es cargar los datos y echar un vistazo r√°pido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris(as_frame=True)\n",
    "list(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.target.head(3)  # ten en cuenta que las instancias no se mezclan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuaci√≥n, dividiremos los datos y entrenaremos un modelo de regresi√≥n log√≠stica en el conjunto de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = iris.data[[\"petal width (cm)\"]].values\n",
    "y = iris.target_names[iris.target] == 'virginica'\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos las probabilidades estimadas del modelo para flores con anchuras de p√©talo que var√≠an de\n",
    "de 0 cm a 3 cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.linspace(0, 3, 1000).reshape(-1, 1)  # remodelar para obtener un vector columna\n",
    "y_proba = log_reg.predict_proba(X_new)\n",
    "decision_boundary = X_new[y_proba[:, 1] >= 0.5][0, 0]\n",
    "\n",
    "plt.figure(figsize=(8, 3)) \n",
    "plt.plot(X_new, y_proba[:, 0], \"b--\", linewidth=2,\n",
    "         label=\"Not Iris virginica proba\")\n",
    "plt.plot(X_new, y_proba[:, 1], \"g-\", linewidth=2, label=\"Iris virginica proba\")\n",
    "plt.plot([decision_boundary, decision_boundary], [0, 1], \"k:\", linewidth=2,\n",
    "         label=\"Decision boundary\")\n",
    "\n",
    "plt.arrow(x=decision_boundary, y=0.08, dx=-0.3, dy=0,\n",
    "          head_width=0.05, head_length=0.1, fc=\"b\", ec=\"b\")\n",
    "plt.arrow(x=decision_boundary, y=0.92, dx=0.3, dy=0,\n",
    "          head_width=0.05, head_length=0.1, fc=\"g\", ec=\"g\")\n",
    "plt.plot(X_train[y_train == 0], y_train[y_train == 0], \"bs\")\n",
    "plt.plot(X_train[y_train == 1], y_train[y_train == 1], \"g^\")\n",
    "plt.xlabel(\"Petal width (cm)\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.legend(loc=\"center left\")\n",
    "plt.axis([0, 3, -0.02, 1.02])\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El ancho de los p√©talos de Iris virginica var√≠a entre 1.4 cm y 2.5 cm. El clasificador predice con alta confianza que es Iris virginica si el ancho es mayor a 2 cm, y que no lo es si es menor a 1 cm. La frontera de decisi√≥n est√° en 1.6 cm: por encima predice Iris virginica, y por debajo, predice que no lo es."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.predict([[1.7], [1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En un gr√°fico que muestra las caracter√≠sticas de ancho y largo de los p√©talos del conjunto de datos, un clasificador de regresi√≥n log√≠stica entrenado puede estimar la probabilidad de que una nueva flor sea Iris virginica basado en estas dos caracter√≠sticas. La l√≠nea discontinua en el gr√°fico representa la frontera de decisi√≥n, donde la probabilidad estimada es del 50%. Esta frontera es lineal. Las l√≠neas paralelas indican diferentes probabilidades, desde el 15% (esquina inferior izquierda) hasta el 90% (esquina superior derecha). Las flores m√°s all√° de la l√≠nea superior derecha tienen m√°s del 90% de probabilidad de ser Iris virginica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\n",
    "y = iris.target_names[iris.target] == 'virginica'\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "log_reg = LogisticRegression(C=2, random_state=42)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# para el gr√°fico de contornos\n",
    "x0, x1 = np.meshgrid(np.linspace(2.9, 7, 500).reshape(-1, 1),\n",
    "                     np.linspace(0.8, 2.7, 200).reshape(-1, 1))\n",
    "X_new = np.c_[x0.ravel(), x1.ravel()]  # una instancia por punto de la figura\n",
    "y_proba = log_reg.predict_proba(X_new)\n",
    "zz = y_proba[:, 1].reshape(x0.shape)\n",
    "\n",
    "# para el l√≠mite de decisi√≥n\n",
    "left_right = np.array([2.9, 7])\n",
    "boundary = -((log_reg.coef_[0, 0] * left_right + log_reg.intercept_[0])\n",
    "             / log_reg.coef_[0, 1])\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(X_train[y_train == 0, 0], X_train[y_train == 0, 1], \"bs\")\n",
    "plt.plot(X_train[y_train == 1, 0], X_train[y_train == 1, 1], \"g^\")\n",
    "contour = plt.contour(x0, x1, zz, cmap=plt.cm.brg)\n",
    "plt.clabel(contour, inline=1)\n",
    "plt.plot(left_right, boundary, \"k--\", linewidth=3)\n",
    "plt.text(3.5, 1.27, \"No Iris virginica\", color=\"b\", ha=\"center\")\n",
    "plt.text(6.5, 2.3, \"Iris virginica\", color=\"g\", ha=\"center\")\n",
    "plt.xlabel(\"Longitud de los p√©talos\")\n",
    "plt.ylabel(\"Anchura del p√©talo\")\n",
    "plt.axis([2.9, 7, 0.8, 2.7])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El hiperpar√°metro que controla la fuerza de regularizaci√≥n de un modelo LogisticRegression de Scikit-Learn no es alfa (como en otros modelos lineales), sino su inverso: C. Cuanto mayor sea el valor de C, menor ser√° la regularizaci√≥n del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La regresi√≥n softmax es una extensi√≥n de la regresi√≥n log√≠stica para problemas de clasificaci√≥n multiclase. En lugar de entrenar varios clasificadores binarios, softmax estima la probabilidad de cada clase directamente.\n",
    "\n",
    "Primero, el modelo calcula un score $ùë†_{ùëò}(ùë•)$ para cada clase ùëò usando un vector de par√°metros espec√≠fico para esa clase. Luego, aplica la funci√≥n softmax para convertir estos scores en probabilidades normalizadas:\n",
    "\n",
    "$$\n",
    "p_k = \\frac{\\exp(s_k(x))}{\\sum_{j=1}^{K} \\exp(s_j(x))}\n",
    "$$\n",
    "\n",
    "El modelo predice la clase con la probabilidad m√°s alta. Para entrenar el modelo, se minimiza la funci√≥n de costo de entrop√≠a cruzada, que penaliza las predicciones incorrectas:\n",
    "\n",
    "$$\n",
    "J(\\Theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\sum_{k=1}^{K} y_i^k \\log(p_i^k)\n",
    "$$\n",
    "\n",
    "La entrop√≠a cruzada mide cu√°n bien las probabilidades estimadas coinciden con las clases reales. El gradiente de esta funci√≥n se utiliza en algoritmos de optimizaci√≥n como el descenso de gradiente para ajustar los par√°metros del modelo.\n",
    "\n",
    "La regresi√≥n softmax se usa en problemas con clases mutuamente excluyentes, como la clasificaci√≥n de especies de plantas en el conjunto de datos Iris. En Scikit-Learn, el clasificador LogisticRegression utiliza softmax autom√°ticamente cuando se entrena en m√∫ltiples clases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\n",
    "y = iris[\"target\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "softmax_reg = LogisticRegression(C=30, random_state=42)\n",
    "softmax_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "softmax_reg.predict([[5, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "softmax_reg.predict_proba([[5, 2]]).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "custom_cmap = ListedColormap([\"#fafab0\", \"#9898ff\", \"#a0faa0\"])\n",
    "\n",
    "x0, x1 = np.meshgrid(np.linspace(0, 8, 500).reshape(-1, 1),\n",
    "                     np.linspace(0, 3.5, 200).reshape(-1, 1))\n",
    "X_new = np.c_[x0.ravel(), x1.ravel()]\n",
    "\n",
    "y_proba = softmax_reg.predict_proba(X_new)\n",
    "y_predict = softmax_reg.predict(X_new)\n",
    "\n",
    "zz1 = y_proba[:, 1].reshape(x0.shape)\n",
    "zz = y_predict.reshape(x0.shape)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(X[y == 2, 0], X[y == 2, 1], \"g^\", label=\"Iris virginica\")\n",
    "plt.plot(X[y == 1, 0], X[y == 1, 1], \"bs\", label=\"Iris versicolor\")\n",
    "plt.plot(X[y == 0, 0], X[y == 0, 1], \"yo\", label=\"Iris setosa\")\n",
    "\n",
    "plt.contourf(x0, x1, zz, cmap=custom_cmap)\n",
    "contour = plt.contour(x0, x1, zz1, cmap=\"hot\")\n",
    "plt.clabel(contour, inline=1)\n",
    "plt.xlabel(\"Petal length\")\n",
    "plt.ylabel(\"Petal width\")\n",
    "plt.legend(loc=\"center left\")\n",
    "plt.axis([0.5, 7, 0, 3.5])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "muestra los l√≠mites de decisi√≥n resultantes, representados por los colores de fondo. Observe que los l√≠mites de decisi√≥n entre dos clases cualesquiera son lineales. La figura tambi√©n muestra las probabilidades de la clase Iris versicolor, representadas por las l√≠neas curvas (por ejemplo, la l√≠nea marcada con 0,30 representa el l√≠mite de probabilidad del 30%). Observe que el modelo puede predecir una clase con una probabilidad estimada inferior al 50%. Por ejemplo, en el punto en el que se encuentran todos los l√≠mites de decisi√≥n, todas las clases tienen una probabilidad estimada del 33%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

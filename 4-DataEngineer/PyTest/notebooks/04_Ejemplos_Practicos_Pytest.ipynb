{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplos Prácticos de Pytest para Data Engineering\n",
    "\n",
    "En este notebook, exploraremos ejemplos prácticos y casos de uso específicos de pytest en el contexto de Data Engineering. Estos ejemplos están diseñados para mostrar cómo pytest puede ayudarnos a resolver problemas reales que enfrentamos en proyectos de ingeniería de datos.\n",
    "\n",
    "Utilizaremos nuestro dataset de ventas de productos y crearemos ejemplos que cubren diferentes aspectos del testing en Data Engineering, desde la validación de datos hasta el testing de pipelines de ETL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración Inicial\n",
    "\n",
    "Primero, vamos a importar las bibliotecas necesarias y cargar nuestro dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytest\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Añadimos el directorio raíz al path para poder importar los módulos\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "# Cargamos el dataset\n",
    "df_ventas = pd.read_csv('../data/ventas_productos.csv')\n",
    "\n",
    "# Mostramos las primeras filas\n",
    "df_ventas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo 1: Testing de Funciones de Agregación\n",
    "\n",
    "En Data Engineering, a menudo necesitamos agregar datos para análisis y reportes. Vamos a crear una función que calcule las ventas totales por categoría y mes, y luego escribiremos tests para verificar su correcto funcionamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agregar_ventas_por_categoria_y_mes(df):\n",
    "    \"\"\"Agrega las ventas por categoría y mes.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame con columnas 'fecha', 'categoria' y 'total'\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame con ventas agregadas por categoría y mes\n",
    "    \"\"\"\n",
    "    # Convertimos la columna fecha a datetime\n",
    "    df = df.copy()\n",
    "    df['fecha'] = pd.to_datetime(df['fecha'])\n",
    "    \n",
    "    # Extraemos el mes y año\n",
    "    df['mes'] = df['fecha'].dt.strftime('%Y-%m')\n",
    "    \n",
    "    # Agregamos por categoría y mes\n",
    "    ventas_agregadas = df.groupby(['categoria', 'mes'])['total'].sum().reset_index()\n",
    "    \n",
    "    # Ordenamos por categoría y mes\n",
    "    ventas_agregadas = ventas_agregadas.sort_values(['categoria', 'mes'])\n",
    "    \n",
    "    return ventas_agregadas\n",
    "\n",
    "# Probamos la función\n",
    "ventas_por_categoria_y_mes = agregar_ventas_por_categoria_y_mes(df_ventas)\n",
    "ventas_por_categoria_y_mes.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, vamos a escribir tests para esta función. Crearemos un archivo temporal para los tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file ../tests/test_agregacion.py\n",
    "import pytest\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def agregar_ventas_por_categoria_y_mes(df):\n",
    "    \"\"\"Agrega las ventas por categoría y mes.\"\"\"\n",
    "    # Convertimos la columna fecha a datetime\n",
    "    df = df.copy()\n",
    "    df['fecha'] = pd.to_datetime(df['fecha'])\n",
    "    \n",
    "    # Extraemos el mes y año\n",
    "    df['mes'] = df['fecha'].dt.strftime('%Y-%m')\n",
    "    \n",
    "    # Agregamos por categoría y mes\n",
    "    ventas_agregadas = df.groupby(['categoria', 'mes'])['total'].sum().reset_index()\n",
    "    \n",
    "    # Ordenamos por categoría y mes\n",
    "    ventas_agregadas = ventas_agregadas.sort_values(['categoria', 'mes'])\n",
    "    \n",
    "    return ventas_agregadas\n",
    "\n",
    "@pytest.fixture\n",
    "def df_ventas_test():\n",
    "    \"\"\"Fixture que crea un DataFrame de prueba.\"\"\"\n",
    "    data = {\n",
    "        'fecha': ['2023-01-05', '2023-01-10', '2023-02-15', '2023-02-20'],\n",
    "        'categoria': ['Electrónica', 'Electrónica', 'Accesorios', 'Accesorios'],\n",
    "        'total': [100.0, 200.0, 50.0, 75.0]\n",
    "    }\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def test_agregar_ventas_estructura(df_ventas_test):\n",
    "    \"\"\"Test que verifica la estructura del DataFrame resultante.\"\"\"\n",
    "    resultado = agregar_ventas_por_categoria_y_mes(df_ventas_test)\n",
    "    \n",
    "    # Verificamos que el DataFrame tenga las columnas esperadas\n",
    "    assert set(resultado.columns) == {'categoria', 'mes', 'total'}\n",
    "    \n",
    "    # Verificamos que el número de filas sea correcto (2 categorías x 2 meses = 4 filas)\n",
    "    assert len(resultado) == 3  # Solo hay 3 combinaciones únicas de categoría-mes en los datos de prueba\n",
    "\n",
    "def test_agregar_ventas_valores(df_ventas_test):\n",
    "    \"\"\"Test que verifica los valores agregados.\"\"\"\n",
    "    resultado = agregar_ventas_por_categoria_y_mes(df_ventas_test)\n",
    "    \n",
    "    # Verificamos los valores agregados para cada combinación de categoría y mes\n",
    "    # Electrónica, 2023-01: 100 + 200 = 300\n",
    "    electronica_enero = resultado[(resultado['categoria'] == 'Electrónica') & (resultado['mes'] == '2023-01')]\n",
    "    assert len(electronica_enero) == 1\n",
    "    assert electronica_enero.iloc[0]['total'] == 300.0\n",
    "    \n",
    "    # Accesorios, 2023-02: 50 + 75 = 125\n",
    "    accesorios_febrero = resultado[(resultado['categoria'] == 'Accesorios') & (resultado['mes'] == '2023-02')]\n",
    "    assert len(accesorios_febrero) == 1\n",
    "    assert accesorios_febrero.iloc[0]['total'] == 125.0\n",
    "\n",
    "def test_agregar_ventas_ordenamiento(df_ventas_test):\n",
    "    \"\"\"Test que verifica que el resultado esté ordenado por categoría y mes.\"\"\"\n",
    "    resultado = agregar_ventas_por_categoria_y_mes(df_ventas_test)\n",
    "    \n",
    "    # Convertimos a lista para verificar el orden\n",
    "    categorias = resultado['categoria'].tolist()\n",
    "    meses = resultado['mes'].tolist()\n",
    "    \n",
    "    # Verificamos que las categorías estén ordenadas alfabéticamente\n",
    "    assert categorias == sorted(categorias)\n",
    "    \n",
    "    # Verificamos que dentro de cada categoría, los meses estén ordenados cronológicamente\n",
    "    for categoria in set(categorias):\n",
    "        meses_categoria = resultado[resultado['categoria'] == categoria]['mes'].tolist()\n",
    "        assert meses_categoria == sorted(meses_categoria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutamos los tests\n",
    "!pytest -xvs test_agregacion.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo 2: Testing de Transformaciones de Datos\n",
    "\n",
    "En Data Engineering, a menudo necesitamos transformar datos de un formato a otro. Vamos a crear una función que transforme nuestro dataset de ventas a un formato \"ancho\" (wide format) para análisis, y luego escribiremos tests para verificar esta transformación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformar_a_formato_ancho(df):\n",
    "    \"\"\"Transforma el DataFrame de ventas a un formato ancho por categoría y mes.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame con columnas 'fecha', 'categoria' y 'total'\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame en formato ancho con meses como columnas y categorías como filas\n",
    "    \"\"\"\n",
    "    # Primero agregamos por categoría y mes\n",
    "    df_agregado = agregar_ventas_por_categoria_y_mes(df)\n",
    "    \n",
    "    # Pivotamos para obtener el formato ancho\n",
    "    df_ancho = df_agregado.pivot(index='categoria', columns='mes', values='total')\n",
    "    \n",
    "    # Rellenamos los valores nulos con ceros\n",
    "    df_ancho = df_ancho.fillna(0)\n",
    "    \n",
    "    # Añadimos una columna de total por categoría\n",
    "    df_ancho['Total'] = df_ancho.sum(axis=1)\n",
    "    \n",
    "    return df_ancho\n",
    "\n",
    "# Probamos la función\n",
    "ventas_formato_ancho = transformar_a_formato_ancho(df_ventas)\n",
    "ventas_formato_ancho.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, vamos a escribir tests para esta función:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file ../tests/test_transformacion.py\n",
    "import pytest\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def agregar_ventas_por_categoria_y_mes(df):\n",
    "    \"\"\"Agrega las ventas por categoría y mes.\"\"\"\n",
    "    df = df.copy()\n",
    "    df['fecha'] = pd.to_datetime(df['fecha'])\n",
    "    df['mes'] = df['fecha'].dt.strftime('%Y-%m')\n",
    "    ventas_agregadas = df.groupby(['categoria', 'mes'])['total'].sum().reset_index()\n",
    "    ventas_agregadas = ventas_agregadas.sort_values(['categoria', 'mes'])\n",
    "    return ventas_agregadas\n",
    "\n",
    "def transformar_a_formato_ancho(df):\n",
    "    \"\"\"Transforma el DataFrame de ventas a un formato ancho por categoría y mes.\"\"\"\n",
    "    df_agregado = agregar_ventas_por_categoria_y_mes(df)\n",
    "    df_ancho = df_agregado.pivot(index='categoria', columns='mes', values='total')\n",
    "    df_ancho = df_ancho.fillna(0)\n",
    "    df_ancho['Total'] = df_ancho.sum(axis=1)\n",
    "    return df_ancho\n",
    "\n",
    "@pytest.fixture\n",
    "def df_ventas_test():\n",
    "    \"\"\"Fixture que crea un DataFrame de prueba.\"\"\"\n",
    "    data = {\n",
    "        'fecha': ['2023-01-05', '2023-01-10', '2023-02-15', '2023-02-20'],\n",
    "        'categoria': ['Electrónica', 'Electrónica', 'Accesorios', 'Accesorios'],\n",
    "        'total': [100.0, 200.0, 50.0, 75.0]\n",
    "    }\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def test_transformar_a_formato_ancho_estructura(df_ventas_test):\n",
    "    \"\"\"Test que verifica la estructura del DataFrame en formato ancho.\"\"\"\n",
    "    resultado = transformar_a_formato_ancho(df_ventas_test)\n",
    "    \n",
    "    # Verificamos que el índice contenga las categorías\n",
    "    assert set(resultado.index) == {'Electrónica', 'Accesorios'}\n",
    "    \n",
    "    # Verificamos que las columnas incluyan los meses y el total\n",
    "    assert set(resultado.columns) == {'2023-01', '2023-02', 'Total'}\n",
    "\n",
    "def test_transformar_a_formato_ancho_valores(df_ventas_test):\n",
    "    \"\"\"Test que verifica los valores en el DataFrame en formato ancho.\"\"\"\n",
    "    resultado = transformar_a_formato_ancho(df_ventas_test)\n",
    "    \n",
    "    # Verificamos los valores para Electrónica\n",
    "    assert resultado.loc['Electrónica', '2023-01'] == 300.0\n",
    "    assert resultado.loc['Electrónica', '2023-02'] == 0.0  # No hay ventas en febrero\n",
    "    assert resultado.loc['Electrónica', 'Total'] == 300.0\n",
    "    \n",
    "    # Verificamos los valores para Accesorios\n",
    "    assert resultado.loc['Accesorios', '2023-01'] == 0.0  # No hay ventas en enero\n",
    "    assert resultado.loc['Accesorios', '2023-02'] == 125.0\n",
    "    assert resultado.loc['Accesorios', 'Total'] == 125.0\n",
    "\n",
    "def test_transformar_a_formato_ancho_sin_nulos(df_ventas_test):\n",
    "    \"\"\"Test que verifica que no haya valores nulos en el resultado.\"\"\"\n",
    "    resultado = transformar_a_formato_ancho(df_ventas_test)\n",
    "    \n",
    "    # Verificamos que no haya valores nulos\n",
    "    assert not resultado.isnull().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutamos los tests\n",
    "!pytest -xvs test_transformacion.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo 3: Testing de Validación de Datos con Esquemas\n",
    "\n",
    "En Data Engineering, es común validar que los datos cumplan con un esquema específico. Vamos a crear una función que valide el esquema de nuestro dataset de ventas y escribiremos tests para verificar esta validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validar_esquema(df, esquema):\n",
    "    \"\"\"Valida que un DataFrame cumpla con un esquema específico.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame a validar\n",
    "        esquema: Diccionario con columnas y sus tipos de datos esperados\n",
    "        \n",
    "    Returns:\n",
    "        dict: Resultado de la validación\n",
    "    \"\"\"\n",
    "    # Verificamos que todas las columnas requeridas estén presentes\n",
    "    columnas_faltantes = [col for col in esquema.keys() if col not in df.columns]\n",
    "    if columnas_faltantes:\n",
    "        return {\n",
    "            'valido': False,\n",
    "            'error': f\"Columnas faltantes: {', '.join(columnas_faltantes)}\"\n",
    "        }\n",
    "    \n",
    "    # Verificamos los tipos de datos\n",
    "    tipos_incorrectos = {}\n",
    "    for columna, tipo_esperado in esquema.items():\n",
    "        # Para tipos numéricos, verificamos si son compatibles\n",
    "        if tipo_esperado in ['int', 'float'] and pd.api.types.is_numeric_dtype(df[columna]):\n",
    "            continue\n",
    "        \n",
    "        # Para fechas, verificamos si se pueden convertir\n",
    "        if tipo_esperado == 'datetime' and pd.api.types.is_string_dtype(df[columna]):\n",
    "            try:\n",
    "                pd.to_datetime(df[columna])\n",
    "                continue\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Para strings, verificamos si son object o string\n",
    "        if tipo_esperado == 'string' and pd.api.types.is_string_dtype(df[columna]):\n",
    "            continue\n",
    "        \n",
    "        # Si llegamos aquí, el tipo no es compatible\n",
    "        tipos_incorrectos[columna] = {\n",
    "            'esperado': tipo_esperado,\n",
    "            'actual': str(df[columna].dtype)\n",
    "        }\n",
    "    \n",
    "    if tipos_incorrectos:\n",
    "        return {\n",
    "            'valido': False,\n",
    "            'error': f\"Tipos de datos incorrectos en {len(tipos_incorrectos)} columnas\",\n",
    "            'detalle': tipos_incorrectos\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        'valido': True,\n",
    "        'mensaje': \"El DataFrame cumple con el esquema especificado\"\n",
    "    }\n",
    "\n",
    "# Definimos el esquema esperado para nuestro dataset de ventas\n",
    "esquema_ventas = {\n",
    "    'id': 'int',\n",
    "    'fecha': 'datetime',\n",
    "    'producto': 'string',\n",
    "    'categoria': 'string',\n",
    "    'precio': 'float',\n",
    "    'cantidad': 'int',\n",
    "    'descuento': 'float',\n",
    "    'total': 'float'\n",
    "}\n",
    "\n",
    "# Probamos la función\n",
    "resultado_validacion = validar_esquema(df_ventas, esquema_ventas)\n",
    "resultado_validacion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, vamos a escribir tests para esta función:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file ../tests/test_esquema.py\n",
    "import pytest\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def validar_esquema(df, esquema):\n",
    "    \"\"\"Valida que un DataFrame cumpla con un esquema específico.\"\"\"\n",
    "    # Verificamos que todas las columnas requeridas estén presentes\n",
    "    columnas_faltantes = [col for col in esquema.keys() if col not in df.columns]\n",
    "    if columnas_faltantes:\n",
    "        return {\n",
    "            'valido': False,\n",
    "            'error': f\"Columnas faltantes: {', '.join(columnas_faltantes)}\"\n",
    "        }\n",
    "    \n",
    "    # Verificamos los tipos de datos\n",
    "    tipos_incorrectos = {}\n",
    "    for columna, tipo_esperado in esquema.items():\n",
    "        # Para tipos numéricos, verificamos si son compatibles\n",
    "        if tipo_esperado in ['int', 'float'] and pd.api.types.is_numeric_dtype(df[columna]):\n",
    "            continue\n",
    "        \n",
    "        # Para fechas, verificamos si se pueden convertir\n",
    "        if tipo_esperado == 'datetime' and pd.api.types.is_string_dtype(df[columna]):\n",
    "            try:\n",
    "                pd.to_datetime(df[columna])\n",
    "                continue\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Para strings, verificamos si son object o string\n",
    "        if tipo_esperado == 'string' and pd.api.types.is_string_dtype(df[columna]):\n",
    "            continue\n",
    "        \n",
    "        # Si llegamos aquí, el tipo no es compatible\n",
    "        tipos_incorrectos[columna] = {\n",
    "            'esperado': tipo_esperado,\n",
    "            'actual': str(df[columna].dtype)\n",
    "        }\n",
    "    \n",
    "    if tipos_incorrectos:\n",
    "        return {\n",
    "            'valido': False,\n",
    "            'error': f\"Tipos de datos incorrectos en {len(tipos_incorrectos)} columnas\",\n",
    "            'detalle': tipos_incorrectos\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        'valido': True,\n",
    "        'mensaje': \"El DataFrame cumple con el esquema especificado\"\n",
    "    }\n",
    "\n",
    "@pytest.fixture\n",
    "def df_valido():\n",
    "    \"\"\"Fixture que crea un DataFrame válido.\"\"\"\n",
    "    data = {\n",
    "        'id': [1, 2, 3],\n",
    "        'fecha': ['2023-01-05', '2023-01-10', '2023-01-15'],\n",
    "        'producto': ['Laptop HP', 'Monitor Dell', 'Teclado Logitech'],\n",
    "        'categoria': ['Electrónica', 'Electrónica', 'Accesorios'],\n",
    "        'precio': [899.99, 249.99, 59.99],\n",
    "        'cantidad': [1, 2, 3],\n",
    "        'descuento': [0.05, 0.00, 0.10],\n",
    "        'total': [854.99, 499.98, 161.97]\n",
    "    }\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "@pytest.fixture\n",
    "def df_columnas_faltantes():\n",
    "    \"\"\"Fixture que crea un DataFrame con columnas faltantes.\"\"\"\n",
    "    data = {\n",
    "        'id': [1, 2, 3],\n",
    "        'fecha': ['2023-01-05', '2023-01-10', '2023-01-15'],\n",
    "        'producto': ['Laptop HP', 'Monitor Dell', 'Teclado Logitech'],\n",
    "        # Falta 'categoria'\n",
    "        'precio': [899.99, 249.99, 59.99],\n",
    "        'cantidad': [1, 2, 3],\n",
    "        # Falta 'descuento'\n",
    "        'total': [854.99, 499.98, 161.97]\n",
    "    }\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "@pytest.fixture\n",
    "def df_tipos_incorrectos():\n",
    "    \"\"\"Fixture que crea un DataFrame con tipos de datos incorrectos.\"\"\"\n",
    "    data = {\n",
    "        'id': ['1', '2', '3'],  # String en lugar de int\n",
    "        'fecha': ['2023-01-05', '2023-01-10', '2023-01-15'],\n",
    "        'producto': ['Laptop HP', 'Monitor Dell', 'Teclado Logitech'],\n",
    "        'categoria': ['Electrónica', 'Electrónica', 'Accesorios'],\n",
    "        'precio': [899.99, 249.99, 59.99],\n",
    "        'cantidad': [1, 2, 3],\n",
    "        'descuento': [0.05, 0.00, 0.10],\n",
    "        'total': ['854.99', '499.98', '161.97']  # String en lugar de float\n",
    "    }\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def test_validar_esquema_df_valido(df_valido):\n",
    "    \"\"\"Test para validar un DataFrame que cumple con el esquema.\"\"\"\n",
    "    esquema = {\n",
    "        'id': 'int',\n",
    "        'fecha': 'datetime',\n",
    "        'producto': 'string',\n",
    "        'categoria': 'string',\n",
    "        'precio': 'float',\n",
    "        'cantidad': 'int',\n",
    "        'descuento': 'float',\n",
    "        'total': 'float'\n",
    "    }\n",
    "    resultado = validar_esquema(df_valido, esquema)\n",
    "    assert resultado['valido'] is True\n",
    "    assert 'mensaje' in resultado\n",
    "\n",
    "def test_validar_esquema_columnas_faltantes(df_columnas_faltantes):\n",
    "    \"\"\"Test para validar un DataFrame con columnas faltantes.\"\"\"\n",
    "    esquema = {\n",
    "        'id': 'int',\n",
    "        'fecha': 'datetime',\n",
    "        'producto': 'string',\n",
    "        'categoria': 'string',  # Esta columna falta en el DataFrame\n",
    "        'precio': 'float',\n",
    "        'cantidad': 'int',\n",
    "        'descuento': 'float',  # Esta columna falta en el DataFrame\n",
    "        'total': 'float'\n",
    "    }\n",
    "    resultado = validar_esquema(df_columnas_faltantes, esquema)\n",
    "    assert resultado['valido'] is False\n",
    "    assert 'error' in resultado\n",
    "    assert 'categoria' in resultado['error']\n",
    "    assert 'descuento' in resultado['error']\n",
    "\n",
    "def test_validar_esquema_tipos_incorrectos(df_tipos_incorrectos):\n",
    "    \"\"\"Test para validar un DataFrame con tipos de datos incorrectos.\"\"\"\n",
    "    esquema = {\n",
    "        'id': 'int',  # En df_tipos_incorrectos, id es string\n",
    "        'fecha': 'datetime',\n",
    "        'producto': 'string',\n",
    "        'categoria': 'string',\n",
    "        'precio': 'float',\n",
    "        'cantidad': 'int',\n",
    "        'descuento': 'float',\n",
    "        'total': 'float'  # En df_tipos_incorrectos, total es string\n",
    "    }\n",
    "    resultado = validar_esquema(df_tipos_incorrectos, esquema)\n",
    "    assert resultado['valido'] is False\n",
    "    assert 'error' in resultado\n",
    "    assert 'detalle' in resultado\n",
    "    assert 'id' in resultado['detalle']\n",
    "    assert 'total' in resultado['detalle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutamos los tests\n",
    "!pytest -xvs test_esquema.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo 4: Testing de Funciones de Limpieza de Datos\n",
    "\n",
    "La limpieza de datos es una tarea común en Data Engineering. Vamos a crear una función que limpie nuestro dataset de ventas (eliminando duplicados, corrigiendo formatos, etc.) y escribiremos tests para verificar esta limpieza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_dataset_ventas(df):\n",
    "    \"\"\"Limpia el dataset de ventas.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame de ventas\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame limpio\n",
    "    \"\"\"\n",
    "    # Creamos una copia para no modificar el original\n",
    "    df_limpio = df.copy()\n",
    "    \n",
    "    # 1. Convertimos la columna fecha a datetime\n",
    "    df_limpio['fecha'] = pd.to_datetime(df_limpio['fecha'])\n",
    "    \n",
    "    # 2. Convertimos las columnas numéricas al tipo correcto\n",
    "    df_limpio['precio'] = pd.to_numeric(df_limpio['precio'])\n",
    "    df_limpio['cantidad'] = pd.to_numeric(df_limpio['cantidad']).astype(int)\n",
    "    df_limpio['descuento'] = pd.to_numeric(df_limpio['descuento'])\n",
    "    df_limpio['total'] = pd.to_numeric(df_limpio['total'])\n",
    "    \n",
    "    # 3. Eliminamos duplicados basados en todas las columnas\n",
    "    df_limpio = df_limpio.drop_duplicates()\n",
    "    \n",
    "    # 4. Verificamos que el total sea correcto (precio * cantidad * (1 - descuento))\n",
    "    df_limpio['total_calculado'] = df_limpio['precio'] * df_limpio['cantidad'] * (1 - df_limpio['descuento'])\n",
    "    \n",
    "    # 5. Corregimos los totales que difieren más de 0.01 del calculado\n",
    "    diferencia = abs(df_limpio['total'] - df_limpio['total_calculado'])\n",
    "    df_limpio.loc[diferencia > 0.01, 'total'] = df_limpio.loc[diferencia > 0.01, 'total_calculado']\n",
    "    \n",
    "    # 6. Eliminamos la columna temporal\n",
    "    df_limpio = df_limpio.drop(columns=['total_calculado'])\n",
    "    \n",
    "    # 7. Normalizamos las categorías (primera letra mayúscula, resto minúsculas)\n",
    "    df_limpio['categoria'] = df_limpio['categoria'].str.capitalize()\n",
    "    \n",
    "    return df_limpio\n",
    "\n",
    "# Probamos la función\n",
    "df_ventas_limpio = limpiar_dataset_ventas(df_ventas)\n",
    "df_ventas_limpio.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, vamos a escribir tests para esta función:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file ../tests/test_limpieza.py\n",
    "import pytest\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def limpiar_dataset_ventas(df):\n",
    "    \"\"\"Limpia el dataset de ventas.\"\"\"\n",
    "    df_limpio = df.copy()\n",
    "    df_limpio['fecha'] = pd.to_datetime(df_limpio['fecha'])\n",
    "    df_limpio['precio'] = pd.to_numeric(df_limpio['precio'])\n",
    "    df_limpio['cantidad'] = pd.to_numeric(df_limpio['cantidad']).astype(int)\n",
    "    df_limpio['descuento'] = pd.to_numeric(df_limpio['descuento'])\n",
    "    df_limpio['total'] = pd.to_numeric(df_limpio['total'])\n",
    "    df_limpio = df_limpio.drop_duplicates()\n",
    "    df_limpio['total_calculado'] = df_limpio['precio'] * df_limpio['cantidad'] * (1 - df_limpio['descuento'])\n",
    "    diferencia = abs(df_limpio['total'] - df_limpio['total_calculado'])\n",
    "    df_limpio.loc[diferencia > 0.01, 'total'] = df_limpio.loc[diferencia > 0.01, 'total_calculado']\n",
    "    df_limpio = df_limpio.drop(columns=['total_calculado'])\n",
    "    df_limpio['categoria'] = df_limpio['categoria'].str.capitalize()\n",
    "    return df_limpio\n",
    "\n",
    "@pytest.fixture\n",
    "def df_con_problemas():\n",
    "    \"\"\"Fixture que crea un DataFrame con problemas para limpiar.\"\"\"\n",
    "    data = {\n",
    "        'id': [1, 2, 3, 3],  # ID duplicado\n",
    "        'fecha': ['2023-01-05', '2023-01-10', '2023-01-15', '2023-01-15'],\n",
    "        'producto': ['Laptop HP', 'Monitor Dell', 'Teclado Logitech', 'Teclado Logitech'],\n",
    "        'categoria': ['ELECTRÓNICA', 'electrónica', 'Accesorios', 'accesorios'],  # Inconsistencia en mayúsculas/minúsculas\n",
    "        'precio': [899.99, 249.99, 59.99, 59.99],\n",
    "        'cantidad': [1, 2, 3, 3],\n",
    "        'descuento': [0.05, 0.00, 0.10, 0.10],\n",
    "        'total': [854.99, 499.98, 161.97, 200.00]  # Total incorrecto en la última fila\n",
    "    }\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def test_limpiar_dataset_elimina_duplicados(df_con_problemas):\n",
    "    \"\"\"Test que verifica que la función elimine duplicados.\"\"\"\n",
    "    resultado = limpiar_dataset_ventas(df_con_problemas)\n",
    "    \n",
    "    # Verificamos que se hayan eliminado los duplicados\n",
    "    assert len(resultado) == 3  # El DataFrame original tiene 4 filas, una duplicada\n",
    "    assert resultado['id'].nunique() == 3  # Debe haber 3 IDs únicos\n",
    "\n",
    "def test_limpiar_dataset_corrige_totales(df_con_problemas):\n",
    "    \"\"\"Test que verifica que la función corrija los totales incorrectos.\"\"\"\n",
    "    resultado = limpiar_dataset_ventas(df_con_problemas)\n",
    "    \n",
    "    # Calculamos los totales esperados\n",
    "    totales_esperados = df_con_problemas['precio'] * df_con_problemas['cantidad'] * (1 - df_con_problemas['descuento'])\n",
    "    \n",
    "    # Verificamos que los totales se hayan corregido\n",
    "    for i, row in resultado.iterrows():\n",
    "        total_calculado = row['precio'] * row['cantidad'] * (1 - row['descuento'])\n",
    "        assert abs(row['total'] - total_calculado) < 0.01, f\"Total incorrecto en la fila {i}\"\n",
    "\n",
    "def test_limpiar_dataset_normaliza_categorias(df_con_problemas):\n",
    "    \"\"\"Test que verifica que la función normalice las categorías.\"\"\"\n",
    "    resultado = limpiar_dataset_ventas(df_con_problemas)\n",
    "    \n",
    "    # Verificamos que las categorías estén normalizadas (primera letra mayúscula, resto minúsculas)\n",
    "    categorias_esperadas = {'Electrónica', 'Accesorios'}\n",
    "    assert set(resultado['categoria'].unique()) == categorias_esperadas\n",
    "\n",
    "def test_limpiar_dataset_convierte_tipos(df_con_problemas):\n",
    "    \"\"\"Test que verifica que la función convierta los tipos de datos correctamente.\"\"\"\n",
    "    resultado = limpiar_dataset_ventas(df_con_problemas)\n",
    "    \n",
    "    # Verificamos los tipos de datos\n",
    "    assert pd.api.types.is_datetime64_dtype(resultado['fecha'])\n",
    "    assert pd.api.types.is_float_dtype(resultado['precio'])\n",
    "    assert pd.api.types.is_integer_dtype(resultado['cantidad'])\n",
    "    assert pd.api.types.is_float_dtype(resultado['descuento'])\n",
    "    assert pd.api.types.is_float_dtype(resultado['total'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutamos los tests\n",
    "!pytest -xvs test_limpieza.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo 5: Testing de Integración para un Pipeline de ETL\n",
    "\n",
    "En Data Engineering, a menudo trabajamos con pipelines de ETL (Extract, Transform, Load). Vamos a crear un pequeño pipeline de ETL para nuestro dataset de ventas y escribiremos tests de integración para verificar que todo el pipeline funcione correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PipelineETL:\n",
    "    \"\"\"Clase que implementa un pipeline de ETL para el dataset de ventas.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Inicializa el pipeline.\"\"\"\n",
    "        self.datos_originales = None\n",
    "        self.datos_limpios = None\n",
    "        self.datos_transformados = None\n",
    "        self.metricas = None\n",
    "    \n",
    "    def extraer(self, ruta_archivo):\n",
    "        \"\"\"Extrae los datos del archivo CSV.\n",
    "        \n",
    "        Args:\n",
    "            ruta_archivo: Ruta al archivo CSV\n",
    "            \n",
    "        Returns:\n",
    "            self: Para encadenamiento de métodos\n",
    "        \"\"\"\n",
    "        self.datos_originales = pd.read_csv(ruta_archivo)\n",
    "        return self\n",
    "    \n",
    "    def transformar(self):\n",
    "        \"\"\"Transforma los datos (limpieza y agregación).\n",
    "        \n",
    "        Returns:\n",
    "            self: Para encadenamiento de métodos\n",
    "        \"\"\"\n",
    "        if self.datos_originales is None:\n",
    "            raise ValueError(\"No hay datos para transformar. Ejecuta extraer() primero.\")\n",
    "        \n",
    "        # Limpiamos los datos\n",
    "        self.datos_limpios = limpiar_dataset_ventas(self.datos_originales)\n",
    "        \n",
    "        # Transformamos a formato ancho por categoría y mes\n",
    "        self.datos_transformados = transformar_a_formato_ancho(self.datos_limpios)\n",
    "        \n",
    "        # Calculamos métricas\n",
    "        self.metricas = calcular_metricas_ventas(self.datos_limpios)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def cargar(self, directorio_salida):\n",
    "        \"\"\"Carga los datos transformados en archivos de salida.\n",
    "        \n",
    "        Args:\n",
    "            directorio_salida: Directorio donde guardar los archivos\n",
    "            \n",
    "        Returns:\n",
    "            dict: Rutas de los archivos generados\n",
    "        \"\"\"\n",
    "        if self.datos_transformados is None or self.metricas is None:\n",
    "            raise ValueError(\"No hay datos transformados. Ejecuta transformar() primero.\")\n",
    "        \n",
    "        # Creamos el directorio si no existe\n",
    "        os.makedirs(directorio_salida, exist_ok=True)\n",
    "        \n",
    "        # Guardamos los datos limpios\n",
    "        ruta_datos_limpios = os.path.join(directorio_salida, 'ventas_limpias.csv')\n",
    "        self.datos_limpios.to_csv(ruta_datos_limpios, index=False)\n",
    "        \n",
    "        # Guardamos los datos transformados\n",
    "        ruta_datos_transformados = os.path.join(directorio_salida, 'ventas_por_categoria_mes.csv')\n",
    "        self.datos_transformados.to_csv(ruta_datos_transformados)\n",
    "        \n",
    "        # Guardamos las métricas\n",
    "        ruta_metricas = os.path.join(directorio_salida, 'metricas_ventas.json')\n",
    "        with open(ruta_metricas, 'w') as f:\n",
    "            import json\n",
    "            json.dump(self.metricas, f, indent=4)\n",
    "        \n",
    "        return {\n",
    "            'datos_limpios': ruta_datos_limpios,\n",
    "            'datos_transformados': ruta_datos_transformados,\n",
    "            'metricas': ruta_metricas\n",
    "        }\n",
    "    \n",
    "    def ejecutar_pipeline(self, ruta_archivo, directorio_salida):\n",
    "        \"\"\"Ejecuta todo el pipeline de ETL.\n",
    "        \n",
    "        Args:\n",
    "            ruta_archivo: Ruta al archivo CSV de entrada\n",
    "            directorio_salida: Directorio donde guardar los archivos de salida\n",
    "            \n",
    "        Returns:\n",
    "            dict: Rutas de los archivos generados\n",
    "        \"\"\"\n",
    "        return self.extraer(ruta_archivo).transformar().cargar(directorio_salida)\n",
    "\n",
    "# Probamos el pipeline\n",
    "pipeline = PipelineETL()\n",
    "rutas_archivos = pipeline.ejecutar_pipeline('../data/ventas_productos.csv', '../output')\n",
    "rutas_archivos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, vamos a escribir tests de integración para nuestro pipeline de ETL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file ../tests/test_pipeline_etl.py\n",
    "import pytest\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "# Importamos las funciones necesarias\n",
    "def limpiar_dataset_ventas(df):\n",
    "    \"\"\"Limpia el dataset de ventas.\"\"\"\n",
    "    df_limpio = df.copy()\n",
    "    df_limpio['fecha'] = pd.to_datetime(df_limpio['fecha'])\n",
    "    df_limpio['precio'] = pd.to_numeric(df_limpio['precio'])\n",
    "    df_limpio['cantidad'] = pd.to_numeric(df_limpio['cantidad']).astype(int)\n",
    "    df_limpio['descuento'] = pd.to_numeric(df_limpio['descuento'])\n",
    "    df_limpio['total'] = pd.to_numeric(df_limpio['total'])\n",
    "    df_limpio = df_limpio.drop_duplicates()\n",
    "    df_limpio['total_calculado'] = df_limpio['precio'] * df_limpio['cantidad'] * (1 - df_limpio['descuento'])\n",
    "    diferencia = abs(df_limpio['total'] - df_limpio['total_calculado'])\n",
    "    df_limpio.loc[diferencia > 0.01, 'total'] = df_limpio.loc[diferencia > 0.01, 'total_calculado']\n",
    "    df_limpio = df_limpio.drop(columns=['total_calculado'])\n",
    "    df_limpio['categoria'] = df_limpio['categoria'].str.capitalize()\n",
    "    return df_limpio\n",
    "\n",
    "def agregar_ventas_por_categoria_y_mes(df):\n",
    "    \"\"\"Agrega las ventas por categoría y mes.\"\"\"\n",
    "    df = df.copy()\n",
    "    df['fecha'] = pd.to_datetime(df['fecha'])\n",
    "    df['mes'] = df['fecha'].dt.strftime('%Y-%m')\n",
    "    ventas_agregadas = df.groupby(['categoria', 'mes'])['total'].sum().reset_index()\n",
    "    ventas_agregadas = ventas_agregadas.sort_values(['categoria', 'mes'])\n",
    "    return ventas_agregadas\n",
    "\n",
    "def transformar_a_formato_ancho(df):\n",
    "    \"\"\"Transforma el DataFrame de ventas a un formato ancho por categoría y mes.\"\"\"\n",
    "    df_agregado = agregar_ventas_por_categoria_y_mes(df)\n",
    "    df_ancho = df_agregado.pivot(index='categoria', columns='mes', values='total')\n",
    "    df_ancho = df_ancho.fillna(0)\n",
    "    df_ancho['Total'] = df_ancho.sum(axis=1)\n",
    "    return df_ancho\n",
    "\n",
    "def calcular_metricas_ventas(df):\n",
    "    \"\"\"Calcula métricas de ventas a partir de un DataFrame de ventas.\"\"\"\n",
    "    metricas = {\n",
    "        'total_ventas': float(df['total'].sum()),\n",
    "        'promedio_venta': float(df['total'].mean()),\n",
    "        'total_productos_vendidos': int(df['cantidad'].sum()),\n",
    "        'precio_promedio': float(df['precio'].mean()),\n",
    "        'descuento_promedio': float(df['descuento'].mean()),\n",
    "        'ahorro_total': float((df['precio'] * df['cantidad'] * df['descuento']).sum())\n",
    "    }\n",
    "    return metricas\n",
    "\n",
    "class PipelineETL:\n",
    "    \"\"\"Clase que implementa un pipeline de ETL para el dataset de ventas.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Inicializa el pipeline.\"\"\"\n",
    "        self.datos_originales = None\n",
    "        self.datos_limpios = None\n",
    "        self.datos_transformados = None\n",
    "        self.metricas = None\n",
    "    \n",
    "    def extraer(self, ruta_archivo):\n",
    "        \"\"\"Extrae los datos del archivo CSV.\"\"\"\n",
    "        self.datos_originales = pd.read_csv(ruta_archivo)\n",
    "        return self\n",
    "    \n",
    "    def transformar(self):\n",
    "        \"\"\"Transforma los datos (limpieza y agregación).\"\"\"\n",
    "        if self.datos_originales is None:\n",
    "            raise ValueError(\"No hay datos para transformar. Ejecuta extraer() primero.\")\n",
    "        \n",
    "        # Limpiamos los datos\n",
    "        self.datos_limpios = limpiar_dataset_ventas(self.datos_originales)\n",
    "        \n",
    "        # Transformamos a formato ancho por categoría y mes\n",
    "        self.datos_transformados = transformar_a_formato_ancho(self.datos_limpios)\n",
    "        \n",
    "        # Calculamos métricas\n",
    "        self.metricas = calcular_metricas_ventas(self.datos_limpios)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def cargar(self, directorio_salida):\n",
    "        \"\"\"Carga los datos transformados en archivos de salida.\"\"\"\n",
    "        if self.datos_transformados is None or self.metricas is None:\n",
    "            raise ValueError(\"No hay datos transformados. Ejecuta transformar() primero.\")\n",
    "        \n",
    "        # Creamos el directorio si no existe\n",
    "        os.makedirs(directorio_salida, exist_ok=True)\n",
    "        \n",
    "        # Guardamos los datos limpios\n",
    "        ruta_datos_limpios = os.path.join(directorio_salida, 'ventas_limpias.csv')\n",
    "        self.datos_limpios.to_csv(ruta_datos_limpios, index=False)\n",
    "        \n",
    "        # Guardamos los datos transformados\n",
    "        ruta_datos_transformados = os.path.join(directorio_salida, 'ventas_por_categoria_mes.csv')\n",
    "        self.datos_transformados.to_csv(ruta_datos_transformados)\n",
    "        \n",
    "        # Guardamos las métricas\n",
    "        ruta_metricas = os.path.join(directorio_salida, 'metricas_ventas.json')\n",
    "        with open(ruta_metricas, 'w') as f:\n",
    "            json.dump(self.metricas, f, indent=4)\n",
    "        \n",
    "        return {\n",
    "            'datos_limpios': ruta_datos_limpios,\n",
    "            'datos_transformados': ruta_datos_transformados,\n",
    "            'metricas': ruta_metricas\n",
    "        }\n",
    "    \n",
    "    def ejecutar_pipeline(self, ruta_archivo, directorio_salida):\n",
    "        \"\"\"Ejecuta todo el pipeline de ETL.\"\"\"\n",
    "        return self.extraer(ruta_archivo).transformar().cargar(directorio_salida)\n",
    "\n",
    "@pytest.fixture\n",
    "def datos_prueba():\n",
    "    \"\"\"Fixture que crea un archivo CSV temporal con datos de prueba.\"\"\"\n",
    "    # Creamos un directorio temporal\n",
    "    directorio_temp = tempfile.mkdtemp()\n",
    "    \n",
    "    # Creamos datos de prueba\n",
    "    data = {\n",
    "        'id': [1, 2, 3, 4],\n",
    "        'fecha': ['2023-01-05', '2023-01-10', '2023-02-15', '2023-02-20'],\n",
    "        'producto': ['Laptop HP', 'Monitor Dell', 'Teclado Logitech', 'Mouse Inalámbrico'],\n",
    "        'categoria': ['Electrónica', 'Electrónica', 'Accesorios', 'Accesorios'],\n",
    "        'precio': [899.99, 249.99, 59.99, 29.99],\n",
    "        'cantidad': [1, 2, 3, 5],\n",
    "        'descuento': [0.05, 0.00, 0.10, 0.00],\n",
    "        'total': [854.99, 499.98, 161.97, 149.95]\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Guardamos en un archivo CSV\n",
    "    ruta_csv = os.path.join(directorio_temp, 'ventas_test.csv')\n",
    "    df.to_csv(ruta_csv, index=False)\n",
    "    \n",
    "    # Creamos un directorio para los archivos de salida\n",
    "    directorio_salida = os.path.join(directorio_temp, 'output')\n",
    "    os.makedirs(directorio_salida, exist_ok=True)\n",
    "    \n",
    "    # Devolvemos las rutas\n",
    "    yield {\n",
    "        'ruta_csv': ruta_csv,\n",
    "        'directorio_salida': directorio_salida,\n",
    "        'directorio_temp': directorio_temp\n",
    "    }\n",
    "    \n",
    "    # Limpiamos después de las pruebas\n",
    "    shutil.rmtree(directorio_temp)\n",
    "\n",
    "@pytest.mark.integracion\n",
    "def test_pipeline_etl_completo(datos_prueba):\n",
    "    \"\"\"Test de integración para el pipeline ETL completo.\"\"\"\n",
    "    # Ejecutamos el pipeline\n",
    "    pipeline = PipelineETL()\n",
    "    rutas_archivos = pipeline.ejecutar_pipeline(\n",
    "        datos_prueba['ruta_csv'],\n",
    "        datos_prueba['directorio_salida']\n",
    "    )\n",
    "    \n",
    "    # Verificamos que se hayan generado los archivos esperados\n",
    "    assert os.path.exists(rutas_archivos['datos_limpios'])\n",
    "    assert os.path.exists(rutas_archivos['datos_transformados'])\n",
    "    assert os.path.exists(rutas_archivos['metricas'])\n",
    "    \n",
    "    # Verificamos el contenido de los archivos\n",
    "    # 1. Datos limpios\n",
    "    df_limpios = pd.read_csv(rutas_archivos['datos_limpios'])\n",
    "    assert len(df_limpios) == 4  # No debería haber duplicados\n",
    "    assert 'id' in df_limpios.columns\n",
    "    assert 'fecha' in df_limpios.columns\n",
    "    \n",
    "    # 2. Datos transformados\n",
    "    df_transformados = pd.read_csv(rutas_archivos['datos_transformados'])\n",
    "    assert 'categoria' in df_transformados.columns\n",
    "    assert 'Total' in df_transformados.columns\n",
    "    \n",
    "    # 3. Métricas\n",
    "    with open(rutas_archivos['metricas'], 'r') as f:\n",
    "        metricas = json.load(f)\n",
    "    assert 'total_ventas' in metricas\n",
    "    assert 'promedio_venta' in metricas\n",
    "    assert 'total_productos_vendidos' in metricas\n",
    "\n",
    "@pytest.mark.integracion\n",
    "def test_pipeline_etl_valores(datos_prueba):\n",
    "    \"\"\"Test de integración que verifica los valores generados por el pipeline.\"\"\"\n",
    "    # Ejecutamos el pipeline\n",
    "    pipeline = PipelineETL()\n",
    "    rutas_archivos = pipeline.ejecutar_pipeline(\n",
    "        datos_prueba['ruta_csv'],\n",
    "        datos_prueba['directorio_salida']\n",
    "    )\n",
    "    \n",
    "    # Verificamos las métricas\n",
    "    with open(rutas_archivos['metricas'], 'r') as f:\n",
    "        metricas = json.load(f)\n",
    "    \n",
    "    # La suma de los totales debe ser 854.99 + 499.98 + 161.97 + 149.95 = 1666.89\n",
    "    assert abs(metricas['total_ventas'] - 1666.89) < 0.01\n",
    "    \n",
    "    # El total de productos vendidos debe ser 1 + 2 + 3 + 5 = 11\n",
    "    assert metricas['total_productos_vendidos'] == 11\n",
    "    \n",
    "    # Verificamos los datos transformados\n",
    "    df_transformados = pd.read_csv(rutas_archivos['datos_transformados'], index_col=0)\n",
    "    \n",
    "    # Debe haber 2 categorías\n",
    "    assert len(df_transformados) == 2\n",
    "    \n",
    "    # La categoría 'Accesorios' debe tener un total de 161.97 + 149.95 = 311.92\n",
    "    assert abs(df_transformados.loc['Accesorios', 'Total'] - 311.92) < 0.01\n",
    "    \n",
    "    # La categoría 'Electrónica' debe tener un total de 854.99 + 499.98 = 1354.97\n",
    "    assert abs(df_transformados.loc['Electrónica', 'Total'] - 1354.97) < 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutamos los tests de integración\n",
    "!pytest -xvs test_pipeline_etl.py -m integracion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo 6: Testing de Rendimiento\n",
    "\n",
    "En Data Engineering, el rendimiento es a menudo una preocupación importante, especialmente cuando trabajamos con grandes conjuntos de datos. Vamos a crear tests de rendimiento para medir el tiempo de ejecución de nuestras funciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file ../tests/test_rendimiento.py\n",
    "import pytest\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Importamos las funciones a testear\n",
    "def limpiar_dataset_ventas(df):\n",
    "    \"\"\"Limpia el dataset de ventas.\"\"\"\n",
    "    df_limpio = df.copy()\n",
    "    df_limpio['fecha'] = pd.to_datetime(df_limpio['fecha'])\n",
    "    df_limpio['precio'] = pd.to_numeric(df_limpio['precio'])\n",
    "    df_limpio['cantidad'] = pd.to_numeric(df_limpio['cantidad']).astype(int)\n",
    "    df_limpio['descuento'] = pd.to_numeric(df_limpio['descuento'])\n",
    "    df_limpio['total'] = pd.to_numeric(df_limpio['total'])\n",
    "    df_limpio = df_limpio.drop_duplicates()\n",
    "    df_limpio['total_calculado'] = df_limpio['precio'] * df_limpio['cantidad'] * (1 - df_limpio['descuento'])\n",
    "    diferencia = abs(df_limpio['total'] - df_limpio['total_calculado'])\n",
    "    df_limpio.loc[diferencia > 0.01, 'total'] = df_limpio.loc[diferencia > 0.01, 'total_calculado']\n",
    "    df_limpio = df_limpio.drop(columns=['total_calculado'])\n",
    "    df_limpio['categoria'] = df_limpio['categoria'].str.capitalize()\n",
    "    return df_limpio\n",
    "\n",
    "def agregar_ventas_por_categoria_y_mes(df):\n",
    "    \"\"\"Agrega las ventas por categoría y mes.\"\"\"\n",
    "    df = df.copy()\n",
    "    df['fecha'] = pd.to_datetime(df['fecha'])\n",
    "    df['mes'] = df['fecha'].dt.strftime('%Y-%m')\n",
    "    ventas_agregadas = df.groupby(['categoria', 'mes'])['total'].sum().reset_index()\n",
    "    ventas_agregadas = ventas_agregadas.sort_values(['categoria', 'mes'])\n",
    "    return ventas_agregadas\n",
    "\n",
    "def transformar_a_formato_ancho(df):\n",
    "    \"\"\"Transforma el DataFrame de ventas a un formato ancho por categoría y mes.\"\"\"\n",
    "    df_agregado = agregar_ventas_por_categoria_y_mes(df)\n",
    "    df_ancho = df_agregado.pivot(index='categoria', columns='mes', values='total')\n",
    "    df_ancho = df_ancho.fillna(0)\n",
    "    df_ancho['Total'] = df_ancho.sum(axis=1)\n",
    "    return df_ancho\n",
    "\n",
    "@pytest.fixture\n",
    "def df_ventas_pequeno():\n",
    "    \"\"\"Fixture que crea un DataFrame pequeño (20 filas).\"\"\"\n",
    "    # Cargamos el dataset original\n",
    "    ruta_csv = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'data', 'ventas_productos.csv')\n",
    "    return pd.read_csv(ruta_csv)\n",
    "\n",
    "@pytest.fixture\n",
    "def df_ventas_mediano():\n",
    "    \"\"\"Fixture que crea un DataFrame mediano (200 filas).\"\"\"\n",
    "    # Cargamos el dataset original y lo replicamos 10 veces\n",
    "    ruta_csv = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'data', 'ventas_productos.csv')\n",
    "    df_original = pd.read_csv(ruta_csv)\n",
    "    return pd.concat([df_original] * 10, ignore_index=True)\n",
    "\n",
    "@pytest.fixture\n",
    "def df_ventas_grande():\n",
    "    \"\"\"Fixture que crea un DataFrame grande (2000 filas).\"\"\"\n",
    "    # Cargamos el dataset original y lo replicamos 100 veces\n",
    "    ruta_csv = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'data', 'ventas_productos.csv')\n",
    "    df_original = pd.read_csv(ruta_csv)\n",
    "    return pd.concat([df_original] * 100, ignore_index=True)\n",
    "\n",
    "@pytest.mark.rendimiento\n",
    "def test_rendimiento_limpiar_dataset(df_ventas_pequeno, df_ventas_mediano, df_ventas_grande):\n",
    "    \"\"\"Test de rendimiento para la función de limpieza de datos.\"\"\"\n",
    "    # Medimos el tiempo para el dataset pequeño\n",
    "    inicio = time.time()\n",
    "    limpiar_dataset_ventas(df_ventas_pequeno)\n",
    "    tiempo_pequeno = time.time() - inicio\n",
    "    \n",
    "    # Medimos el tiempo para el dataset mediano\n",
    "    inicio = time.time()\n",
    "    limpiar_dataset_ventas(df_ventas_mediano)\n",
    "    tiempo_mediano = time.time() - inicio\n",
    "    \n",
    "    # Medimos el tiempo para el dataset grande\n",
    "    inicio = time.time()\n",
    "    limpiar_dataset_ventas(df_ventas_grande)\n",
    "    tiempo_grande = time.time() - inicio\n",
    "    \n",
    "    # Imprimimos los resultados\n",
    "    print(f\"\\nTiempos de ejecución para limpiar_dataset_ventas:\")\n",
    "    print(f\"Dataset pequeño (20 filas): {tiempo_pequeno:.4f} segundos\")\n",
    "    print(f\"Dataset mediano (200 filas): {tiempo_mediano:.4f} segundos\")\n",
    "    print(f\"Dataset grande (2000 filas): {tiempo_grande:.4f} segundos\")\n",
    "    \n",
    "    # Verificamos que el tiempo de ejecución sea aproximadamente lineal\n",
    "    # (el tiempo para el dataset grande debería ser aproximadamente 10 veces el tiempo para el dataset mediano)\n",
    "    ratio = tiempo_grande / tiempo_mediano\n",
    "    assert 5 < ratio < 15, f\"La relación de tiempo grande/mediano es {ratio}, debería ser cercana a 10\"\n",
    "\n",
    "@pytest.mark.rendimiento\n",
    "def test_rendimiento_transformar_a_formato_ancho(df_ventas_pequeno, df_ventas_mediano, df_ventas_grande):\n",
    "    \"\"\"Test de rendimiento para la función de transformación a formato ancho.\"\"\"\n",
    "    # Limpiamos los datasets primero\n",
    "    df_pequeno_limpio = limpiar_dataset_ventas(df_ventas_pequeno)\n",
    "    df_mediano_limpio = limpiar_dataset_ventas(df_ventas_mediano)\n",
    "    df_grande_limpio = limpiar_dataset_ventas(df_ventas_grande)\n",
    "    \n",
    "    # Medimos el tiempo para el dataset pequeño\n",
    "    inicio = time.time()\n",
    "    transformar_a_formato_ancho(df_pequeno_limpio)\n",
    "    tiempo_pequeno = time.time() - inicio\n",
    "    \n",
    "    # Medimos el tiempo para el dataset mediano\n",
    "    inicio = time.time()\n",
    "    transformar_a_formato_ancho(df_mediano_limpio)\n",
    "    tiempo_mediano = time.time() - inicio\n",
    "    \n",
    "    # Medimos el tiempo para el dataset grande\n",
    "    inicio = time.time()\n",
    "    transformar_a_formato_ancho(df_grande_limpio)\n",
    "    tiempo_grande = time.time() - inicio\n",
    "    \n",
    "    # Imprimimos los resultados\n",
    "    print(f\"\\nTiempos de ejecución para transformar_a_formato_ancho:\")\n",
    "    print(f\"Dataset pequeño (20 filas): {tiempo_pequeno:.4f} segundos\")\n",
    "    print(f\"Dataset mediano (200 filas): {tiempo_mediano:.4f} segundos\")\n",
    "    print(f\"Dataset grande (2000 filas): {tiempo_grande:.4f} segundos\")\n",
    "    \n",
    "    # Verificamos que el tiempo de ejecución no sea excesivo\n",
    "    assert tiempo_grande < 5, f\"El tiempo para el dataset grande es {tiempo_grande:.4f} segundos, debería ser menor a 5 segundos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutamos los tests de rendimiento\n",
    "!pytest -xvs test_rendimiento.py -m rendimiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusión\n",
    "\n",
    "En este notebook, hemos explorado varios ejemplos prácticos de cómo utilizar pytest en proyectos de Data Engineering. Hemos visto:\n",
    "\n",
    "1. **Testing de funciones de agregación**: Verificamos que nuestras funciones de agregación produzcan los resultados esperados.\n",
    "2. **Testing de transformaciones de datos**: Aseguramos que nuestras transformaciones de datos funcionen correctamente.\n",
    "3. **Testing de validación de datos con esquemas**: Verificamos que nuestros datos cumplan con esquemas específicos.\n",
    "4. **Testing de funciones de limpieza de datos**: Aseguramos que nuestras funciones de limpieza corrijan problemas comunes en los datos.\n",
    "5. **Testing de integración para un pipeline de ETL**: Verificamos que todo el pipeline de ETL funcione correctamente de principio a fin.\n",
    "6. **Testing de rendimiento**: Medimos y verificamos el rendimiento de nuestras funciones con diferentes tamaños de datos.\n",
    "\n",
    "Estos ejemplos ilustran cómo pytest puede ayudarnos a asegurar la calidad y confiabilidad de nuestros proyectos de Data Engineering. Al implementar tests automatizados, podemos detectar problemas temprano, refactorizar con confianza y garantizar que nuestros pipelines de datos produzcan resultados correctos y consistentes.\n",
    "\n",
    "En el siguiente notebook, veremos ejercicios prácticos para que puedas aplicar lo aprendido y desarrollar tus habilidades de testing en Data Engineering."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

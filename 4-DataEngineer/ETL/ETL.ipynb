{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc04b5c4",
   "metadata": {},
   "source": [
    "# ETL: Extract, Transform, Load\n",
    "## Fundamentos para Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ca90ba",
   "metadata": {},
   "source": [
    "## Índice\n",
    "1. Introducción a ETL\n",
    "- ¿Qué es ETL?\n",
    "- Importancia en Data Engineering\n",
    "- ETL vs ELT\n",
    "- Herramientas comunes para ETL\n",
    "2. Fase de Extracción (Extract)\n",
    "- Fuentes de datos\n",
    "- Métodos de extracción\n",
    "- Consideraciones importantes\n",
    "- Ejemplos prácticos\n",
    "3. Fase de Transformación (Transform)\n",
    "- Tipos de transformaciones\n",
    "- Limpieza de datos\n",
    "- Normalización y estandarización\n",
    "- Enriquecimiento de datos\n",
    "- Ejemplos prácticos\n",
    "4. Fase de Carga (Load)\n",
    "- Destinos de carga\n",
    "- Métodos de carga\n",
    "- Consideraciones de rendimiento\n",
    "- Ejemplos prácticos\n",
    "5. Proyecto Práctico Completo\n",
    "- Implementación de un pipeline ETL\n",
    "- Análisis de resultados\n",
    "6. Mejores Prácticas y Consideraciones\n",
    "- Monitoreo y logging\n",
    "- Manejo de errores\n",
    "- Escalabilidad\n",
    "- Seguridad\n",
    "7. Ejercicios para Estudiantes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba346757",
   "metadata": {},
   "source": [
    "## 1. Introducción a ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ce2318",
   "metadata": {},
   "source": [
    "### ¿Qué es ETL?\n",
    "ETL (Extract, Transform, Load) es un proceso fundamental en el ámbito de la ingeniería de datos que consiste en:\n",
    "- Extraer datos de diversas fuentes (bases de datos, APIs, archivos, etc.)\n",
    "- Transformar los datos para que sean útiles para análisis (limpieza, normalización, enriquecimiento)\n",
    "- Cargar los datos en un destino final (data warehouse, data lake, base de datos, etc.)\n",
    "El proceso ETL permite integrar datos de múltiples fuentes heterogéneas en un único repositorio centralizado, facilitando el análisis y la toma de decisiones basada en datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbe80d8",
   "metadata": {},
   "source": [
    "### Importancia en Data Engineering\n",
    "Los procesos ETL son fundamentales en Data Engineering por varias razones:\n",
    "1. Integración de datos: Permiten combinar datos de múltiples fuentes en un formato coherente.\n",
    "2. Mejora de la calidad de datos: Durante la fase de transformación, se pueden identificar y corregir problemas de calidad.\n",
    "3. Estandarización: Convierten datos de diferentes formatos a un formato común para análisis.\n",
    "4. Historización: Facilitan el almacenamiento de datos históricos para análisis de tendencias.\n",
    "5. Preparación para análisis: Transforman los datos en formatos optimizados para consultas analíticas.\n",
    "6. Automatización: Permiten automatizar flujos de datos complejos, reduciendo la intervención manual."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac94849",
   "metadata": {},
   "source": [
    "### ETL vs ELT\n",
    "En los últimos años, ha surgido un enfoque alternativo conocido como ELT (Extract, Load, Transform):\n",
    "| Característica | ETL | ELT |\n",
    "|----------------|-----|-----|\n",
    "| Orden de operaciones | Extracción → Transformación → Carga | Extracción → Carga → Transformación |\n",
    "| Dónde ocurre la transformación | En un servidor intermedio o herramienta ETL | En el destino final (data warehouse) |\n",
    "| Ventajas | Mayor control sobre transformaciones, mejor para datos sensibles | Mayor velocidad de carga, aprovecha potencia del data warehouse |\n",
    "| Casos de uso típicos | Datos que requieren limpieza significativa, entornos con restricciones de seguridad | Big data, data lakes, cuando se necesita flexibilidad |\n",
    "| Tecnologías asociadas | Informatica, Talend, SSIS | Snowflake, BigQuery, Redshift |\n",
    "\n",
    "La elección entre ETL y ELT depende de varios factores, incluyendo el volumen de datos, requisitos de procesamiento, infraestructura existente y casos de uso específicos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dcf8f7",
   "metadata": {},
   "source": [
    "### Herramientas comunes para ETL\n",
    "Existen numerosas herramientas para implementar procesos ETL:\n",
    "Herramientas de código abierto:\n",
    "- Apache Airflow\n",
    "- Apache NiFi\n",
    "- Talend Open Studio\n",
    "- Pentaho Data Integration\n",
    "Herramientas comerciales:\n",
    "- Informatica PowerCenter\n",
    "- IBM DataStage\n",
    "- Microsoft SSIS\n",
    "- Oracle Data Integrator\n",
    "Servicios en la nube:\n",
    "- AWS Glue\n",
    "- Azure Data Factory\n",
    "- Google Cloud Dataflow\n",
    "- Fivetran\n",
    "- Stitch\n",
    "Frameworks y bibliotecas de programación:\n",
    "- Python (Pandas, PySpark)\n",
    "- Scala (Spark)\n",
    "- dbt (data build tool)\n",
    "En este notebook, nos enfocaremos en implementar procesos ETL utilizando Python y sus bibliotecas, especialmente Pandas, que es ampliamente utilizado para manipulación y análisis de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d0b533",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Importamos las bibliotecas necesarias para nuestro trabajo ETL\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "from io import StringIO\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "# Configuraciones generales\n",
    "plt.style.use('ggplot')\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Bibliotecas importadas correctamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c235a59b",
   "metadata": {},
   "source": [
    "## 2. Fase de Extracción (Extract)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d31430",
   "metadata": {},
   "source": [
    "### Fuentes de datos\n",
    "La fase de extracción consiste en obtener datos de diversas fuentes. Estas fuentes pueden ser:\n",
    "1. Bases de datos relacionales:\n",
    "- MySQL, PostgreSQL, Oracle, SQL Server, etc.\n",
    "- Se accede mediante conexiones JDBC/ODBC o bibliotecas específicas\n",
    "2. Bases de datos NoSQL:\n",
    "- MongoDB, Cassandra, Redis, etc.\n",
    "- Cada una tiene sus propios métodos de acceso\n",
    "3. Archivos planos:\n",
    "- CSV, TSV, Excel, JSON, XML, etc.\n",
    "- Fáciles de procesar con bibliotecas estándar\n",
    "4. APIs y servicios web:\n",
    "- REST, SOAP, GraphQL\n",
    "- Requieren autenticación y manejo de respuestas\n",
    "5. Sistemas de mensajería:\n",
    "- Kafka, RabbitMQ, etc.\n",
    "- Útiles para datos en tiempo real\n",
    "6. Web scraping:\n",
    "- Extracción de datos de páginas web\n",
    "- Requiere herramientas como BeautifulSoup, Scrapy\n",
    "7. Sistemas de archivos distribuidos:\n",
    "- HDFS, S3, etc.\n",
    "- Para grandes volúmenes de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09af6bae",
   "metadata": {},
   "source": [
    "### Métodos de extracción\n",
    "Existen diferentes métodos para extraer datos:\n",
    "1. Extracción completa: Se extraen todos los datos de la fuente cada vez.\n",
    "2. Extracción incremental: Solo se extraen los datos nuevos o modificados desde la última extracción.\n",
    "- Basada en marcas de tiempo\n",
    "- Basada en identificadores secuenciales\n",
    "- Basada en registros de cambios (CDC - Change Data Capture)\n",
    "3. Extracción por lotes (batch): Los datos se extraen en lotes programados (diarios, semanales, etc.).\n",
    "4. Extracción en tiempo real (streaming): Los datos se extraen continuamente a medida que se generan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e626ec21",
   "metadata": {},
   "source": [
    "### Consideraciones importantes\n",
    "Al diseñar la fase de extracción, es importante considerar:\n",
    "- Volumen de datos: ¿Cuántos datos necesitamos extraer? ¿Podemos manejarlos con nuestros recursos?\n",
    "- Frecuencia de extracción: ¿Con qué frecuencia necesitamos actualizar los datos?\n",
    "- Impacto en los sistemas fuente: ¿La extracción afectará el rendimiento de los sistemas de origen?\n",
    "- Seguridad y permisos: ¿Tenemos los permisos necesarios para acceder a los datos?\n",
    "- Manejo de errores: ¿Qué sucede si la extracción falla? ¿Cómo recuperamos?\n",
    "- Validación de datos: ¿Cómo verificamos que los datos extraídos son correctos y completos?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5756f4f9",
   "metadata": {},
   "source": [
    "### Ejemplos prácticos de extracción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ed3500",
   "metadata": {},
   "source": [
    "#### Ejemplo 1: Extracción desde CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aa05be",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def extract_from_csv(file_path):\n",
    "    \"\"\"Extrae datos desde un archivo CSV\"\"\"\n",
    "    print(f\"Extrayendo datos desde {file_path}...\")\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "# Creamos un CSV de ejemplo\n",
    "sample_data = pd.DataFrame({\n",
    "    'id': range(1, 11),\n",
    "    'nombre': ['Ana', 'Juan', 'María', 'Pedro', 'Luis', 'Carla', 'Diego', 'Sofía', 'Miguel', 'Laura'],\n",
    "    'edad': [25, 30, 22, 35, 28, 33, 27, 24, 31, 29],\n",
    "    'ciudad': ['Madrid', 'Barcelona', 'Sevilla', 'Valencia', 'Madrid', 'Barcelona', 'Sevilla', 'Valencia', 'Madrid', 'Barcelona'],\n",
    "    'salario': [35000, 42000, 30000, 38000, 36000, 45000, 33000, 37000, 40000, 39000]\n",
    "})\n",
    "\n",
    "# Guardamos el CSV\n",
    "sample_data.to_csv('datos_empleados.csv', index=False)\n",
    "\n",
    "# Extraemos los datos\n",
    "df_csv = extract_from_csv('datos_empleados.csv')\n",
    "print(\"Muestra de datos extraídos desde CSV:\")\n",
    "df_csv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5082b2",
   "metadata": {},
   "source": [
    "#### Ejemplo 2: Extracción desde API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623c460b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def extract_from_api(url):\n",
    "    \"\"\"Extrae datos desde una API\"\"\"\n",
    "    print(f\"Extrayendo datos desde API: {url}...\")\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return pd.DataFrame(response.json())\n",
    "    else:\n",
    "        print(f\"Error al extraer datos: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Usamos una API pública para el ejemplo\n",
    "api_url = \"https://jsonplaceholder.typicode.com/users\"\n",
    "df_api = extract_from_api(api_url)\n",
    "print(\"\\nMuestra de datos extraídos desde API:\")\n",
    "df_api[['id', 'name', 'email', 'username']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1640fb96",
   "metadata": {},
   "source": [
    "#### Ejemplo 3: Extracción desde base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4aae88",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def extract_from_database(query, connection):\n",
    "    \"\"\"Extrae datos desde una base de datos\"\"\"\n",
    "    print(f\"Extrayendo datos con query: {query}...\")\n",
    "    return pd.read_sql(query, connection)\n",
    "\n",
    "# Creamos una base de datos SQLite de ejemplo\n",
    "conn = sqlite3.connect('ejemplo_db.sqlite')\n",
    "sample_data.to_sql('empleados', conn, if_exists='replace', index=False)\n",
    "\n",
    "# Extraemos datos\n",
    "query = \"SELECT * FROM empleados WHERE salario > 35000\"\n",
    "df_db = extract_from_database(query, conn)\n",
    "print(\"\\nMuestra de datos extraídos desde base de datos:\")\n",
    "df_db.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c791e1",
   "metadata": {},
   "source": [
    "#### Ejemplo 4: Extracción desde JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbe67ae",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def extract_from_json(file_path):\n",
    "    \"\"\"Extrae datos desde un archivo JSON\"\"\"\n",
    "    print(f\"Extrayendo datos desde JSON: {file_path}...\")\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Creamos un archivo JSON de ejemplo\n",
    "productos = [\n",
    "    {\"id\": 1, \"nombre\": \"Laptop\", \"precio\": 1200, \"categoria\": \"Electrónica\", \"stock\": 15},\n",
    "    {\"id\": 2, \"nombre\": \"Smartphone\", \"precio\": 800, \"categoria\": \"Electrónica\", \"stock\": 25},\n",
    "    {\"id\": 3, \"nombre\": \"Auriculares\", \"precio\": 100, \"categoria\": \"Accesorios\", \"stock\": 50},\n",
    "    {\"id\": 4, \"nombre\": \"Monitor\", \"precio\": 300, \"categoria\": \"Electrónica\", \"stock\": 10},\n",
    "    {\"id\": 5, \"nombre\": \"Teclado\", \"precio\": 80, \"categoria\": \"Accesorios\", \"stock\": 30}\n",
    "]\n",
    "\n",
    "with open('productos.json', 'w') as f:\n",
    "    json.dump(productos, f)\n",
    "\n",
    "# Extraemos datos del JSON\n",
    "df_json = extract_from_json('productos.json')\n",
    "print(\"\\nMuestra de datos extraídos desde JSON:\")\n",
    "df_json.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e842e53a",
   "metadata": {},
   "source": [
    "#### Ejemplo 5: Extracción desde Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f850a88",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def extract_from_excel(file_path, sheet_name=0):\n",
    "    \"\"\"Extrae datos desde un archivo Excel\"\"\"\n",
    "    print(f\"Extrayendo datos desde Excel: {file_path}, hoja: {sheet_name}...\")\n",
    "    return pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "\n",
    "# Creamos un archivo Excel de ejemplo\n",
    "clientes = pd.DataFrame({\n",
    "    'id': range(1, 6),\n",
    "    'nombre': ['Empresa A', 'Empresa B', 'Empresa C', 'Empresa D', 'Empresa E'],\n",
    "    'contacto': ['Juan Pérez', 'María López', 'Carlos Ruiz', 'Ana Martínez', 'Pedro Sánchez'],\n",
    "    'email': ['info@empresaa.com', 'contacto@empresab.com', 'ventas@empresac.com',\n",
    "             'soporte@empresad.com', 'admin@empresae.com'],\n",
    "    'pais': ['España', 'México', 'Argentina', 'Colombia', 'Chile']\n",
    "})\n",
    "\n",
    "# Guardamos en Excel\n",
    "clientes.to_excel('clientes.xlsx', index=False)\n",
    "\n",
    "# Extraemos datos del Excel\n",
    "df_excel = extract_from_excel('clientes.xlsx')\n",
    "print(\"\\nMuestra de datos extraídos desde Excel:\")\n",
    "df_excel.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fdee5a",
   "metadata": {},
   "source": [
    "#### Ejemplo 6: Extracción desde una URL (CSV remoto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fcc9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_url_csv(url):\n",
    "    \"\"\"Extrae datos CSV desde una URL\"\"\"\n",
    "    print(f\"Extrayendo datos CSV desde URL: {url}...\")\n",
    "    return pd.read_csv(url)\n",
    "\n",
    "# Usamos un dataset público de Iris\n",
    "iris_url = 'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv'\n",
    "df_iris = extract_from_url_csv(iris_url)\n",
    "print(\"\\nMuestra de datos Iris extraídos desde URL:\")\n",
    "df_iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3615714",
   "metadata": {},
   "source": [
    "## 3. Fase de Transformación (Transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f67c90",
   "metadata": {},
   "source": [
    "### Tipos de transformaciones\n",
    "La fase de transformación es donde los datos extraídos se convierten, limpian y estructuran para su análisis. Las transformaciones más comunes incluyen:\n",
    "1. Limpieza de datos:\n",
    "- Manejo de valores nulos\n",
    "- Eliminación de duplicados\n",
    "- Corrección de errores\n",
    "- Validación de datos\n",
    "2. Transformaciones estructurales:\n",
    "- Cambio de tipos de datos\n",
    "- Normalización de esquemas\n",
    "- Pivoteo de datos\n",
    "- Desnormalización\n",
    "3. Transformaciones de valores:\n",
    "- Normalización y estandarización\n",
    "- Codificación de variables categóricas\n",
    "- Discretización de variables continuas\n",
    "- Escalado de valores\n",
    "4. Enriquecimiento de datos:\n",
    "- Agregación de datos\n",
    "- Cálculo de métricas derivadas\n",
    "- Combinación con datos externos\n",
    "- Generación de características (feature engineering)\n",
    "5. Transformaciones temporales:\n",
    "- Conversión de zonas horarias\n",
    "- Cálculo de duraciones\n",
    "- Agregaciones por períodos (diario, mensual, etc.)\n",
    "- Detección de patrones temporales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab223327",
   "metadata": {},
   "source": [
    "### Limpieza de datos\n",
    "La limpieza de datos es fundamental para garantizar la calidad de los análisis posteriores. Algunas técnicas comunes incluyen:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4003f8be",
   "metadata": {},
   "source": [
    "#### Manejo de valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f30911a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuamos con los datos de empleados\n",
    "print(\"Datos originales:\")\n",
    "df_csv.head()\n",
    "\n",
    "# Simulamos algunos valores nulos\n",
    "df_limpieza = df_csv.copy()\n",
    "df_limpieza.loc[2, 'salario'] = None\n",
    "df_limpieza.loc[5, 'edad'] = None\n",
    "df_limpieza.loc[8, 'ciudad'] = None\n",
    "\n",
    "print(\"\\nDatos con valores nulos simulados:\")\n",
    "print(df_limpieza.isnull().sum())\n",
    "\n",
    "# Estrategias para manejar valores nulos\n",
    "# 1. Eliminar filas con valores nulos\n",
    "df_drop_na = df_limpieza.dropna()\n",
    "print(\"\\nDespués de eliminar filas con valores nulos:\")\n",
    "print(f\"Filas originales: {len(df_limpieza)}, Filas después de dropna: {len(df_drop_na)}\")\n",
    "\n",
    "# 2. Rellenar con valores específicos\n",
    "df_fill = df_limpieza.copy()\n",
    "df_fill['salario'] = df_fill['salario'].fillna(df_fill['salario'].mean())\n",
    "df_fill['edad'] = df_fill['edad'].fillna(df_fill['edad'].median())\n",
    "df_fill['ciudad'] = df_fill['ciudad'].fillna('Desconocida')\n",
    "\n",
    "print(\"\\nDespués de rellenar valores nulos:\")\n",
    "print(df_fill.isnull().sum())\n",
    "print(\"\\nValores después de rellenar:\")\n",
    "print(df_fill.loc[[2, 5, 8]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d105ac",
   "metadata": {},
   "source": [
    "#### Eliminación de duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e50b096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos datos con duplicados\n",
    "df_con_duplicados = pd.concat([df_csv, df_csv.iloc[0:3]], ignore_index=True)\n",
    "\n",
    "print(f\"Datos con duplicados (filas: {len(df_con_duplicados)}):\")\n",
    "print(df_con_duplicados)\n",
    "\n",
    "# Identificar duplicados\n",
    "duplicados = df_con_duplicados.duplicated()\n",
    "print(f\"\\nFilas duplicadas: {duplicados.sum()}\")\n",
    "\n",
    "# Eliminar duplicados\n",
    "df_sin_duplicados = df_con_duplicados.drop_duplicates()\n",
    "print(f\"\\nDatos sin duplicados (filas: {len(df_sin_duplicados)}):\")\n",
    "print(df_sin_duplicados.head())\n",
    "\n",
    "# También podemos eliminar duplicados basados en ciertas columnas\n",
    "df_sin_duplicados_ciudad = df_con_duplicados.drop_duplicates(subset=['ciudad'])\n",
    "print(f\"\\nDatos sin duplicados por ciudad (filas: {len(df_sin_duplicados_ciudad)}):\")\n",
    "print(df_sin_duplicados_ciudad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daa26dd",
   "metadata": {},
   "source": [
    "#### Corrección de errores y validación de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43edf0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos datos con errores\n",
    "df_con_errores = df_csv.copy()\n",
    "df_con_errores.loc[0, 'edad'] = -5  # Edad negativa (imposible)\n",
    "df_con_errores.loc[1, 'salario'] = -1000  # Salario negativo (imposible)\n",
    "df_con_errores.loc[2, 'ciudad'] = 'MaDRid'  # Inconsistencia en mayúsculas/minúsculas\n",
    "df_con_errores.loc[3, 'nombre'] = 'P3dr0'  # Caracteres no válidos en nombre\n",
    "\n",
    "print(\"Datos con errores:\")\n",
    "print(df_con_errores.iloc[0:4])\n",
    "\n",
    "# Corregimos los errores\n",
    "df_corregido = df_con_errores.copy()\n",
    "\n",
    "# 1. Corregir edades negativas\n",
    "df_corregido['edad'] = df_corregido['edad'].apply(lambda x: max(0, x))\n",
    "\n",
    "# 2. Corregir salarios negativos\n",
    "df_corregido['salario'] = df_corregido['salario'].apply(lambda x: max(0, x))\n",
    "\n",
    "# 3. Normalizar ciudades (todo a mayúsculas)\n",
    "df_corregido['ciudad'] = df_corregido['ciudad'].str.upper()\n",
    "\n",
    "# 4. Limpiar nombres (solo letras y espacios)\n",
    "df_corregido['nombre'] = df_corregido['nombre'].apply(\n",
    "    lambda x: re.sub(r'[^a-zA-ZáéíóúÁÉÍÓÚñÑ\\s]', '', x) if isinstance(x, str) else x\n",
    ")\n",
    "\n",
    "print(\"\\nDatos corregidos:\")\n",
    "print(df_corregido.iloc[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca91db81",
   "metadata": {},
   "source": [
    "### Normalización y estandarización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70da2215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usamos el dataset de Iris para ejemplos de normalización\n",
    "print(\"Datos originales de Iris:\")\n",
    "print(df_iris.describe())\n",
    "\n",
    "# Visualizamos la distribución original\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df_iris['sepal_length'], kde=True)\n",
    "plt.title('Distribución original de sepal_length')\n",
    "\n",
    "# 1. Min-Max Scaling (normalización)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler_minmax = MinMaxScaler()\n",
    "df_iris_minmax = df_iris.copy()\n",
    "df_iris_minmax[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']] = scaler_minmax.fit_transform(\n",
    "    df_iris[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]\n",
    ")\n",
    "\n",
    "print(\"\\nDatos normalizados con Min-Max Scaling:\")\n",
    "print(df_iris_minmax.describe())\n",
    "\n",
    "# 2. Standardization (estandarización)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler_standard = StandardScaler()\n",
    "df_iris_standard = df_iris.copy()\n",
    "df_iris_standard[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']] = scaler_standard.fit_transform(\n",
    "    df_iris[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]\n",
    ")\n",
    "\n",
    "print(\"\\nDatos estandarizados:\")\n",
    "print(df_iris_standard.describe())\n",
    "\n",
    "# Visualizamos la distribución estandarizada\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(df_iris_standard['sepal_length'], kde=True)\n",
    "plt.title('Distribución estandarizada de sepal_length')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618fab93",
   "metadata": {},
   "source": [
    "### Enriquecimiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db9bf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuamos con los datos de empleados\n",
    "df_enriquecido = df_csv.copy()\n",
    "\n",
    "# 1. Creación de nuevas columnas derivadas\n",
    "# Categoría de salario\n",
    "df_enriquecido['categoria_salario'] = pd.cut(\n",
    "    df_enriquecido['salario'],\n",
    "    bins=[0, 30000, 40000, 100000],\n",
    "    labels=['Bajo', 'Medio', 'Alto']\n",
    ")\n",
    "\n",
    "# Antigüedad simulada (años en la empresa)\n",
    "np.random.seed(42)\n",
    "df_enriquecido['antiguedad'] = np.random.randint(1, 10, size=len(df_enriquecido))\n",
    "\n",
    "# Bono anual (5% del salario por año de antigüedad)\n",
    "df_enriquecido['bono_anual'] = df_enriquecido['salario'] * 0.05 * df_enriquecido['antiguedad']\n",
    "df_enriquecido['bono_anual'] = df_enriquecido['bono_anual'].round(2)\n",
    "\n",
    "# 2. Agregación de datos externos\n",
    "# Simulamos datos de departamentos\n",
    "departamentos = pd.DataFrame({\n",
    "    'ciudad': ['MADRID', 'BARCELONA', 'SEVILLA', 'VALENCIA'],\n",
    "    'departamento_principal': ['Finanzas', 'Marketing', 'Operaciones', 'Tecnología'],\n",
    "    'costo_vida': [85, 90, 75, 80]  # Índice de costo de vida (ficticio)\n",
    "})\n",
    "\n",
    "# Unimos con los datos de empleados\n",
    "df_enriquecido['ciudad'] = df_enriquecido['ciudad'].str.upper()\n",
    "df_enriquecido = df_enriquecido.merge(departamentos, on='ciudad', how='left')\n",
    "\n",
    "# 3. Cálculo de salario ajustado por costo de vida\n",
    "df_enriquecido['salario_ajustado'] = df_enriquecido['salario'] / (df_enriquecido['costo_vida'] / 100)\n",
    "df_enriquecido['salario_ajustado'] = df_enriquecido['salario_ajustado'].round(2)\n",
    "\n",
    "print(\"Datos enriquecidos:\")\n",
    "df_enriquecido.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67317767",
   "metadata": {},
   "source": [
    "### Transformaciones de tiempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716c9eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos datos con fechas\n",
    "fechas_ventas = pd.DataFrame({\n",
    "    'fecha': pd.date_range(start='2023-01-01', periods=100),\n",
    "    'producto_id': np.random.randint(1, 6, size=100),\n",
    "    'cantidad': np.random.randint(1, 20, size=100),\n",
    "    'precio_unitario': np.random.uniform(10, 100, size=100).round(2)\n",
    "})\n",
    "\n",
    "fechas_ventas['monto'] = fechas_ventas['cantidad'] * fechas_ventas['precio_unitario']\n",
    "\n",
    "print(\"Datos de ventas con fechas:\")\n",
    "fechas_ventas.head()\n",
    "\n",
    "# 1. Extracción de componentes de fecha\n",
    "fechas_ventas['año'] = fechas_ventas['fecha'].dt.year\n",
    "fechas_ventas['mes'] = fechas_ventas['fecha'].dt.month\n",
    "fechas_ventas['dia'] = fechas_ventas['fecha'].dt.day\n",
    "fechas_ventas['dia_semana'] = fechas_ventas['fecha'].dt.day_name()\n",
    "fechas_ventas['trimestre'] = fechas_ventas['fecha'].dt.quarter\n",
    "\n",
    "print(\"\\nDatos con componentes de fecha extraídos:\")\n",
    "fechas_ventas.head()\n",
    "\n",
    "# 2. Agregación por período\n",
    "ventas_mensuales = fechas_ventas.groupby(['año', 'mes']).agg({\n",
    "    'monto': 'sum',\n",
    "    'cantidad': 'sum',\n",
    "    'fecha': 'count'\n",
    "}).rename(columns={'fecha': 'num_transacciones'}).reset_index()\n",
    "\n",
    "print(\"\\nVentas agregadas por mes:\")\n",
    "print(ventas_mensuales)\n",
    "\n",
    "# 3. Visualización de tendencias temporales\n",
    "plt.figure(figsize=(12, 6))\n",
    "ventas_diarias = fechas_ventas.groupby('fecha')['monto'].sum().reset_index()\n",
    "plt.plot(ventas_diarias['fecha'], ventas_diarias['monto'])\n",
    "plt.title('Tendencia de ventas diarias')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Monto de ventas')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f999bbe5",
   "metadata": {},
   "source": [
    "### Transformaciones estructurales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17e3671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de pivoteo de datos\n",
    "# Usamos los datos de ventas por fecha\n",
    "# Pivoteo: Convertir de formato largo a ancho\n",
    "ventas_pivot = fechas_ventas.pivot_table(\n",
    "    index='fecha',\n",
    "    columns='producto_id',\n",
    "    values='monto',\n",
    "    aggfunc='sum'\n",
    ").fillna(0)\n",
    "\n",
    "print(\"Datos pivoteados (ventas por producto y fecha):\")\n",
    "print(ventas_pivot.head())\n",
    "\n",
    "# Melt: Convertir de formato ancho a largo\n",
    "ventas_melt = ventas_pivot.reset_index().melt(\n",
    "    id_vars=['fecha'],\n",
    "    value_vars=ventas_pivot.columns,\n",
    "    var_name='producto_id',\n",
    "    value_name='monto'\n",
    ")\n",
    "\n",
    "print(\"\\nDatos convertidos de ancho a largo:\")\n",
    "print(ventas_melt.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fc1ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de agrupación y agregación\n",
    "ventas_por_producto = fechas_ventas.groupby('producto_id').agg({\n",
    "    'monto': ['sum', 'mean', 'min', 'max', 'count'],\n",
    "    'cantidad': ['sum', 'mean']\n",
    "})\n",
    "\n",
    "print(\"Datos agregados por producto:\")\n",
    "print(ventas_por_producto)\n",
    "\n",
    "# Aplanamos los nombres de columnas para facilitar su uso\n",
    "ventas_por_producto.columns = ['_'.join(col).strip() for col in ventas_por_producto.columns.values]\n",
    "ventas_por_producto = ventas_por_producto.reset_index()\n",
    "\n",
    "print(\"\\nDatos agregados con columnas aplanadas:\")\n",
    "print(ventas_por_producto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc6b4b7",
   "metadata": {},
   "source": [
    "### Codificación de variables categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec55564",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Usamos el dataset de Iris para ejemplos de codificación\n",
    "print(\"Datos originales de Iris (primeras filas):\")\n",
    "print(df_iris.head())\n",
    "\n",
    "# 1. One-Hot Encoding\n",
    "df_iris_onehot = pd.get_dummies(df_iris, columns=['species'], prefix='species')\n",
    "print(\"\\nDatos con One-Hot Encoding:\")\n",
    "print(df_iris_onehot.head())\n",
    "\n",
    "# 2. Label Encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df_iris_label = df_iris.copy()\n",
    "label_encoder = LabelEncoder()\n",
    "df_iris_label['species_encoded'] = label_encoder.fit_transform(df_iris_label['species'])\n",
    "\n",
    "print(\"\\nDatos con Label Encoding:\")\n",
    "print(df_iris_label.head())\n",
    "print(\"\\nMapeo de etiquetas:\")\n",
    "for i, label in enumerate(label_encoder.classes_):\n",
    "    print(f\"{label} -> {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc65fdbd",
   "metadata": {},
   "source": [
    "## 4. Fase de Carga (Load)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879d0b41",
   "metadata": {},
   "source": [
    "### Destinos de carga\n",
    "La fase de carga es donde los datos transformados se almacenan en un destino final. Los destinos comunes incluyen:\n",
    "1. Bases de datos relacionales:\n",
    "- PostgreSQL\n",
    "- MySQL, Oracle, SQL Server, etc.\n",
    "- Optimizadas para transacciones y consultas estructuradas\n",
    "2. Data Warehouses:\n",
    "- Snowflake, Amazon Redshift, Google BigQuery, etc.\n",
    "- Optimizados para consultas analíticas y grandes volúmenes\n",
    "3. Data Lakes:\n",
    "- Amazon S3, Azure Data Lake, Google Cloud Storage, etc.\n",
    "- Almacenamiento de datos en bruto y procesados\n",
    "4. Sistemas NoSQL:\n",
    "- MongoDB, Cassandra, etc.\n",
    "- Para datos no estructurados o semi-estructurados\n",
    "5. Archivos:\n",
    "- CSV, Parquet, Avro, ORC, etc.\n",
    "- Para intercambio de datos o análisis local"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09fb555",
   "metadata": {},
   "source": [
    "### Métodos de carga\n",
    "Existen diferentes métodos para cargar datos:\n",
    "1. Carga completa (Full Load):\n",
    "- Se reemplaza todo el conjunto de datos en el destino\n",
    "- Simple pero ineficiente para grandes volúmenes\n",
    "2. Carga incremental:\n",
    "- Solo se cargan los datos nuevos o modificados\n",
    "- Más eficiente pero requiere seguimiento de cambios\n",
    "3. Carga por lotes (Batch Load):\n",
    "- Los datos se cargan en lotes programados\n",
    "- Equilibrio entre rendimiento y actualidad\n",
    "4. Carga en tiempo real (Streaming Load):\n",
    "- Los datos se cargan continuamente\n",
    "- Para casos que requieren datos muy actualizados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ada52b",
   "metadata": {},
   "source": [
    "### Consideraciones de rendimiento\n",
    "Al diseñar la fase de carga, es importante considerar:\n",
    "- Volumen de datos: ¿Cuántos datos necesitamos cargar? ¿El destino puede manejarlos?\n",
    "- Frecuencia de carga: ¿Con qué frecuencia necesitamos actualizar los datos en el destino?\n",
    "- Ventanas de mantenimiento: ¿Hay períodos específicos para realizar cargas masivas?\n",
    "- Integridad de datos: ¿Cómo garantizamos que los datos se carguen correctamente?\n",
    "- Transaccionalidad: ¿Necesitamos garantizar que todas las cargas sean atómicas?\n",
    "- Paralelismo: ¿Podemos cargar datos en paralelo para mejorar el rendimiento?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fb50c5",
   "metadata": {},
   "source": [
    "### Ejemplos prácticos de carga"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de744573",
   "metadata": {},
   "source": [
    "#### Ejemplo 1: Carga en base de datos SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3b321b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_to_database(df, table_name, connection, if_exists='replace'):\n",
    "    \"\"\"Carga datos a una base de datos\"\"\"\n",
    "    print(f\"Cargando datos a la tabla {table_name}...\")\n",
    "    df.to_sql(table_name, connection, if_exists=if_exists, index=False)\n",
    "    print(f\"Datos cargados exitosamente a {table_name}\")\n",
    "    \n",
    "    # Verificamos la carga\n",
    "    count_query = f\"SELECT COUNT(*) FROM {table_name}\"\n",
    "    count = pd.read_sql(count_query, connection).iloc[0, 0]\n",
    "    print(f\"Registros cargados: {count}\")\n",
    "\n",
    "# Usamos los datos enriquecidos de empleados\n",
    "conn_destino = sqlite3.connect('datawarehouse.db')\n",
    "load_to_database(df_enriquecido, 'empleados_dim', conn_destino, 'replace')\n",
    "\n",
    "# Verificamos con una consulta\n",
    "query_result = pd.read_sql(\"SELECT * FROM empleados_dim LIMIT 5\", conn_destino)\n",
    "print(\"\\nVerificación de datos cargados en la base de datos:\")\n",
    "print(query_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6054717",
   "metadata": {},
   "source": [
    "#### Ejemplo 2: Carga incremental en base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b524ed",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Simulamos datos nuevos para carga incremental\n",
    "nuevos_empleados = pd.DataFrame({\n",
    "    'id': range(11, 16),\n",
    "    'nombre': ['Roberto', 'Elena', 'Javier', 'Carmen', 'Francisco'],\n",
    "    'edad': [26, 34, 29, 31, 40],\n",
    "    'ciudad': ['MADRID', 'VALENCIA', 'BARCELONA', 'SEVILLA', 'MADRID'],\n",
    "    'salario': [36000, 39000, 41000, 34000, 44000]\n",
    "})\n",
    "\n",
    "# Enriquecemos los nuevos datos (similar a los anteriores)\n",
    "nuevos_empleados['categoria_salario'] = pd.cut(\n",
    "    nuevos_empleados['salario'],\n",
    "    bins=[0, 30000, 40000, 100000],\n",
    "    labels=['Bajo', 'Medio', 'Alto']\n",
    ")\n",
    "\n",
    "nuevos_empleados['antiguedad'] = np.random.randint(1, 5, size=len(nuevos_empleados))\n",
    "nuevos_empleados['bono_anual'] = nuevos_empleados['salario'] * 0.05 * nuevos_empleados['antiguedad']\n",
    "nuevos_empleados['bono_anual'] = nuevos_empleados['bono_anual'].round(2)\n",
    "\n",
    "# Unimos con datos de departamentos\n",
    "nuevos_empleados = nuevos_empleados.merge(departamentos, on='ciudad', how='left')\n",
    "nuevos_empleados['salario_ajustado'] = nuevos_empleados['salario'] / (nuevos_empleados['costo_vida'] / 100)\n",
    "nuevos_empleados['salario_ajustado'] = nuevos_empleados['salario_ajustado'].round(2)\n",
    "\n",
    "# Carga incremental (append)\n",
    "load_to_database(nuevos_empleados, 'empleados_dim', conn_destino, 'append')\n",
    "\n",
    "# Verificamos la carga incremental\n",
    "total_empleados = pd.read_sql(\"SELECT COUNT(*) as total FROM empleados_dim\", conn_destino)\n",
    "print(f\"\\nTotal de empleados después de carga incremental: {total_empleados.iloc[0, 0]}\")\n",
    "\n",
    "# Verificamos los nuevos registros\n",
    "nuevos_registros = pd.read_sql(\"SELECT * FROM empleados_dim WHERE id > 10\", conn_destino)\n",
    "print(\"\\nNuevos registros cargados:\")\n",
    "print(nuevos_registros)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb48d12",
   "metadata": {},
   "source": [
    "#### Ejemplo 3: Carga en archivos CSV y Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b628de3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_to_csv(df, file_path):\n",
    "    \"\"\"Carga datos a un archivo CSV\"\"\"\n",
    "    print(f\"Guardando datos en {file_path}...\")\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Datos guardados exitosamente en {file_path}\")\n",
    "    \n",
    "    # Verificamos el archivo\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    print(f\"Tamaño del archivo: {file_size} bytes\")\n",
    "\n",
    "def load_to_excel(df, file_path):\n",
    "    \"\"\"Carga datos a un archivo Excel\"\"\"\n",
    "    print(f\"Guardando datos en {file_path}...\")\n",
    "    df.to_excel(file_path, index=False)\n",
    "    print(f\"Datos guardados exitosamente en {file_path}\")\n",
    "    \n",
    "    # Verificamos el archivo\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    print(f\"Tamaño del archivo: {file_size} bytes\")\n",
    "\n",
    "# Cargamos los datos de ventas agregados\n",
    "load_to_csv(ventas_por_producto, 'ventas_por_producto.csv')\n",
    "load_to_excel(ventas_mensuales, 'ventas_mensuales.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae5a0de",
   "metadata": {},
   "source": [
    "#### Ejemplo 4: Carga en formato Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d17fdc",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import pyarrow.parquet as pq\n",
    "    import pyarrow as pa\n",
    "    \n",
    "    def load_to_parquet(df, file_path):\n",
    "        \"\"\"Carga datos a un archivo Parquet\"\"\"\n",
    "        print(f\"Guardando datos en formato Parquet: {file_path}...\")\n",
    "        df.to_parquet(file_path, index=False)\n",
    "        print(f\"Datos guardados exitosamente en {file_path}\")\n",
    "        \n",
    "        # Verificamos el archivo\n",
    "        file_size = os.path.getsize(file_path)\n",
    "        print(f\"Tamaño del archivo: {file_size} bytes\")\n",
    "    \n",
    "    # Cargamos los datos de Iris en formato Parquet\n",
    "    load_to_parquet(df_iris, 'iris_dataset.parquet')\n",
    "    \n",
    "    # Leemos el archivo Parquet para verificar\n",
    "    df_parquet = pd.read_parquet('iris_dataset.parquet')\n",
    "    print(\"\\nDatos leídos desde Parquet (primeras filas):\")\n",
    "    print(df_parquet.head())\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"Para usar Parquet, instala: pip install pyarrow\")\n",
    "    print(\"Continuando sin la demostración de Parquet...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3c473d",
   "metadata": {},
   "source": [
    "#### Ejemplo 5: Carga en formato JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1c7a9b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_to_json(df, file_path, orient='records'):\n",
    "    \"\"\"Carga datos a un archivo JSON\"\"\"\n",
    "    print(f\"Guardando datos en formato JSON: {file_path}...\")\n",
    "    df.to_json(file_path, orient=orient)\n",
    "    print(f\"Datos guardados exitosamente en {file_path}\")\n",
    "    \n",
    "    # Verificamos el archivo\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    print(f\"Tamaño del archivo: {file_size} bytes\")\n",
    "\n",
    "# Cargamos los datos de empleados en formato JSON\n",
    "load_to_json(df_enriquecido, 'empleados_enriquecidos.json')\n",
    "\n",
    "# Leemos el archivo JSON para verificar\n",
    "with open('empleados_enriquecidos.json', 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "print(f\"\\nDatos leídos desde JSON (número de registros: {len(json_data)}):\")\n",
    "print(json_data[0])  # Mostramos el primer registro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e32fff",
   "metadata": {},
   "source": [
    "## 5. Proyecto Práctico Completo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0c1ac8",
   "metadata": {},
   "source": [
    "### Implementación de un pipeline ETL completo\n",
    "Ahora implementaremos un proceso ETL completo con un caso más realista:\n",
    "1. Extraer datos de ventas de múltiples fuentes\n",
    "2. Transformar y limpiar los datos\n",
    "3. Cargar los datos en una base de datos para análisis\n",
    "Este ejemplo simula un escenario donde tenemos datos de ventas de diferentes regiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8cb567",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Creamos datos simulados para el ejemplo\n",
    "def create_sample_data():\n",
    "    \"\"\"Crea archivos de muestra para el proyecto ETL\"\"\"\n",
    "    \n",
    "    # Datos de ventas región Norte\n",
    "    norte_data = pd.DataFrame({\n",
    "        'fecha': pd.date_range(start='2023-01-01', periods=100),\n",
    "        'producto_id': np.random.randint(1, 21, size=100),\n",
    "        'cantidad': np.random.randint(1, 50, size=100),\n",
    "        'precio_unitario': np.random.uniform(10, 100, size=100).round(2),\n",
    "        'cliente_id': np.random.randint(1, 51, size=100),\n",
    "        'region': 'Norte'\n",
    "    })\n",
    "    norte_data.to_csv('ventas_norte.csv', index=False)\n",
    "    \n",
    "    # Datos de ventas región Sur\n",
    "    sur_data = pd.DataFrame({\n",
    "        'fecha': pd.date_range(start='2023-01-01', periods=120),\n",
    "        'producto_id': np.random.randint(1, 21, size=120),\n",
    "        'cantidad': np.random.randint(1, 50, size=120),\n",
    "        'precio_unitario': np.random.uniform(10, 100, size=120).round(2),\n",
    "        'cliente_id': np.random.randint(1, 51, size=120),\n",
    "        'region': 'Sur'\n",
    "    })\n",
    "    sur_data.to_csv('ventas_sur.csv', index=False)\n",
    "    \n",
    "    # Datos de ventas región Este (en formato JSON)\n",
    "    este_data = pd.DataFrame({\n",
    "        'fecha': pd.date_range(start='2023-01-01', periods=80),\n",
    "        'producto_id': np.random.randint(1, 21, size=80),\n",
    "        'cantidad': np.random.randint(1, 50, size=80),\n",
    "        'precio_unitario': np.random.uniform(10, 100, size=80).round(2),\n",
    "        'cliente_id': np.random.randint(1, 51, size=80),\n",
    "        'region': 'Este'\n",
    "    })\n",
    "    # Convertimos fechas a string para JSON\n",
    "    este_data['fecha'] = este_data['fecha'].dt.strftime('%Y-%m-%d')\n",
    "    este_data.to_json('ventas_este.json', orient='records')\n",
    "    \n",
    "    # Datos de productos (en formato JSON)\n",
    "    productos = pd.DataFrame({\n",
    "        'producto_id': range(1, 21),\n",
    "        'nombre': [f'Producto {i}' for i in range(1, 21)],\n",
    "        'categoria': np.random.choice(['Electrónica', 'Ropa', 'Hogar', 'Alimentos', 'Juguetes'], size=20),\n",
    "        'proveedor_id': np.random.randint(1, 6, size=20)\n",
    "    })\n",
    "    productos.to_json('productos.json', orient='records')\n",
    "    \n",
    "    # Datos de clientes (en base de datos)\n",
    "    clientes = pd.DataFrame({\n",
    "        'cliente_id': range(1, 51),\n",
    "        'nombre': [f'Cliente {i}' for i in range(1, 51)],\n",
    "        'email': [f'cliente{i}@ejemplo.com' for i in range(1, 51)],\n",
    "        'tipo': np.random.choice(['Regular', 'Premium', 'VIP'], size=50),\n",
    "        'fecha_registro': pd.date_range(start='2022-01-01', periods=50)\n",
    "    })\n",
    "    \n",
    "    # Creamos una base de datos para clientes\n",
    "    conn_ventas = sqlite3.connect('ventas.db')\n",
    "    clientes.to_sql('clientes', conn_ventas, if_exists='replace', index=False)\n",
    "    conn_ventas.close()\n",
    "    \n",
    "    print(\"Datos de muestra creados exitosamente.\")\n",
    "    return norte_data, sur_data, este_data, productos, clientes\n",
    "\n",
    "# Creamos los datos de muestra\n",
    "norte_data, sur_data, este_data, productos, clientes = create_sample_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dd02c8",
   "metadata": {},
   "source": [
    "### Implementación del proceso ETL completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dbc3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def etl_ventas():\n",
    "    \"\"\"Proceso ETL completo para datos de ventas\"\"\"\n",
    "    \n",
    "    print(\"Iniciando proceso ETL para datos de ventas...\")\n",
    "    print(f\"Hora de inicio: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    # 1. EXTRACCIÓN\n",
    "    print(\"\\n--- FASE DE EXTRACCIÓN ---\")\n",
    "    \n",
    "    # Extraer datos de ventas de CSV\n",
    "    print(\"Extrayendo datos de ventas región Norte...\")\n",
    "    ventas_norte = pd.read_csv('ventas_norte.csv')\n",
    "    print(f\"Registros extraídos: {len(ventas_norte)}\")\n",
    "    \n",
    "    print(\"\\nExtrayendo datos de ventas región Sur...\")\n",
    "    ventas_sur = pd.read_csv('ventas_sur.csv')\n",
    "    print(f\"Registros extraídos: {len(ventas_sur)}\")\n",
    "    \n",
    "    # Extraer datos de ventas de JSON\n",
    "    print(\"\\nExtrayendo datos de ventas región Este...\")\n",
    "    with open('ventas_este.json', 'r') as f:\n",
    "        ventas_este_json = json.load(f)\n",
    "    ventas_este = pd.DataFrame(ventas_este_json)\n",
    "    print(f\"Registros extraídos: {len(ventas_este)}\")\n",
    "    \n",
    "    # Extraer datos de productos de JSON\n",
    "    print(\"\\nExtrayendo datos de productos...\")\n",
    "    with open('productos.json', 'r') as f:\n",
    "        productos_json = json.load(f)\n",
    "    productos = pd.DataFrame(productos_json)\n",
    "    print(f\"Productos extraídos: {len(productos)}\")\n",
    "    \n",
    "    # Extraer datos de clientes de la base de datos\n",
    "    print(\"\\nExtrayendo datos de clientes desde la base de datos...\")\n",
    "    conn_ventas = sqlite3.connect('ventas.db')\n",
    "    clientes = pd.read_sql(\"SELECT * FROM clientes\", conn_ventas)\n",
    "    print(f\"Clientes extraídos: {len(clientes)}\")\n",
    "    \n",
    "    # 2. TRANSFORMACIÓN\n",
    "    print(\"\\n--- FASE DE TRANSFORMACIÓN ---\")\n",
    "    \n",
    "    # Combinar datos de ventas\n",
    "    print(\"Combinando datos de ventas...\")\n",
    "    ventas = pd.concat([ventas_norte, ventas_sur, ventas_este], ignore_index=True)\n",
    "    print(f\"Total de registros de ventas: {len(ventas)}\")\n",
    "    \n",
    "    # Convertir fechas\n",
    "    print(\"\\nConvirtiendo fechas...\")\n",
    "    ventas['fecha'] = pd.to_datetime(ventas['fecha'])\n",
    "    print(f\"Rango de fechas: {ventas['fecha'].min()} a {ventas['fecha'].max()}\")\n",
    "    \n",
    "    # Calcular monto total\n",
    "    print(\"\\nCalculando monto total...\")\n",
    "    ventas['monto_total'] = ventas['cantidad'] * ventas['precio_unitario']\n",
    "    ventas['monto_total'] = ventas['monto_total'].round(2)\n",
    "    \n",
    "    # Añadir información de mes y año para análisis\n",
    "    print(\"\\nAgregando columnas de tiempo...\")\n",
    "    ventas['mes'] = ventas['fecha'].dt.month\n",
    "    ventas['año'] = ventas['fecha'].dt.year\n",
    "    ventas['trimestre'] = ventas['fecha'].dt.quarter\n",
    "    \n",
    "    # Unir con información de productos\n",
    "    print(\"\\nUniendo con información de productos...\")\n",
    "    ventas_con_productos = ventas.merge(\n",
    "        productos[['producto_id', 'nombre', 'categoria']],\n",
    "        on='producto_id',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Unir con información de clientes\n",
    "    print(\"\\nUniendo con información de clientes...\")\n",
    "    ventas_completas = ventas_con_productos.merge(\n",
    "        clientes[['cliente_id', 'nombre', 'tipo']],\n",
    "        on='cliente_id',\n",
    "        how='left',\n",
    "        suffixes=('_producto', '_cliente')\n",
    "    )\n",
    "    \n",
    "    # Verificar valores nulos\n",
    "    print(\"\\nVerificando valores nulos...\")\n",
    "    nulos = ventas_completas.isnull().sum()\n",
    "    if nulos.sum() > 0:\n",
    "        print(\"Se encontraron valores nulos:\")\n",
    "        print(nulos[nulos > 0])\n",
    "        \n",
    "        # Rellenar valores nulos\n",
    "        print(\"Rellenando valores nulos...\")\n",
    "        ventas_completas['nombre_producto'].fillna('Desconocido', inplace=True)\n",
    "        ventas_completas['categoria'].fillna('Sin categoría', inplace=True)\n",
    "        ventas_completas['nombre_cliente'].fillna('Cliente no registrado', inplace=True)\n",
    "        ventas_completas['tipo'].fillna('Regular', inplace=True)\n",
    "    else:\n",
    "        print(\"No se encontraron valores nulos.\")\n",
    "    \n",
    "    # Crear tabla de hechos y dimensiones para modelo estrella\n",
    "    print(\"\\nCreando modelo dimensional...\")\n",
    "    \n",
    "    # Dimensión tiempo\n",
    "    dim_tiempo = ventas_completas[['fecha', 'mes', 'año', 'trimestre']].drop_duplicates().reset_index(drop=True)\n",
    "    dim_tiempo['tiempo_id'] = dim_tiempo.index + 1\n",
    "    \n",
    "    # Dimensión producto\n",
    "    dim_producto = ventas_completas[['producto_id', 'nombre_producto', 'categoria']].drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    # Dimensión cliente\n",
    "    dim_cliente = ventas_completas[['cliente_id', 'nombre_cliente', 'tipo']].drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    # Dimensión región\n",
    "    dim_region = ventas_completas[['region']].drop_duplicates().reset_index(drop=True)\n",
    "    dim_region['region_id'] = dim_region.index + 1\n",
    "    \n",
    "    # Tabla de hechos\n",
    "    hechos_ventas = ventas_completas.merge(dim_tiempo[['fecha', 'tiempo_id']], on='fecha')\n",
    "    hechos_ventas = hechos_ventas.merge(dim_region[['region', 'region_id']], on='region')\n",
    "    \n",
    "    hechos_ventas = hechos_ventas[[\n",
    "        'tiempo_id', 'producto_id', 'cliente_id', 'region_id',\n",
    "        'cantidad', 'precio_unitario', 'monto_total'\n",
    "    ]]\n",
    "    \n",
    "    # 3. CARGA\n",
    "    print(\"\\n--- FASE DE CARGA ---\")\n",
    "    \n",
    "    # Crear conexión a la base de datos\n",
    "    print(\"Conectando a la base de datos...\")\n",
    "    conn_dw = sqlite3.connect('ventas_datawarehouse.db')\n",
    "    \n",
    "    # Cargar dimensiones\n",
    "    print(\"\\nCargando dimensiones...\")\n",
    "    dim_tiempo.to_sql('dim_tiempo', conn_dw, if_exists='replace', index=False)\n",
    "    dim_producto.to_sql('dim_producto', conn_dw, if_exists='replace', index=False)\n",
    "    dim_cliente.to_sql('dim_cliente', conn_dw, if_exists='replace', index=False)\n",
    "    dim_region.to_sql('dim_region', conn_dw, if_exists='replace', index=False)\n",
    "    \n",
    "    # Cargar tabla de hechos\n",
    "    print(\"Cargando tabla de hechos...\")\n",
    "    hechos_ventas.to_sql('hechos_ventas', conn_dw, if_exists='replace', index=False)\n",
    "    \n",
    "    # Verificar carga\n",
    "    print(\"\\nVerificando carga de datos...\")\n",
    "    tablas = pd.read_sql(\"SELECT name FROM sqlite_master WHERE type='table'\", conn_dw)\n",
    "    print(\"Tablas creadas:\")\n",
    "    print(tablas)\n",
    "    \n",
    "    for tabla in tablas['name']:\n",
    "        count = pd.read_sql(f\"SELECT COUNT(*) as count FROM {tabla}\", conn_dw).iloc[0, 0]\n",
    "        print(f\"Tabla {tabla}: {count} registros\")\n",
    "    \n",
    "    # Crear vista para análisis\n",
    "    print(\"\\nCreando vista para análisis...\")\n",
    "    vista_query = \"\"\"\n",
    "    CREATE VIEW IF NOT EXISTS ventas_analisis AS\n",
    "    SELECT \n",
    "        t.año, \n",
    "        t.mes, \n",
    "        t.trimestre,\n",
    "        p.nombre_producto,\n",
    "        p.categoria,\n",
    "        c.nombre_cliente,\n",
    "        c.tipo,\n",
    "        r.region,\n",
    "        SUM(h.cantidad) as total_unidades,\n",
    "        SUM(h.monto_total) as total_ventas\n",
    "    FROM hechos_ventas h\n",
    "    JOIN dim_tiempo t ON h.tiempo_id = t.tiempo_id\n",
    "    JOIN dim_producto p ON h.producto_id = p.producto_id\n",
    "    JOIN dim_cliente c ON h.cliente_id = c.cliente_id\n",
    "    JOIN dim_region r ON h.region_id = r.region_id\n",
    "    GROUP BY t.año, t.mes, t.trimestre, p.nombre_producto, p.categoria, c.nombre_cliente, c.tipo, r.region\n",
    "    \"\"\"\n",
    "    \n",
    "    conn_dw.execute(vista_query)\n",
    "    conn_dw.commit()\n",
    "    \n",
    "    # Ejemplo de consulta\n",
    "    print(\"\\nEjemplo de consulta a la vista de análisis:\")\n",
    "    query_result = pd.read_sql(\"SELECT * FROM ventas_analisis LIMIT 10\", conn_dw)\n",
    "    print(query_result)\n",
    "    \n",
    "    # Cerrar conexiones\n",
    "    conn_ventas.close()\n",
    "    conn_dw.close()\n",
    "    \n",
    "    print(\"\\nProceso ETL completado exitosamente.\")\n",
    "    print(f\"Hora de finalización: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    return {\n",
    "        'dim_tiempo': dim_tiempo,\n",
    "        'dim_producto': dim_producto,\n",
    "        'dim_cliente': dim_cliente,\n",
    "        'dim_region': dim_region,\n",
    "        'hechos_ventas': hechos_ventas\n",
    "    }\n",
    "\n",
    "# Ejecutamos el proceso ETL\n",
    "resultado_etl = etl_ventas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc6a9ba",
   "metadata": {},
   "source": [
    "### Análisis de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c73f2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conectamos a la base de datos\n",
    "conn_dw = sqlite3.connect('ventas_datawarehouse.db')\n",
    "\n",
    "# Ventas por categoría\n",
    "print(\"\\nVentas por categoría de producto:\")\n",
    "ventas_categoria = pd.read_sql(\"\"\"\n",
    "SELECT categoria, SUM(total_ventas) as ventas_totales\n",
    "FROM ventas_analisis\n",
    "GROUP BY categoria\n",
    "ORDER BY ventas_totales DESC\n",
    "\"\"\", conn_dw)\n",
    "\n",
    "print(ventas_categoria)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=ventas_categoria, x='categoria', y='ventas_totales')\n",
    "plt.title('Ventas Totales por Categoría')\n",
    "plt.xlabel('Categoría')\n",
    "plt.ylabel('Ventas Totales ($)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Ventas por región y trimestre\n",
    "print(\"\\nVentas por región y trimestre:\")\n",
    "ventas_region_trimestre = pd.read_sql(\"\"\"\n",
    "SELECT region, trimestre, SUM(total_ventas) as ventas_totales\n",
    "FROM ventas_analisis\n",
    "GROUP BY region, trimestre\n",
    "ORDER BY region, trimestre\n",
    "\"\"\", conn_dw)\n",
    "\n",
    "print(ventas_region_trimestre)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "ventas_pivot = ventas_region_trimestre.pivot(index='region', columns='trimestre', values='ventas_totales')\n",
    "ventas_pivot.plot(kind='bar', figsize=(10, 6))\n",
    "plt.title('Ventas por Región y Trimestre')\n",
    "plt.xlabel('Región')\n",
    "plt.ylabel('Ventas Totales ($)')\n",
    "plt.legend(title='Trimestre')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Ventas por tipo de cliente\n",
    "print(\"\\nVentas por tipo de cliente:\")\n",
    "ventas_tipo_cliente = pd.read_sql(\"\"\"\n",
    "SELECT tipo, SUM(total_ventas) as ventas_totales, COUNT(DISTINCT nombre_cliente) as num_clientes\n",
    "FROM ventas_analisis\n",
    "GROUP BY tipo\n",
    "ORDER BY ventas_totales DESC\n",
    "\"\"\", conn_dw)\n",
    "\n",
    "print(ventas_tipo_cliente)\n",
    "\n",
    "# Añadimos columna de ventas promedio por cliente\n",
    "ventas_tipo_cliente['ventas_promedio_por_cliente'] = (ventas_tipo_cliente['ventas_totales'] / ventas_tipo_cliente['num_clientes']).round(2)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=ventas_tipo_cliente, x='tipo', y='ventas_promedio_por_cliente')\n",
    "plt.title('Ventas Promedio por Tipo de Cliente')\n",
    "plt.xlabel('Tipo de Cliente')\n",
    "plt.ylabel('Ventas Promedio ($)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cerramos la conexión\n",
    "conn_dw.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a3dcb7",
   "metadata": {},
   "source": [
    "## 6. Mejores Prácticas y Consideraciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cce1a7",
   "metadata": {},
   "source": [
    "### Monitoreo y logging\n",
    "El monitoreo y logging son fundamentales para mantener procesos ETL robustos:\n",
    "1. Logging detallado:\n",
    "- Registrar inicio y fin de cada fase\n",
    "- Capturar errores y excepciones\n",
    "- Registrar métricas de rendimiento\n",
    "2. Alertas:\n",
    "- Configurar alertas para fallos\n",
    "- Notificar sobre problemas de calidad de datos\n",
    "- Alertar sobre tiempos de ejecución anormales\n",
    "3. Dashboards de monitoreo:\n",
    "- Visualizar métricas clave\n",
    "- Seguir tendencias de rendimiento\n",
    "- Identificar cuellos de botella "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724d9bcd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Ejemplo simple de logging en un proceso ETL\n",
    "import logging\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('etl_process.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger = logging.getLogger('ETL_Ventas')\n",
    "\n",
    "def etl_con_logging():\n",
    "    \"\"\"Ejemplo de ETL con logging\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Inicio del proceso\n",
    "        logger.info(\"Iniciando proceso ETL\")\n",
    "        \n",
    "        # Fase de extracción\n",
    "        logger.info(\"Iniciando fase de extracción\")\n",
    "        try:\n",
    "            # Simulamos extracción\n",
    "            logger.info(\"Extrayendo datos de ventas_norte.csv\")\n",
    "            df_norte = pd.read_csv('ventas_norte.csv')\n",
    "            logger.info(f\"Extraídos {len(df_norte)} registros de ventas_norte.csv\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error en extracción de ventas_norte.csv: {str(e)}\")\n",
    "            raise\n",
    "        \n",
    "        # Fase de transformación\n",
    "        logger.info(\"Iniciando fase de transformación\")\n",
    "        try:\n",
    "            # Simulamos transformación\n",
    "            logger.info(\"Calculando montos totales\")\n",
    "            df_norte['monto_total'] = df_norte['cantidad'] * df_norte['precio_unitario']\n",
    "            \n",
    "            # Verificamos calidad de datos\n",
    "            nulos = df_norte.isnull().sum().sum()\n",
    "            if nulos > 0:\n",
    "                logger.warning(f\"Se encontraron {nulos} valores nulos en los datos\")\n",
    "            \n",
    "            logger.info(\"Transformación completada\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error en transformación: {str(e)}\")\n",
    "            raise\n",
    "        \n",
    "        # Fase de carga\n",
    "        logger.info(\"Iniciando fase de carga\")\n",
    "        try:\n",
    "            # Simulamos carga\n",
    "            logger.info(\"Cargando datos en base de datos\")\n",
    "            # Aquí iría el código de carga\n",
    "            logger.info(\"Carga completada exitosamente\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error en carga: {str(e)}\")\n",
    "            raise\n",
    "        \n",
    "        # Fin del proceso\n",
    "        logger.info(\"Proceso ETL completado exitosamente\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.critical(f\"El proceso ETL falló: {str(e)}\")\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Ejecutamos el proceso con logging\n",
    "etl_con_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1016d3",
   "metadata": {},
   "source": [
    "### Manejo de errores\n",
    "Un buen manejo de errores es crucial para procesos ETL robustos:\n",
    "1. Estrategias de manejo de errores:\n",
    "- Reintentos para errores transitorios\n",
    "- Manejo de excepciones específicas\n",
    "- Rollback de transacciones en caso de error\n",
    "2. Validación de datos:\n",
    "- Verificar integridad antes de cargar\n",
    "- Validar contra reglas de negocio\n",
    "- Registrar y reportar anomalías\n",
    "3. Recuperación:\n",
    "- Puntos de control (checkpoints)\n",
    "- Capacidad de reanudar desde el último punto exitoso\n",
    "- Procedimientos de recuperación documentados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170eeed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_with_retry(file_path, max_retries=3, retry_delay=2):\n",
    "    \"\"\"Extrae datos con reintentos en caso de error\"\"\"\n",
    "    import time\n",
    "\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            logger.info(f\"Intento {attempt} de extracción desde {file_path}\")\n",
    "\n",
    "            # Simulamos una posible falla aleatoria\n",
    "            if attempt == 1 and np.random.random() < 0.5:\n",
    "                raise IOError(\"Error simulado de lectura de archivo\")\n",
    "\n",
    "            # Extracción real\n",
    "            data = pd.read_csv(file_path)\n",
    "            logger.info(f\"Extracción exitosa desde {file_path}\")\n",
    "            return data\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error en intento {attempt}: {str(e)}\")\n",
    "\n",
    "            if attempt < max_retries:\n",
    "                logger.info(f\"Reintentando en {retry_delay} segundos...\")\n",
    "                time.sleep(retry_delay)\n",
    "            else:\n",
    "                logger.error(f\"Fallaron todos los intentos de extracción desde {file_path}\")\n",
    "                raise\n",
    "\n",
    "# Ejemplo de uso\n",
    "try:\n",
    "    data = extract_with_retry('ventas_norte.csv')\n",
    "    print(\"Extracción completada con éxito\")\n",
    "except Exception as e:\n",
    "    print(f\"Error final: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1503bc38",
   "metadata": {},
   "source": [
    "### Escalabilidad\n",
    "Diseñar procesos ETL escalables es esencial para manejar volúmenes crecientes de datos:\n",
    "1. Procesamiento paralelo:\n",
    "- Dividir datos en particiones\n",
    "- Procesar particiones en paralelo\n",
    "- Utilizar múltiples hilos o procesos\n",
    "2. Procesamiento distribuido:\n",
    "- Frameworks como Apache Spark o Dask\n",
    "- Distribución de carga entre múltiples nodos\n",
    "- Escalado horizontal\n",
    "3. Optimización de recursos:\n",
    "- Procesamiento por lotes\n",
    "- Gestión eficiente de memoria\n",
    "- Compresión de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12f4f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo simple de procesamiento paralelo con Pandas\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import multiprocessing\n",
    "\n",
    "def procesar_particion(df_partition):\n",
    "    \"\"\"Procesa una partición de datos\"\"\"\n",
    "    # Simulamos algún procesamiento\n",
    "    df_result = df_partition.copy()\n",
    "    df_result['monto_total'] = df_result['cantidad'] * df_result['precio_unitario']\n",
    "    df_result['descuento'] = df_result['monto_total'] * 0.1\n",
    "    df_result['monto_final'] = df_result['monto_total'] - df_result['descuento']\n",
    "\n",
    "    # Simulamos un procesamiento intensivo\n",
    "    import time\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    return df_result\n",
    "\n",
    "def procesar_en_paralelo(df, n_partitions=None):\n",
    "    \"\"\"Procesa un DataFrame en paralelo\"\"\"\n",
    "    if n_partitions is None:\n",
    "        n_partitions = multiprocessing.cpu_count()\n",
    "\n",
    "    # Dividimos el DataFrame en particiones\n",
    "    df_split = np.array_split(df, n_partitions)\n",
    "\n",
    "    # Procesamos en paralelo\n",
    "    print(f\"Procesando {len(df)} registros en {n_partitions} particiones...\")\n",
    "\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=n_partitions) as executor:\n",
    "        results = list(executor.map(procesar_particion, df_split))\n",
    "\n",
    "    # Combinamos los resultados\n",
    "    df_result = pd.concat(results)\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    duration = (end_time - start_time).total_seconds()\n",
    "\n",
    "    print(f\"Procesamiento completado en {duration:.2f} segundos\")\n",
    "\n",
    "    return df_result\n",
    "\n",
    "# Cargamos datos para el ejemplo\n",
    "df_ventas = pd.read_csv('ventas_norte.csv')\n",
    "\n",
    "# Procesamiento secuencial para comparación\n",
    "print(\"Procesamiento secuencial:\")\n",
    "start_time = datetime.now()\n",
    "df_result_seq = procesar_particion(df_ventas)\n",
    "end_time = datetime.now()\n",
    "duration_seq = (end_time - start_time).total_seconds()\n",
    "print(f\"Tiempo secuencial: {duration_seq:.2f} segundos\")\n",
    "\n",
    "# Procesamiento paralelo\n",
    "print(\"\\nProcesamiento paralelo:\")\n",
    "df_result_par = procesar_en_paralelo(df_ventas)\n",
    "\n",
    "# Verificamos que los resultados sean iguales\n",
    "print(\"\\nVerificación de resultados:\")\n",
    "print(f\"Filas en resultado secuencial: {len(df_result_seq)}\")\n",
    "print(f\"Filas en resultado paralelo: {len(df_result_par)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff22aee1",
   "metadata": {},
   "source": [
    "### Seguridad\n",
    "La seguridad es un aspecto crítico en los procesos ETL, especialmente cuando se manejan datos sensibles:\n",
    "1. Protección de datos:\n",
    "- Cifrado de datos en tránsito y en reposo\n",
    "- Enmascaramiento de datos sensibles\n",
    "- Gestión segura de credenciales\n",
    "2. Control de acceso:\n",
    "- Principio de mínimo privilegio\n",
    "- Autenticación y autorización\n",
    "- Auditoría de accesos\n",
    "3. Cumplimiento normativo:\n",
    "- GDPR, HIPAA, PCI-DSS, etc.\n",
    "- Retención y eliminación de datos\n",
    "- Documentación de lineamientos de seguridad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64af508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de manejo seguro de credenciales\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import base64\n",
    "\n",
    "# Simulamos un archivo .env (en producción, este archivo no se incluiría en el control de versiones)\n",
    "with open('.env', 'w') as f:\n",
    "    f.write(\"DB_USER=admin\\n\")\n",
    "    f.write(\"DB_PASSWORD=secretpassword123\\n\")\n",
    "    f.write(\"DB_HOST=database.example.com\\n\")\n",
    "    f.write(\"DB_PORT=5432\\n\")\n",
    "    f.write(\"DB_NAME=datawarehouse\\n\")\n",
    "\n",
    "# Cargamos variables de entorno\n",
    "load_dotenv()\n",
    "\n",
    "def get_db_connection_string():\n",
    "    \"\"\"Obtiene la cadena de conexión a la base de datos de forma segura\"\"\"\n",
    "    user = os.getenv('DB_USER')\n",
    "    password = os.getenv('DB_PASSWORD')\n",
    "    host = os.getenv('DB_HOST')\n",
    "    port = os.getenv('DB_PORT')\n",
    "    db_name = os.getenv('DB_NAME')\n",
    "\n",
    "    # No mostramos la contraseña en logs\n",
    "    logger.info(f\"Conectando a la base de datos {db_name} en {host}:{port} como {user}\")\n",
    "\n",
    "    # Retornamos la cadena de conexión\n",
    "    return f\"postgresql://{user}:{password}@{host}:{port}/{db_name}\"\n",
    "\n",
    "# Ejemplo de enmascaramiento de datos sensibles\n",
    "def mask_sensitive_data(df, sensitive_columns):\n",
    "    \"\"\"Enmascara datos sensibles para logs o depuración\"\"\"\n",
    "    df_masked = df.copy()\n",
    "\n",
    "    for col in sensitive_columns:\n",
    "        if col in df.columns:\n",
    "            # Diferentes estrategias según el tipo de dato\n",
    "            if df[col].dtype == 'object':  # Para strings\n",
    "                df_masked[col] = df[col].apply(lambda x: x[:2] + '*' * (len(str(x)) - 4) + str(x)[-2:] if pd.notna(x) else x)\n",
    "            else:  # Para números\n",
    "                df_masked[col] = '****'\n",
    "\n",
    "    return df_masked\n",
    "\n",
    "# Ejemplo de uso\n",
    "sensitive_cols = ['email', 'nombre_cliente']\n",
    "df_clientes = pd.DataFrame({\n",
    "    'cliente_id': [1, 2, 3],\n",
    "    'nombre_cliente': ['Juan Pérez', 'María López', 'Carlos Rodríguez'],\n",
    "    'email': ['juan@ejemplo.com', 'maria@ejemplo.com', 'carlos@ejemplo.com'],\n",
    "    'tipo': ['Regular', 'Premium', 'VIP']\n",
    "})\n",
    "\n",
    "print(\"Datos originales:\")\n",
    "print(df_clientes)\n",
    "\n",
    "print(\"\\nDatos enmascarados para logs:\")\n",
    "df_masked = mask_sensitive_data(df_clientes, sensitive_cols)\n",
    "print(df_masked)\n",
    "\n",
    "# Limpiamos el archivo .env de ejemplo\n",
    "os.remove('.env')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f23d2c",
   "metadata": {},
   "source": [
    "## 7. Ejercicios para Estudiantes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fc8074",
   "metadata": {},
   "source": [
    "### Ejercicio 1: Extracción de datos\n",
    "Implementa un proceso de extracción que obtenga datos de:\n",
    "1. Una API pública (por ejemplo, JSONPlaceholder)\n",
    "2. Un archivo CSV local\n",
    "Combina ambos conjuntos de datos en un único DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ac4c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu código aquí\n",
    "\n",
    "# 1. Extracción desde API\n",
    "# Pista: Usa requests.get() y pd.DataFrame()\n",
    "\n",
    "# 2. Extracción desde CSV\n",
    "# Pista: Usa pd.read_csv()\n",
    "\n",
    "# 3. Combina los datos\n",
    "# Pista: Usa pd.concat() o pd.merge()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb511e6",
   "metadata": {},
   "source": [
    "### Ejercicio 2: Transformación de datos\n",
    "\n",
    "Utilizando el dataset de Iris:\n",
    "1. Identifica y maneja valores atípicos (outliers)\n",
    "2. Normaliza las variables numéricas\n",
    "3. Crea una nueva característica que sea la relación entre sepal_length y sepal_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbfaa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu código aquí\n",
    "\n",
    "# 1. Carga el dataset de Iris\n",
    "# Pista: Usa la URL proporcionada anteriormente\n",
    "\n",
    "# 2. Identifica outliers\n",
    "# Pista: Puedes usar el método IQR (rango intercuartílico)\n",
    "\n",
    "# 3. Normaliza variables\n",
    "# Pista: Usa MinMaxScaler o StandardScaler\n",
    "\n",
    "# 4. Crea nueva característica\n",
    "# Pista: Simplemente divide una columna por otra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64179ac",
   "metadata": {},
   "source": [
    "### Ejercicio 3: Carga de datos\n",
    "\n",
    "Implementa un proceso de carga que:\n",
    "1. Cree una base de datos SQLite\n",
    "2. Cargue los datos transformados en una tabla\n",
    "3. Realice una consulta para verificar la carga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac37503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu código aquí\n",
    "\n",
    "# 1. Crea conexión a SQLite\n",
    "# Pista: Usa sqlite3.connect()\n",
    "\n",
    "# 2. Carga los datos\n",
    "# Pista: Usa DataFrame.to_sql()\n",
    "\n",
    "# 3. Verifica con una consulta\n",
    "# Pista: Usa pd.read_sql()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46948d20",
   "metadata": {},
   "source": [
    "### Ejercicio 4: Pipeline ETL completo\n",
    "\n",
    "Implementa un pipeline ETL completo que:\n",
    "1. Extraiga datos de una fuente a tu elección\n",
    "2. Realice al menos tres transformaciones diferentes\n",
    "3. Cargue los datos en un destino\n",
    "4. Incluya manejo de errores y logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08473e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu código aquí\n",
    "\n",
    "# Implementa un pipeline ETL completo con las fases:\n",
    "# - Extracción\n",
    "# - Transformación\n",
    "# - Carga\n",
    "# - Manejo de errores\n",
    "# - Logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306a62c4",
   "metadata": {},
   "source": [
    "### Ejercicio 5: Proyecto final\n",
    "\n",
    "Diseña e implementa un proyecto ETL completo que resuelva un problema real o simulado. Debe incluir:\n",
    "1. Múltiples fuentes de datos\n",
    "2. Transformaciones complejas\n",
    "3. Carga en un modelo dimensional\n",
    "4. Visualización de resultados\n",
    "5. Documentación del proceso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72fe61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu proyecto final aquí\n",
    "\n",
    "# Implementa un proyecto ETL completo siguiendo las mejores prácticas\n",
    "# Documenta cada paso del proceso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e9c15b",
   "metadata": {},
   "source": [
    "## Recursos adicionales\n",
    "\n",
    "- **Libros**:\n",
    "  - \"The Data Warehouse Toolkit\" por Ralph Kimball\n",
    "  - \"Building a Data Warehouse: With Examples in SQL Server\" por Vincent Rainardi\n",
    "\n",
    "- **Cursos en línea**:\n",
    "  - [Coursera: Data Engineering with Google Cloud](https://www.coursera.org/specializations/gcp-data-engineering)\n",
    "  - [Udemy: The Complete ETL Developer Course](https://www.udemy.com/course/the-complete-etl-developer-course/)\n",
    "\n",
    "- **Herramientas**:\n",
    "  - [Apache Airflow](https://airflow.apache.org/)\n",
    "  - [dbt (data build tool)](https://www.getdbt.com/)\n",
    "  - [Apache NiFi](https://nifi.apache.org/)\n",
    "\n",
    "- **Blogs y sitios web**:\n",
    "  - [Towards Data Science](https://towardsdatascience.com/)\n",
    "  - [Data Engineering Weekly](https://dataengineeringweekly.substack.com/)\n",
    "  - [The Seattle Data Guy](https://www.theseattledataguy.com/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

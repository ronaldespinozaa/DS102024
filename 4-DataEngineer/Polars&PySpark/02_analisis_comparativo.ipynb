{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis Comparativo: Polars, PySpark y Pandas\n",
    "\n",
    "En esta sección, realizaremos un análisis comparativo detallado entre Polars, PySpark y Pandas. Examinaremos las diferencias en términos de rendimiento, sintaxis, funcionalidades y casos de uso específicos. Esta comparación nos ayudará a entender mejor cuándo y por qué elegir una tecnología sobre las otras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparativa de Rendimiento\n",
    "\n",
    "El rendimiento es uno de los factores más importantes al elegir una herramienta para procesamiento de datos. Veamos cómo se comparan estas tecnologías en diferentes escenarios de rendimiento.\n",
    "\n",
    "### Métricas de Rendimiento Clave\n",
    "\n",
    "1. **Tiempo de ejecución**: Tiempo necesario para completar operaciones comunes.\n",
    "2. **Uso de memoria**: Cantidad de RAM consumida durante el procesamiento.\n",
    "3. **Escalabilidad**: Capacidad para manejar conjuntos de datos cada vez más grandes.\n",
    "4. **Paralelismo**: Capacidad para aprovechar múltiples núcleos/máquinas.\n",
    "\n",
    "### Pandas vs Polars: Benchmark de Operaciones Comunes\n",
    "\n",
    "Polars generalmente supera a Pandas en rendimiento por varios factores:\n",
    "\n",
    "| Operación | Mejora de Rendimiento (Polars vs Pandas) |\n",
    "|-----------|------------------------------------------|\n",
    "| Filtrado | 5-10x más rápido |\n",
    "| Agrupación y agregación | 10-20x más rápido |\n",
    "| Uniones (joins) | 3-8x más rápido |\n",
    "| Ordenación | 2-5x más rápido |\n",
    "| Lectura de CSV/Parquet | 2-4x más rápido |\n",
    "\n",
    "### Factores que Contribuyen al Mejor Rendimiento de Polars\n",
    "\n",
    "1. **Implementación en Rust**: Polars está escrito en Rust, un lenguaje de programación de sistemas que ofrece rendimiento cercano a C/C++ con garantías de seguridad de memoria.\n",
    "\n",
    "2. **Formato columnar**: El almacenamiento columnar permite:\n",
    "   - Mejor utilización de la caché de CPU\n",
    "   - Compresión más eficiente\n",
    "   - Operaciones vectorizadas más rápidas\n",
    "   - Lectura selectiva de columnas (solo se cargan las columnas necesarias)\n",
    "\n",
    "3. **Ejecución perezosa (lazy)**: Permite optimizar el plan de ejecución completo antes de ejecutarlo.\n",
    "\n",
    "4. **Paralelismo automático**: Aprovecha todos los núcleos disponibles sin configuración adicional.\n",
    "\n",
    "### PySpark: Rendimiento a Escala\n",
    "\n",
    "PySpark está diseñado para escalar horizontalmente a través de múltiples máquinas, lo que lo hace ideal para conjuntos de datos extremadamente grandes:\n",
    "\n",
    "1. **Procesamiento distribuido**: Puede manejar petabytes de datos distribuyendo el trabajo entre cientos o miles de máquinas.\n",
    "\n",
    "2. **Procesamiento en memoria**: Mantiene los datos en memoria para iteraciones múltiples, lo que lo hace mucho más rápido que sistemas basados en disco como Hadoop MapReduce.\n",
    "\n",
    "3. **Optimizador Catalyst**: Optimiza automáticamente los planes de ejecución de consultas, similar al optimizador de consultas en bases de datos relacionales.\n",
    "\n",
    "4. **Limitaciones de rendimiento en conjuntos pequeños**: Para conjuntos de datos pequeños, la sobrecarga de la distribución puede hacer que PySpark sea más lento que Polars o incluso Pandas.\n",
    "\n",
    "### Comparativa de Uso de Memoria\n",
    "\n",
    "El uso eficiente de la memoria es crucial, especialmente cuando se trabaja con conjuntos de datos grandes:\n",
    "\n",
    "1. **Pandas**: \n",
    "   - Alto consumo de memoria (5-10x el tamaño de los datos originales)\n",
    "   - Limitado por la RAM disponible en una sola máquina\n",
    "   - Copias profundas frecuentes que aumentan el uso de memoria\n",
    "\n",
    "2. **Polars**: \n",
    "   - Uso de memoria más eficiente (2-3x el tamaño de los datos originales)\n",
    "   - Mejor gestión de memoria con menos copias innecesarias\n",
    "   - Sigue limitado por la RAM disponible en una sola máquina\n",
    "\n",
    "3. **PySpark**: \n",
    "   - No limitado por la RAM de una sola máquina\n",
    "   - Puede utilizar memoria distribuida en un clúster\n",
    "   - Spilling automático a disco cuando la memoria es insuficiente\n",
    "   - Gestión de memoria más compleja con regiones de memoria y garbage collection de la JVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparativa de Sintaxis\n",
    "\n",
    "La sintaxis y la API de una biblioteca afectan significativamente la experiencia del desarrollador y la productividad. Veamos cómo se comparan las sintaxis de Pandas, Polars y PySpark.\n",
    "\n",
    "### Creación de DataFrames\n",
    "\n",
    "Comparemos cómo se crean DataFrames en cada tecnología:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Datos de ejemplo\n",
    "data = {\n",
    "    'nombre': ['Ana', 'Carlos', 'María', 'Juan', 'Elena'],\n",
    "    'edad': [25, 32, 28, 41, 37],\n",
    "    'ciudad': ['Madrid', 'Barcelona', 'Sevilla', 'Valencia', 'Bilbao'],\n",
    "    'salario': [35000, 42000, 38000, 45000, 51000]\n",
    "}\n",
    "\n",
    "# Crear sesión de Spark si no existe\n",
    "if 'spark' not in locals():\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Comparativa Sintaxis\") \\\n",
    "        .config(\"spark.executor.memory\", \"2g\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Pandas\n",
    "df_pandas = pd.DataFrame(data)\n",
    "\n",
    "# Polars\n",
    "df_polars = pl.DataFrame(data)\n",
    "\n",
    "# PySpark\n",
    "df_spark = spark.createDataFrame([(name, age, city, salary) \n",
    "                                for name, age, city, salary in zip(data['nombre'], data['edad'], data['ciudad'], data['salario'])],\n",
    "                              schema=['nombre', 'edad', 'ciudad', 'salario'])\n",
    "\n",
    "# Alternativa más sencilla en PySpark usando pandas\n",
    "df_spark_alt = spark.createDataFrame(df_pandas)\n",
    "\n",
    "print(\"Pandas:\")\n",
    "print(df_pandas)\n",
    "print(\"\\nPolars:\")\n",
    "print(df_polars)\n",
    "print(\"\\nPySpark:\")\n",
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operaciones de Filtrado\n",
    "\n",
    "Comparemos cómo se realizan operaciones de filtrado en cada tecnología:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Filtrar personas mayores de 30 años\n",
    "\n",
    "# Pandas\n",
    "filtro_pandas = df_pandas[df_pandas['edad'] > 30]\n",
    "print(\"Filtro en Pandas:\")\n",
    "print(filtro_pandas)\n",
    "\n",
    "# Polars\n",
    "filtro_polars = df_polars.filter(pl.col(\"edad\") > 30)\n",
    "print(\"\\nFiltro en Polars:\")\n",
    "print(filtro_polars)\n",
    "\n",
    "# PySpark\n",
    "filtro_spark = df_spark.filter(df_spark.edad > 30)\n",
    "print(\"\\nFiltro en PySpark:\")\n",
    "filtro_spark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operaciones de Agrupación y Agregación\n",
    "\n",
    "Comparemos cómo se realizan operaciones de agrupación y agregación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calcular salario promedio por ciudad\n",
    "\n",
    "# Pandas\n",
    "agg_pandas = df_pandas.groupby('ciudad')['salario'].mean().reset_index()\n",
    "print(\"Agregación en Pandas:\")\n",
    "print(agg_pandas)\n",
    "\n",
    "# Polars\n",
    "agg_polars = df_polars.group_by('ciudad').agg(pl.col('salario').mean().alias('salario_promedio'))\n",
    "print(\"\\nAgregación en Polars:\")\n",
    "print(agg_polars)\n",
    "\n",
    "# PySpark\n",
    "from pyspark.sql.functions import avg\n",
    "agg_spark = df_spark.groupBy('ciudad').agg(avg('salario').alias('salario_promedio'))\n",
    "print(\"\\nAgregación en PySpark:\")\n",
    "agg_spark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operaciones de Unión (Join)\n",
    "\n",
    "Comparemos cómo se realizan operaciones de unión (join):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Crear un segundo DataFrame con información adicional\n",
    "datos_departamento = {\n",
    "    'nombre': ['Ana', 'Carlos', 'María', 'Juan', 'Elena'],\n",
    "    'departamento': ['Ventas', 'IT', 'Marketing', 'Finanzas', 'RRHH']\n",
    "}\n",
    "\n",
    "# Pandas\n",
    "df_dept_pandas = pd.DataFrame(datos_departamento)\n",
    "join_pandas = df_pandas.merge(df_dept_pandas, on='nombre')\n",
    "print(\"Join en Pandas:\")\n",
    "print(join_pandas)\n",
    "\n",
    "# Polars\n",
    "df_dept_polars = pl.DataFrame(datos_departamento)\n",
    "join_polars = df_polars.join(df_dept_polars, on='nombre')\n",
    "print(\"\\nJoin en Polars:\")\n",
    "print(join_polars)\n",
    "\n",
    "# PySpark\n",
    "df_dept_spark = spark.createDataFrame(datos_departamento)\n",
    "join_spark = df_spark.join(df_dept_spark, on='nombre')\n",
    "print(\"\\nJoin en PySpark:\")\n",
    "join_spark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecución Perezosa (Lazy Execution)\n",
    "\n",
    "Una diferencia importante entre estas tecnologías es el soporte para ejecución perezosa (lazy execution):\n",
    "\n",
    "- **Pandas**: Principalmente usa ejecución inmediata (eager execution).\n",
    "- **Polars**: Soporta tanto ejecución inmediata como perezosa.\n",
    "- **PySpark**: Utiliza ejecución perezosa por defecto.\n",
    "\n",
    "Veamos cómo se ve la ejecución perezosa en Polars y PySpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Ejecución perezosa en Polars\n",
    "lazy_query_polars = df_polars.lazy().filter(pl.col(\"edad\") > 30).group_by(\"ciudad\").agg(pl.col(\"salario\").mean())\n",
    "print(\"Plan de ejecución en Polars:\")\n",
    "print(lazy_query_polars)\n",
    "print(\"\\nResultado después de ejecutar:\")\n",
    "print(lazy_query_polars.collect())\n",
    "\n",
    "# Ejecución perezosa en PySpark (por defecto)\n",
    "lazy_query_spark = df_spark.filter(df_spark.edad > 30).groupBy(\"ciudad\").agg(avg(\"salario\"))\n",
    "print(\"\\nPlan de ejecución en PySpark:\")\n",
    "print(lazy_query_spark.explain())\n",
    "print(\"\\nResultado después de ejecutar:\")\n",
    "lazy_query_spark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparativa de Funcionalidades\n",
    "\n",
    "Además del rendimiento y la sintaxis, es importante comparar las funcionalidades disponibles en cada tecnología.\n",
    "\n",
    "### Ecosistema y Integración\n",
    "\n",
    "| Característica | Pandas | Polars | PySpark |\n",
    "|---------------|--------|--------|--------|\n",
    "| Integración con visualización | Excelente (Matplotlib, Seaborn, Plotly) | Buena (a través de conversión a Pandas) | Limitada (principalmente a través de conversión a Pandas) |\n",
    "| Integración con ML | Excelente (Scikit-learn, etc.) | Buena (a través de conversión a NumPy/Pandas) | Excelente (MLlib integrado, soporte para TensorFlow, PyTorch) |\n",
    "| Ecosistema | Muy maduro y extenso | En crecimiento | Muy maduro y extenso |\n",
    "| Comunidad y soporte | Muy grande | Creciente | Muy grande |\n",
    "| Documentación | Extensa | Buena, en desarrollo | Extensa |\n",
    "\n",
    "### Tipos de Datos y Operaciones\n",
    "\n",
    "| Característica | Pandas | Polars | PySpark |\n",
    "|---------------|--------|--------|--------|\n",
    "| Tipos de datos temporales | Bueno | Excelente | Bueno |\n",
    "| Manejo de datos faltantes | Excelente | Muy bueno | Bueno |\n",
    "| Operaciones de ventana | Disponible | Excelente | Excelente |\n",
    "| Expresiones complejas | Limitado | Excelente | Muy bueno |\n",
    "| Operaciones de texto | Excelente | Bueno | Bueno |\n",
    "| Operaciones estadísticas | Excelente | Bueno | Bueno |\n",
    "\n",
    "### Formatos de Entrada/Salida\n",
    "\n",
    "| Formato | Pandas | Polars | PySpark |\n",
    "|---------|--------|--------|--------|\n",
    "| CSV | ✓ | ✓ | ✓ |\n",
    "| JSON | ✓ | ✓ | ✓ |\n",
    "| Parquet | ✓ | ✓ | ✓ |\n",
    "| ORC | Limitado | ✓ | ✓ |\n",
    "| Excel | ✓ | Limitado | Limitado |\n",
    "| SQL | ✓ | Limitado | ✓ |\n",
    "| Avro | Limitado | Limitado | ✓ |\n",
    "| Bases de datos | Excelente | Limitado | Excelente |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Casos de Uso Específicos\n",
    "\n",
    "Basándonos en las comparativas anteriores, podemos identificar los casos de uso ideales para cada tecnología:\n",
    "\n",
    "### Pandas es ideal para:\n",
    "\n",
    "1. **Análisis exploratorio rápido**: Cuando necesitas explorar rápidamente un conjunto de datos pequeño a mediano.\n",
    "2. **Visualización de datos**: Integración perfecta con bibliotecas de visualización.\n",
    "3. **Manipulación de datos ad-hoc**: Operaciones rápidas y flexibles para transformaciones de datos no planificadas.\n",
    "4. **Prototipado**: Desarrollo rápido de flujos de trabajo de datos antes de escalarlos.\n",
    "5. **Conjuntos de datos pequeños**: Datos que caben cómodamente en la memoria de una sola máquina (hasta unos pocos GB).\n",
    "\n",
    "### Polars es ideal para:\n",
    "\n",
    "1. **Rendimiento en una sola máquina**: Cuando necesitas procesar datos más rápido que Pandas pero aún en una sola máquina.\n",
    "2. **Conjuntos de datos medianos a grandes**: Datos que caben en la memoria de una sola máquina pero son demasiado grandes para Pandas (varios GB a decenas de GB).\n",
    "3. **ETL de alto rendimiento**: Procesos de extracción, transformación y carga que requieren optimización de rendimiento.\n",
    "4. **Migración desde Pandas**: Cuando tu código Pandas se está volviendo demasiado lento pero no quieres cambiar a un framework distribuido.\n",
    "5. **Consultas complejas**: Cuando necesitas construir consultas complejas con múltiples transformaciones y agregaciones.\n",
    "\n",
    "### PySpark es ideal para:\n",
    "\n",
    "1. **Big Data**: Conjuntos de datos que no caben en la memoria de una sola máquina (cientos de GB a petabytes).\n",
    "2. **Procesamiento distribuido**: Cuando necesitas distribuir el procesamiento en múltiples máquinas.\n",
    "3. **Integración con ecosistemas de big data**: Cuando trabajas con HDFS, Hive, Kafka y otras tecnologías de big data.\n",
    "4. **Procesamiento de streaming**: Para procesar datos en tiempo real además de procesamiento por lotes.\n",
    "5. **Machine learning a gran escala**: Para entrenar modelos de ML en conjuntos de datos muy grandes.\n",
    "6. **ETL empresarial**: Procesos ETL robustos y escalables para entornos empresariales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estrategias de Migración\n",
    "\n",
    "Si estás considerando migrar de Pandas a Polars o PySpark, aquí hay algunas estrategias recomendadas:\n",
    "\n",
    "### De Pandas a Polars:\n",
    "\n",
    "1. **Migración gradual**: Comienza reemplazando las operaciones más intensivas en recursos con Polars, manteniendo el resto en Pandas.\n",
    "2. **Conversión bidireccional**: Aprovecha la fácil conversión entre DataFrames de Pandas y Polars:\n",
    "   ```python\n",
    "   # De Pandas a Polars\n",
    "   df_polars = pl.from_pandas(df_pandas)\n",
    "   \n",
    "   # De Polars a Pandas\n",
    "   df_pandas = df_polars.to_pandas()\n",
    "   ```\n",
    "3. **Adopta la ejecución perezosa**: Reescribe las consultas complejas utilizando la API perezosa de Polars para obtener mejores optimizaciones.\n",
    "4. **Actualiza los patrones de indexación**: Polars no tiene un concepto de índice como Pandas, así que necesitarás adaptar tu código.\n",
    "\n",
    "### De Pandas a PySpark:\n",
    "\n",
    "1. **Comprende el modelo distribuido**: Asegúrate de entender las implicaciones del procesamiento distribuido y cómo afecta a tus algoritmos.\n",
    "2. **Evita recolecciones innecesarias**: Minimiza las operaciones que requieren recolectar datos al driver (como `collect()` o `toPandas()`).\n",
    "3. **Optimiza las particiones**: Ajusta el número de particiones para un rendimiento óptimo.\n",
    "4. **Utiliza funciones ventana para operaciones secuenciales**: Reemplaza las operaciones que dependen del orden de las filas con funciones ventana.\n",
    "5. **Aprovecha la API de Pandas en PySpark**: Utiliza `pandas_udf` y la API de Pandas en PySpark para código más familiar:\n",
    "   ```python\n",
    "   from pyspark.sql.functions import pandas_udf\n",
    "   import pandas as pd\n",
    "   \n",
    "   @pandas_udf(\"double\")\n",
    "   def multiply_func(s1: pd.Series, s2: pd.Series) -> pd.Series:\n",
    "       return s1 * s2\n",
    "   \n",
    "   df.select(multiply_func(\"col1\", \"col2\"))\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones del Análisis Comparativo\n",
    "\n",
    "Después de comparar Pandas, Polars y PySpark en términos de rendimiento, sintaxis y funcionalidades, podemos extraer las siguientes conclusiones:\n",
    "\n",
    "1. **No existe una solución única para todos los casos**: La elección entre Pandas, Polars y PySpark depende del tamaño de los datos, los requisitos de rendimiento y el caso de uso específico.\n",
    "\n",
    "2. **Polars ofrece un excelente punto intermedio**: Para muchos casos de uso, Polars proporciona un equilibrio ideal entre la facilidad de uso de Pandas y el rendimiento de sistemas distribuidos como PySpark, especialmente para conjuntos de datos que aún caben en una sola máquina.\n",
    "\n",
    "3. **PySpark sigue siendo insustituible para big data real**: Cuando los datos superan la capacidad de una sola máquina, PySpark y su capacidad de procesamiento distribuido siguen siendo la mejor opción.\n",
    "\n",
    "4. **La transición puede ser gradual**: No es necesario reescribir todo el código de una vez; se puede adoptar un enfoque gradual, comenzando por las partes más críticas en términos de rendimiento.\n",
    "\n",
    "5. **El ecosistema está evolucionando rápidamente**: Polars está ganando popularidad y funcionalidades rápidamente, mientras que PySpark continúa mejorando su integración con el ecosistema de Python.\n",
    "\n",
    "En la siguiente sección, pondremos en práctica estos conocimientos implementando un ejemplo de ETL completo utilizando el dataset de taxis de Nueva York, donde podremos ver en acción las ventajas de Polars y PySpark sobre Pandas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

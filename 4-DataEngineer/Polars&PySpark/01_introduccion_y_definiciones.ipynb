{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento de Datos a Gran Escala: Polars y PySpark\n",
    "\n",
    "## Introducción\n",
    "\n",
    "En el mundo actual del Data Engineering, el procesamiento eficiente de grandes volúmenes de datos se ha convertido en una necesidad crítica. A medida que los conjuntos de datos crecen exponencialmente, las herramientas tradicionales como Pandas comienzan a mostrar limitaciones significativas. En este contexto, han surgido alternativas más potentes como Polars y PySpark, diseñadas específicamente para abordar los desafíos del procesamiento de datos a gran escala.\n",
    "\n",
    "Este notebook educativo está diseñado para proporcionar una comprensión profunda de estas tecnologías, sus ventajas sobre Pandas, y cómo implementarlas efectivamente en flujos de trabajo de ETL (Extracción, Transformación y Carga) modernos.\n",
    "\n",
    "Exploraremos:\n",
    "1. Definiciones y conceptos fundamentales de Pandas, Polars y PySpark\n",
    "2. Análisis comparativo detallado entre estas tecnologías\n",
    "3. Sintaxis y patrones de uso comunes\n",
    "4. Implementación de un ETL completo utilizando el dataset de taxis de Nueva York\n",
    "5. Ejercicios prácticos para consolidar el aprendizaje\n",
    "\n",
    "Comencemos con las definiciones básicas de cada tecnología."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definiciones\n",
    "\n",
    "### Pandas\n",
    "\n",
    "Pandas es una biblioteca de análisis de datos de código abierto para Python que proporciona estructuras de datos flexibles y herramientas para trabajar con datos estructurados. Creada por Wes McKinney en 2008, Pandas se ha convertido en una herramienta estándar en el ecosistema de ciencia de datos de Python.\n",
    "\n",
    "**Características principales:**\n",
    "- **DataFrame**: Estructura de datos bidimensional similar a una tabla de base de datos o una hoja de cálculo.\n",
    "- **Series**: Estructura de datos unidimensional similar a un array o lista etiquetada.\n",
    "- **Manipulación de datos**: Funciones para filtrar, transformar, agregar y visualizar datos.\n",
    "- **Manejo de datos faltantes**: Herramientas para detectar y manejar valores nulos.\n",
    "- **Integración con otras bibliotecas**: Funciona bien con NumPy, Matplotlib, Scikit-learn, etc.\n",
    "\n",
    "**Limitaciones:**\n",
    "- **Rendimiento en memoria**: Pandas carga todos los datos en memoria, lo que limita su capacidad para manejar conjuntos de datos muy grandes.\n",
    "- **Procesamiento en un solo núcleo**: Por defecto, Pandas utiliza un solo núcleo para el procesamiento, lo que limita su velocidad en operaciones complejas.\n",
    "- **Consumo de memoria**: Pandas puede consumir hasta 5-10 veces más memoria que el tamaño original de los datos debido a su estructura interna.\n",
    "\n",
    "**Casos de uso ideales:**\n",
    "- Análisis exploratorio de datos de tamaño pequeño a mediano (hasta unos pocos GB).\n",
    "- Limpieza y transformación de datos para conjuntos de datos que caben en memoria.\n",
    "- Visualización rápida y análisis estadístico de datos estructurados.\n",
    "- Prototipado rápido de flujos de trabajo de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polars\n",
    "\n",
    "Polars es una biblioteca de manipulación de datos de alto rendimiento implementada en Rust con enlaces a Python. Creada por Ritchie Vink en 2020, Polars está diseñada para ser una alternativa más rápida y eficiente a Pandas, especialmente para conjuntos de datos grandes.\n",
    "\n",
    "**Características principales:**\n",
    "- **Basada en Apache Arrow**: Utiliza el formato de memoria columnar de Arrow para un procesamiento eficiente.\n",
    "- **Ejecución perezosa (lazy)**: Permite optimizar consultas antes de ejecutarlas, similar a los motores de bases de datos modernos.\n",
    "- **Paralelismo automático**: Aprovecha múltiples núcleos de CPU sin configuración adicional.\n",
    "- **Operaciones vectorizadas**: Implementa operaciones optimizadas para procesamiento columnar.\n",
    "- **API similar a Pandas**: Facilita la transición desde Pandas con una sintaxis familiar pero más consistente.\n",
    "- **Gestión eficiente de memoria**: Consume significativamente menos memoria que Pandas para las mismas operaciones.\n",
    "\n",
    "**Ventajas sobre Pandas:**\n",
    "- **Rendimiento**: De 5 a 20 veces más rápido que Pandas en muchas operaciones comunes.\n",
    "- **Eficiencia de memoria**: Utiliza menos memoria para las mismas operaciones.\n",
    "- **Escalabilidad**: Puede manejar conjuntos de datos más grandes sin problemas.\n",
    "- **Consistencia de API**: Diseño más coherente y predecible que Pandas.\n",
    "- **Expresiones complejas**: Permite construir consultas complejas de manera más intuitiva.\n",
    "\n",
    "**Casos de uso ideales:**\n",
    "- Procesamiento de conjuntos de datos medianos a grandes (varios GB) que aún caben en la memoria de una sola máquina.\n",
    "- ETL de alto rendimiento para datos estructurados.\n",
    "- Análisis de datos que requieren operaciones complejas y rendimiento optimizado.\n",
    "- Migración desde Pandas cuando se necesita mejor rendimiento sin cambiar a un framework distribuido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark\n",
    "\n",
    "PySpark es la API de Python para Apache Spark, un motor de procesamiento distribuido de código abierto diseñado para el procesamiento de datos a gran escala. Desarrollado originalmente en la Universidad de California, Berkeley, Spark se ha convertido en una herramienta estándar para el procesamiento de big data.\n",
    "\n",
    "**Características principales:**\n",
    "- **Procesamiento distribuido**: Permite distribuir el procesamiento de datos a través de múltiples nodos en un clúster.\n",
    "- **Computación en memoria**: Mantiene los datos en memoria para un acceso más rápido durante iteraciones múltiples.\n",
    "- **Tolerancia a fallos**: Reconstruye automáticamente los datos perdidos en caso de fallos de nodos.\n",
    "- **Ecosistema completo**: Incluye módulos para SQL (Spark SQL), procesamiento de streaming (Structured Streaming), aprendizaje automático (MLlib) y análisis de grafos (GraphX).\n",
    "- **DataFrame API**: Proporciona una abstracción similar a Pandas pero distribuida.\n",
    "- **Optimizador Catalyst**: Optimiza automáticamente los planes de ejecución de consultas.\n",
    "\n",
    "**Ventajas sobre Pandas:**\n",
    "- **Escalabilidad horizontal**: Puede escalar a petabytes de datos distribuyendo el procesamiento en múltiples máquinas.\n",
    "- **Procesamiento fuera de memoria**: No está limitado por la RAM disponible en una sola máquina.\n",
    "- **Paralelismo masivo**: Aprovecha cientos o miles de núcleos en un clúster.\n",
    "- **Integración con ecosistemas de big data**: Se integra fácilmente con HDFS, Hive, Kafka, etc.\n",
    "- **Capacidades de streaming**: Permite procesar datos en tiempo real además de procesamiento por lotes.\n",
    "\n",
    "**Casos de uso ideales:**\n",
    "- Procesamiento de conjuntos de datos muy grandes (decenas de TB o más) que no caben en la memoria de una sola máquina.\n",
    "- ETL a escala de big data.\n",
    "- Análisis de datos distribuidos en tiempo real y por lotes.\n",
    "- Aplicaciones de machine learning a gran escala.\n",
    "- Procesamiento de datos en entornos de nube o clústeres on-premise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquitectura y Fundamentos Técnicos\n",
    "\n",
    "Para entender mejor por qué Polars y PySpark superan a Pandas en ciertos escenarios, es importante comprender las diferencias fundamentales en su arquitectura y diseño técnico.\n",
    "\n",
    "### Arquitectura de Pandas\n",
    "\n",
    "Pandas está construido sobre NumPy y utiliza sus arrays como estructura de datos subyacente. Su arquitectura se caracteriza por:\n",
    "\n",
    "- **Formato de almacenamiento en memoria**: Utiliza un formato de memoria basado en filas (aunque internamente usa arrays de NumPy que son columnares).\n",
    "- **Modelo de ejecución**: Ejecución inmediata (eager execution) de operaciones.\n",
    "- **Paralelismo**: Principalmente de un solo hilo, aunque algunas operaciones pueden usar múltiples núcleos con la opción `swifter` o `pandarallel`.\n",
    "- **Gestión de memoria**: Carga completa de los datos en memoria RAM.\n",
    "- **Optimización**: Limitada, principalmente a nivel de operaciones individuales.\n",
    "\n",
    "### Arquitectura de Polars\n",
    "\n",
    "Polars está implementado en Rust y utiliza Apache Arrow como formato de memoria subyacente. Su arquitectura se caracteriza por:\n",
    "\n",
    "- **Formato de almacenamiento en memoria**: Formato columnar de Apache Arrow, que permite un acceso más eficiente a los datos y mejor utilización de la caché de CPU.\n",
    "- **Modelo de ejecución**: Soporta tanto ejecución inmediata como ejecución perezosa (lazy execution) que permite optimizaciones globales.\n",
    "- **Paralelismo**: Paralelismo automático a nivel de CPU, aprovechando todos los núcleos disponibles.\n",
    "- **Gestión de memoria**: Gestión eficiente de memoria con menor overhead que Pandas.\n",
    "- **Optimización**: Optimizador de consultas que puede reordenar y combinar operaciones para mejorar el rendimiento.\n",
    "\n",
    "### Arquitectura de PySpark\n",
    "\n",
    "PySpark es una interfaz de Python para Apache Spark, que está implementado en Scala y se ejecuta en la JVM. Su arquitectura se caracteriza por:\n",
    "\n",
    "- **Arquitectura distribuida**: Modelo de computación distribuida con un nodo maestro (driver) y múltiples nodos trabajadores (executors).\n",
    "- **RDD (Resilient Distributed Dataset)**: Abstracción fundamental que representa una colección inmutable de objetos distribuidos en un clúster.\n",
    "- **DataFrame API**: Capa de abstracción sobre RDDs que proporciona una interfaz similar a SQL.\n",
    "- **Modelo de ejecución**: Ejecución perezosa con optimización global de consultas mediante el optimizador Catalyst.\n",
    "- **Gestión de memoria**: Combinación de almacenamiento en memoria y disco, con spilling automático a disco cuando la memoria es insuficiente.\n",
    "- **Tolerancia a fallos**: Reconstrucción automática de datos perdidos mediante el seguimiento de linaje (lineage).\n",
    "- **Paralelismo**: Paralelismo a nivel de clúster, con múltiples executors y múltiples núcleos por executor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando las Bibliotecas\n",
    "\n",
    "Veamos cómo importar cada una de estas bibliotecas y verificar sus versiones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Importar Pandas\n",
    "import pandas as pd\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "\n",
    "# Importar Polars\n",
    "import polars as pl\n",
    "print(f\"Polars version: {pl.__version__}\")\n",
    "\n",
    "# Importar PySpark\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "print(f\"PySpark version: {pyspark.__version__}\")\n",
    "\n",
    "# Crear una sesión de Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark Introduction\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Todas las bibliotecas importadas correctamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de DataFrames Básicos\n",
    "\n",
    "Veamos cómo crear un DataFrame simple en cada una de estas tecnologías:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Datos de ejemplo\n",
    "data = {\n",
    "    'nombre': ['Ana', 'Carlos', 'María', 'Juan', 'Elena'],\n",
    "    'edad': [25, 32, 28, 41, 37],\n",
    "    'ciudad': ['Madrid', 'Barcelona', 'Sevilla', 'Valencia', 'Bilbao'],\n",
    "    'salario': [35000, 42000, 38000, 45000, 51000]\n",
    "}\n",
    "\n",
    "# DataFrame en Pandas\n",
    "df_pandas = pd.DataFrame(data)\n",
    "print(\"DataFrame en Pandas:\")\n",
    "print(df_pandas)\n",
    "print(\"\\n\")\n",
    "\n",
    "# DataFrame en Polars\n",
    "df_polars = pl.DataFrame(data)\n",
    "print(\"DataFrame en Polars:\")\n",
    "print(df_polars)\n",
    "print(\"\\n\")\n",
    "\n",
    "# DataFrame en PySpark\n",
    "df_spark = spark.createDataFrame(data=[(name, age, city, salary) \n",
    "                                      for name, age, city, salary in zip(data['nombre'], data['edad'], data['ciudad'], data['salario'])],\n",
    "                                schema=['nombre', 'edad', 'ciudad', 'salario'])\n",
    "print(\"DataFrame en PySpark:\")\n",
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones sobre Definiciones\n",
    "\n",
    "Hemos explorado las definiciones y características fundamentales de Pandas, Polars y PySpark. Cada tecnología tiene sus propias fortalezas y casos de uso ideales:\n",
    "\n",
    "- **Pandas** es excelente para análisis exploratorio rápido y conjuntos de datos pequeños a medianos que caben en memoria.\n",
    "- **Polars** ofrece un rendimiento significativamente mejor que Pandas para conjuntos de datos medianos a grandes, manteniendo una API familiar pero más consistente.\n",
    "- **PySpark** es la solución para procesamiento de datos verdaderamente grandes que requieren distribución en múltiples máquinas.\n",
    "\n",
    "En las siguientes secciones, profundizaremos en un análisis comparativo detallado de estas tecnologías, explorando sus diferencias en términos de rendimiento, sintaxis y funcionalidades."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

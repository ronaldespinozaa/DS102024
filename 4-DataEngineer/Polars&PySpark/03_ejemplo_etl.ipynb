{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo ETL con Polars: Dataset de Taxis de Nueva York\n",
    "\n",
    "En esta sección, implementaremos un ejemplo completo de ETL (Extracción, Transformación y Carga) utilizando Polars para procesar el dataset de taxis de Nueva York. Este ejemplo demostrará las ventajas de Polars sobre Pandas en términos de rendimiento y funcionalidades.\n",
    "\n",
    "Nuestro ETL incluirá:\n",
    "1. Extracción de datos desde archivos Parquet\n",
    "2. Transformación y limpieza de datos con Polars\n",
    "3. Validación de datos con Pydantic\n",
    "4. Carga de datos en una base de datos SQLite utilizando SQLAlchemy\n",
    "5. Implementación de DAGs (Directed Acyclic Graphs) con Prefect\n",
    "6. Configuración de logging para seguimiento del proceso\n",
    "\n",
    "Comencemos explorando la estructura del proyecto y los componentes principales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estructura del Proyecto\n",
    "\n",
    "Nuestro proyecto ETL está organizado de la siguiente manera:\n",
    "\n",
    "```\n",
    "notebook_polars_pyspark/\n",
    "├── data/\n",
    "│   └── yellow_tripdata_2022-01.parquet  # Dataset de taxis de Nueva York\n",
    "├── etl_example/\n",
    "│   ├── __init__.py\n",
    "│   ├── etl_config.py      # Configuración del ETL\n",
    "│   ├── models.py          # Modelos Pydantic para validación\n",
    "│   ├── database.py        # Configuración de SQLAlchemy\n",
    "│   ├── logger.py          # Configuración de logging\n",
    "│   ├── etl_dag.py         # Implementación de DAGs con Prefect\n",
    "│   ├── output/            # Directorio para la base de datos\n",
    "│   └── logs/              # Directorio para logs\n",
    "└── notebooks/\n",
    "    ├── 01_introduccion_y_definiciones.ipynb\n",
    "    ├── 02_analisis_comparativo.ipynb\n",
    "    └── 03_ejemplo_etl.ipynb  # Este notebook\n",
    "```\n",
    "\n",
    "Vamos a examinar cada componente del ETL en detalle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuración del ETL (etl_config.py)\n",
    "\n",
    "El archivo `etl_config.py` contiene la configuración básica para nuestro ETL, incluyendo rutas de archivos, configuración de la base de datos y parámetros de logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Mostrar el contenido del archivo etl_config.py\n",
    "!cat ../etl_example/etl_config.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modelos de Datos con Pydantic (models.py)\n",
    "\n",
    "Utilizamos Pydantic para definir modelos de datos con validación estricta de tipos. Esto nos permite asegurar que los datos cumplen con nuestras expectativas antes de cargarlos en la base de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Mostrar el contenido del archivo models.py\n",
    "!cat ../etl_example/models.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ventajas de Pydantic para Validación de Datos\n",
    "\n",
    "Pydantic ofrece varias ventajas para la validación de datos en flujos ETL:\n",
    "\n",
    "1. **Validación de tipos en tiempo de ejecución**: Pydantic valida automáticamente los tipos de datos y convierte valores cuando es posible.\n",
    "2. **Validadores personalizados**: Podemos definir funciones de validación personalizadas para reglas de negocio específicas.\n",
    "3. **Documentación integrada**: Los modelos Pydantic son autodocumentados con descripciones de campos.\n",
    "4. **Integración con FastAPI y otras bibliotecas**: Pydantic se integra bien con el ecosistema de Python.\n",
    "5. **Manejo de errores detallado**: Proporciona mensajes de error claros cuando la validación falla."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuración de la Base de Datos con SQLAlchemy (database.py)\n",
    "\n",
    "Utilizamos SQLAlchemy para definir el esquema de la base de datos y gestionar las conexiones. SQLAlchemy nos permite trabajar con bases de datos de manera orientada a objetos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Mostrar el contenido del archivo database.py\n",
    "!cat ../etl_example/database.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ventajas de SQLAlchemy para ETL\n",
    "\n",
    "SQLAlchemy ofrece varias ventajas para los procesos ETL:\n",
    "\n",
    "1. **Abstracción de la base de datos**: Podemos cambiar el motor de base de datos sin modificar el código.\n",
    "2. **Mapeo objeto-relacional (ORM)**: Trabajamos con objetos Python en lugar de SQL directo.\n",
    "3. **Gestión de sesiones**: Manejo eficiente de transacciones y conexiones.\n",
    "4. **Migraciones de esquema**: Facilita la evolución del esquema de la base de datos.\n",
    "5. **Validación a nivel de base de datos**: Complementa la validación de Pydantic con restricciones a nivel de base de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configuración de Logging (logger.py)\n",
    "\n",
    "El sistema de logging nos permite seguir el progreso del ETL y diagnosticar problemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Mostrar el contenido del archivo logger.py\n",
    "!cat ../etl_example/logger.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Implementación de DAGs con Prefect (etl_dag.py)\n",
    "\n",
    "Utilizamos Prefect para implementar DAGs (Directed Acyclic Graphs) que definen el flujo de trabajo del ETL. Prefect nos permite definir tareas y sus dependencias, gestionar errores y monitorear el progreso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Mostrar el contenido del archivo etl_dag.py\n",
    "!cat ../etl_example/etl_dag.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecutando el ETL\n",
    "\n",
    "Ahora vamos a ejecutar nuestro ETL y analizar su rendimiento. Primero, importamos los módulos necesarios y configuramos el entorno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Añadir el directorio raíz al path para poder importar los módulos\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "# Importar los módulos del ETL\n",
    "from etl_example.etl_dag import nyc_taxi_etl_flow\n",
    "from etl_example.etl_config import DB_PATH, OUTPUT_DIR\n",
    "\n",
    "# Asegurar que el directorio de salida existe\n",
    "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Eliminar la base de datos si existe para empezar desde cero\n",
    "if DB_PATH.exists():\n",
    "    DB_PATH.unlink()\n",
    "\n",
    "print(f\"Configuración completada. La base de datos se creará en: {DB_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora ejecutamos el flujo ETL y medimos el tiempo que tarda en completarse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Ejecutar el flujo ETL y medir el tiempo\n",
    "start_time = time.time()\n",
    "\n",
    "# Ejecutar el flujo\n",
    "nyc_taxi_etl_flow()\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print(f\"\\nETL completado en {execution_time:.2f} segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificando los Resultados\n",
    "\n",
    "Vamos a verificar que los datos se hayan cargado correctamente en la base de datos SQLite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Conectar a la base de datos\n",
    "conn = sqlite3.connect(str(DB_PATH))\n",
    "\n",
    "# Consultar el número de registros en la tabla de viajes\n",
    "query_count = \"SELECT COUNT(*) FROM taxi_trips\"\n",
    "trip_count = pd.read_sql_query(query_count, conn).iloc[0, 0]\n",
    "\n",
    "# Consultar el número de ubicaciones\n",
    "location_count = pd.read_sql_query(\"SELECT COUNT(*) FROM locations\", conn).iloc[0, 0]\n",
    "\n",
    "print(f\"Número de viajes en la base de datos: {trip_count}\")\n",
    "print(f\"Número de ubicaciones en la base de datos: {location_count}\")\n",
    "\n",
    "# Consultar algunos viajes para verificar\n",
    "query_sample = \"SELECT * FROM taxi_trips LIMIT 5\"\n",
    "sample_trips = pd.read_sql_query(query_sample, conn)\n",
    "\n",
    "print(\"\\nMuestra de viajes:\")\n",
    "sample_trips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparación de Rendimiento: Polars vs Pandas\n",
    "\n",
    "Para demostrar las ventajas de rendimiento de Polars sobre Pandas, vamos a implementar una versión simplificada del mismo proceso ETL utilizando Pandas y comparar los tiempos de ejecución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from etl_example.etl_config import TAXI_DATA_FILE\n",
    "\n",
    "def etl_with_pandas():\n",
    "    # Extracción\n",
    "    start_time = time.time()\n",
    "    print(\"Extrayendo datos con Pandas...\")\n",
    "    df_pandas = pd.read_parquet(TAXI_DATA_FILE)\n",
    "    extraction_time = time.time() - start_time\n",
    "    print(f\"Extracción completada en {extraction_time:.2f} segundos\")\n",
    "    \n",
    "    # Transformación\n",
    "    start_time = time.time()\n",
    "    print(\"Transformando datos con Pandas...\")\n",
    "    \n",
    "    # Renombrar columnas para consistencia\n",
    "    column_mapping = {\n",
    "        \"VendorID\": \"vendor_id\",\n",
    "        \"tpep_pickup_datetime\": \"pickup_datetime\",\n",
    "        \"tpep_dropoff_datetime\": \"dropoff_datetime\",\n",
    "        \"PULocationID\": \"pickup_location_id\",\n",
    "        \"DOLocationID\": \"dropoff_location_id\"\n",
    "    }\n",
    "    df_pandas = df_pandas.rename(columns=column_mapping)\n",
    "    \n",
    "    # Filtrar viajes con distancia válida\n",
    "    df_pandas = df_pandas[df_pandas['trip_distance'] > 0]\n",
    "    \n",
    "    # Filtrar viajes con tarifa válida\n",
    "    df_pandas = df_pandas[df_pandas['fare_amount'] >= 0]\n",
    "    \n",
    "    # Calcular la duración del viaje en minutos\n",
    "    df_pandas['trip_duration_minutes'] = (df_pandas['dropoff_datetime'] - df_pandas['pickup_datetime']).dt.total_seconds() / 60\n",
    "    \n",
    "    # Filtrar viajes con duración válida\n",
    "    df_pandas = df_pandas[df_pandas['trip_duration_minutes'] > 0]\n",
    "    \n",
    "    # Calcular la velocidad promedio\n",
    "    df_pandas['avg_speed_mph'] = df_pandas['trip_distance'] / (df_pandas['trip_duration_minutes'] / 60)\n",
    "    \n",
    "    # Filtrar velocidades razonables\n",
    "    df_pandas = df_pandas[df_pandas['avg_speed_mph'] < 100]\n",
    "    \n",
    "    # Manejar valores nulos\n",
    "    df_pandas['passenger_count'] = df_pandas['passenger_count'].fillna(1)\n",
    "    df_pandas['congestion_surcharge'] = df_pandas['congestion_surcharge'].fillna(0)\n",
    "    df_pandas['Airport_fee'] = df_pandas['Airport_fee'].fillna(0)\n",
    "    \n",
    "    transformation_time = time.time() - start_time\n",
    "    print(f\"Transformación completada en {transformation_time:.2f} segundos\")\n",
    "    \n",
    "    return {\n",
    "        \"extraction_time\": extraction_time,\n",
    "        \"transformation_time\": transformation_time,\n",
    "        \"total_time\": extraction_time + transformation_time,\n",
    "        \"row_count\": len(df_pandas)\n",
    "    }\n",
    "\n",
    "def etl_with_polars():\n",
    "    import polars as pl\n",
    "    \n",
    "    # Extracción\n",
    "    start_time = time.time()\n",
    "    print(\"Extrayendo datos con Polars...\")\n",
    "    df_polars = pl.read_parquet(TAXI_DATA_FILE)\n",
    "    extraction_time = time.time() - start_time\n",
    "    print(f\"Extracción completada en {extraction_time:.2f} segundos\")\n",
    "    \n",
    "    # Transformación\n",
    "    start_time = time.time()\n",
    "    print(\"Transformando datos con Polars...\")\n",
    "    \n",
    "    # Renombrar columnas para consistencia\n",
    "    column_mapping = {\n",
    "        \"VendorID\": \"vendor_id\",\n",
    "        \"tpep_pickup_datetime\": \"pickup_datetime\",\n",
    "        \"tpep_dropoff_datetime\": \"dropoff_datetime\",\n",
    "        \"PULocationID\": \"pickup_location_id\",\n",
    "        \"DOLocationID\": \"dropoff_location_id\"\n",
    "    }\n",
    "    for old_name, new_name in column_mapping.items():\n",
    "        if old_name in df_polars.columns:\n",
    "            df_polars = df_polars.rename({old_name: new_name})\n",
    "    \n",
    "    # Filtrar viajes con distancia válida\n",
    "    df_polars = df_polars.filter(pl.col(\"trip_distance\") > 0)\n",
    "    \n",
    "    # Filtrar viajes con tarifa válida\n",
    "    df_polars = df_polars.filter(pl.col(\"fare_amount\") >= 0)\n",
    "    \n",
    "    # Calcular la duración del viaje en minutos\n",
    "    df_polars = df_polars.with_columns([\n",
    "        ((pl.col(\"dropoff_datetime\").dt.epoch() - pl.col(\"pickup_datetime\").dt.epoch()) / 60).alias(\"trip_duration_minutes\")\n",
    "    ])\n",
    "    \n",
    "    # Filtrar viajes con duración válida\n",
    "    df_polars = df_polars.filter(pl.col(\"trip_duration_minutes\") > 0)\n",
    "    \n",
    "    # Calcular la velocidad promedio\n",
    "    df_polars = df_polars.with_columns([\n",
    "        (pl.col(\"trip_distance\") / (pl.col(\"trip_duration_minutes\") / 60)).alias(\"avg_speed_mph\")\n",
    "    ])\n",
    "    \n",
    "    # Filtrar velocidades razonables\n",
    "    df_polars = df_polars.filter(pl.col(\"avg_speed_mph\") < 100)\n",
    "    \n",
    "    # Manejar valores nulos\n",
    "    df_polars = df_polars.with_columns([\n",
    "        pl.col(\"passenger_count\").fill_null(1),\n",
    "        pl.col(\"congestion_surcharge\").fill_null(0),\n",
    "        pl.col(\"Airport_fee\").fill_null(0)\n",
    "    ])\n",
    "    \n",
    "    transformation_time = time.time() - start_time\n",
    "    print(f\"Transformación completada en {transformation_time:.2f} segundos\")\n",
    "    \n",
    "    return {\n",
    "        \"extraction_time\": extraction_time,\n",
    "        \"transformation_time\": transformation_time,\n",
    "        \"total_time\": extraction_time + transformation_time,\n",
    "        \"row_count\": df_polars.shape[0]\n",
    "    }\n",
    "\n",
    "# Ejecutar ambas versiones y comparar\n",
    "print(\"=== Benchmark: Pandas vs Polars ===\")\n",
    "print(\"\\n1. Ejecutando ETL con Pandas...\")\n",
    "pandas_results = etl_with_pandas()\n",
    "\n",
    "print(\"\\n2. Ejecutando ETL con Polars...\")\n",
    "polars_results = etl_with_polars()\n",
    "\n",
    "# Calcular la mejora de rendimiento\n",
    "speedup_extraction = pandas_results[\"extraction_time\"] / polars_results[\"extraction_time\"]\n",
    "speedup_transformation = pandas_results[\"transformation_time\"] / polars_results[\"transformation_time\"]\n",
    "speedup_total = pandas_results[\"total_time\"] / polars_results[\"total_time\"]\n",
    "\n",
    "print(\"\\n=== Resultados del Benchmark ===\")\n",
    "print(f\"Filas procesadas: {pandas_results['row_count']}\")\n",
    "print(\"\\nTiempos de Pandas:\")\n",
    "print(f\"  - Extracción: {pandas_results['extraction_time']:.2f} segundos\")\n",
    "print(f\"  - Transformación: {pandas_results['transformation_time']:.2f} segundos\")\n",
    "print(f\"  - Total: {pandas_results['total_time']:.2f} segundos\")\n",
    "\n",
    "print(\"\\nTiempos de Polars:\")\n",
    "print(f\"  - Extracción: {polars_results['extraction_time']:.2f} segundos\")\n",
    "print(f\"  - Transformación: {polars_results['transformation_time']:.2f} segundos\")\n",
    "print(f\"  - Total: {polars_results['total_time']:.2f} segundos\")\n",
    "\n",
    "print(\"\\nMejora de rendimiento (Polars vs Pandas):\")\n",
    "print(f\"  - Extracción: {speedup_extraction:.2f}x más rápido\")\n",
    "print(f\"  - Transformación: {speedup_transformation:.2f}x más rápido\")\n",
    "print(f\"  - Total: {speedup_total:.2f}x más rápido\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ventajas de Polars para ETL\n",
    "\n",
    "Basándonos en la implementación y los resultados del benchmark, podemos destacar las siguientes ventajas de Polars para procesos ETL:\n",
    "\n",
    "1. **Rendimiento superior**: Como hemos visto en el benchmark, Polars es significativamente más rápido que Pandas en operaciones de extracción y transformación.\n",
    "\n",
    "2. **Ejecución perezosa (lazy)**: Polars permite definir un plan de ejecución completo antes de ejecutarlo, lo que permite optimizaciones globales.\n",
    "\n",
    "3. **Paralelismo automático**: Polars aprovecha automáticamente todos los núcleos disponibles sin configuración adicional.\n",
    "\n",
    "4. **Eficiencia de memoria**: Polars consume menos memoria que Pandas para las mismas operaciones.\n",
    "\n",
    "5. **API expresiva**: La API de Polars permite expresar transformaciones complejas de manera concisa y legible.\n",
    "\n",
    "6. **Integración con ecosistema de datos**: Polars se integra bien con formatos como Parquet, CSV, JSON, etc.\n",
    "\n",
    "7. **Consistencia de API**: La API de Polars es más consistente y predecible que la de Pandas.\n",
    "\n",
    "Estas ventajas hacen de Polars una excelente opción para procesos ETL que manejan conjuntos de datos medianos a grandes en una sola máquina."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ventajas de la Arquitectura ETL Implementada\n",
    "\n",
    "Nuestra arquitectura ETL combina varias tecnologías modernas para crear un flujo de trabajo robusto y eficiente:\n",
    "\n",
    "1. **Polars para procesamiento de datos**: Aprovechamos el rendimiento y la expresividad de Polars para las operaciones de extracción y transformación.\n",
    "\n",
    "2. **Pydantic para validación de datos**: Utilizamos Pydantic para asegurar que los datos cumplen con nuestras expectativas antes de cargarlos en la base de datos.\n",
    "\n",
    "3. **SQLAlchemy para acceso a base de datos**: Utilizamos SQLAlchemy para definir el esquema de la base de datos y gestionar las conexiones de manera orientada a objetos.\n",
    "\n",
    "4. **Prefect para orquestación de flujos**: Implementamos DAGs con Prefect para definir el flujo de trabajo, gestionar errores y monitorear el progreso.\n",
    "\n",
    "5. **Logging para seguimiento**: Configuramos un sistema de logging para seguir el progreso del ETL y diagnosticar problemas.\n",
    "\n",
    "Esta arquitectura proporciona:\n",
    "\n",
    "- **Modularidad**: Cada componente tiene una responsabilidad clara y puede ser modificado o reemplazado independientemente.\n",
    "- **Escalabilidad**: El diseño permite escalar a conjuntos de datos más grandes y flujos de trabajo más complejos.\n",
    "- **Mantenibilidad**: El código está organizado de manera lógica y sigue buenas prácticas de ingeniería de software.\n",
    "- **Robustez**: La validación de datos y el manejo de errores aseguran que el ETL sea resistente a problemas.\n",
    "- **Observabilidad**: El logging y la monitorización permiten seguir el progreso y diagnosticar problemas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "En este ejemplo, hemos implementado un ETL completo utilizando Polars para procesar el dataset de taxis de Nueva York. Hemos demostrado las ventajas de Polars sobre Pandas en términos de rendimiento y funcionalidades, y hemos construido una arquitectura ETL robusta y eficiente.\n",
    "\n",
    "Las principales conclusiones son:\n",
    "\n",
    "1. **Polars ofrece un rendimiento significativamente mejor que Pandas** para operaciones ETL, especialmente en conjuntos de datos medianos a grandes.\n",
    "\n",
    "2. **La combinación de Polars, Pydantic, SQLAlchemy y Prefect** proporciona una arquitectura ETL robusta, eficiente y mantenible.\n",
    "\n",
    "3. **La validación estricta de tipos con Pydantic** asegura la integridad de los datos antes de cargarlos en la base de datos.\n",
    "\n",
    "4. **La implementación de DAGs con Prefect** permite definir flujos de trabajo complejos de manera clara y gestionar errores de manera efectiva.\n",
    "\n",
    "5. **El logging y la monitorización** son esenciales para seguir el progreso del ETL y diagnosticar problemas.\n",
    "\n",
    "En la siguiente sección, presentaremos un ejercicio práctico para que los estudiantes implementen su propio ETL utilizando estas tecnologías."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio Práctico: Implementación de ETL con Polars y PySpark\n",
    "\n",
    "En este ejercicio, implementarás tu propio proceso ETL (Extracción, Transformación y Carga) utilizando Polars o PySpark para procesar el dataset de taxis de Nueva York. Este ejercicio te permitirá aplicar los conceptos aprendidos sobre procesamiento de datos a gran escala y experimentar con las ventajas de estas tecnologías sobre Pandas.\n",
    "\n",
    "## Objetivos del Ejercicio\n",
    "\n",
    "1. Implementar un proceso ETL completo utilizando Polars o PySpark (puedes elegir la tecnología que prefieras).\n",
    "2. Utilizar DAGs (Directed Acyclic Graphs) para definir el flujo de trabajo.\n",
    "3. Implementar validación estricta de tipos con Pydantic.\n",
    "4. Crear una base de datos SQLite con tablas relacionadas utilizando SQLAlchemy.\n",
    "5. Configurar un sistema de logging para seguimiento del proceso.\n",
    "\n",
    "## Estructura del Ejercicio\n",
    "\n",
    "Se te proporciona una estructura de proyecto con archivos de plantilla que contienen TODOs para que completes. La estructura es la siguiente:\n",
    "\n",
    "```\n",
    "notebook_polars_pyspark/\n",
    "├── data/\n",
    "│   └── yellow_tripdata_2022-01.parquet  # Dataset de taxis de Nueva York\n",
    "├── etl_exercise/\n",
    "│   ├── __init__.py\n",
    "│   ├── etl_config.py      # Configuración del ETL (completo)\n",
    "│   ├── models.py          # Modelos Pydantic para validación (con TODOs)\n",
    "│   ├── database.py        # Configuración de SQLAlchemy (con TODOs)\n",
    "│   ├── logger.py          # Configuración de logging (con TODOs)\n",
    "│   ├── etl_dag.py         # Implementación de DAGs con Prefect (con TODOs)\n",
    "│   ├── output/            # Directorio para la base de datos\n",
    "│   └── logs/              # Directorio para logs\n",
    "└── notebooks/\n",
    "    └── 04_ejercicio_etl.ipynb  # Este notebook\n",
    "```\n",
    "\n",
    "Vamos a examinar cada archivo de plantilla y los TODOs que debes completar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuración del ETL (etl_config.py)\n",
    "\n",
    "El archivo `etl_config.py` ya está completo y contiene la configuración básica para el ETL. Revisa su contenido para entender la configuración."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Mostrar el contenido del archivo etl_config.py\n",
    "!cat ../etl_exercise/etl_config.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modelos de Datos con Pydantic (models.py)\n",
    "\n",
    "El archivo `models.py` contiene plantillas para los modelos Pydantic que utilizarás para validar los datos. Debes completar los TODOs para definir los modelos completos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Mostrar el contenido del archivo models.py\n",
    "!cat ../etl_exercise/models.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tarea 1: Completar los Modelos Pydantic\n",
    "\n",
    "Completa el archivo `models.py` implementando los modelos Pydantic para validar los datos de viajes de taxi y ubicaciones. Asegúrate de incluir todos los campos necesarios y validadores apropiados.\n",
    "\n",
    "Aquí tienes una guía de los campos que debes incluir en el modelo `TaxiTrip`:\n",
    "\n",
    "- **Campos de tiempo**: `pickup_datetime`, `dropoff_datetime` (tipo `datetime`)\n",
    "- **Campos de ubicación**: `pickup_location_id`, `dropoff_location_id` (tipo `int`)\n",
    "- **Campos de pasajeros**: `passenger_count` (tipo `Optional[float]`)\n",
    "- **Campos de distancia**: `trip_distance` (tipo `float`)\n",
    "- **Campos de tarifa**: `fare_amount`, `extra`, `mta_tax`, `tip_amount`, `tolls_amount`, `improvement_surcharge`, `total_amount` (tipo `float`), `congestion_surcharge`, `airport_fee` (tipo `Optional[float]`)\n",
    "- **Campos de pago**: `payment_type` (tipo `int`)\n",
    "\n",
    "Y para el modelo `Location`:\n",
    "\n",
    "- `location_id` (tipo `int`)\n",
    "- `borough` (tipo `str`)\n",
    "- `zone` (tipo `str`)\n",
    "- `service_zone` (tipo `str`)\n",
    "\n",
    "Implementa validadores para asegurar la integridad de los datos, como:\n",
    "- La distancia del viaje debe ser positiva\n",
    "- Los montos deben ser positivos o cero\n",
    "- La fecha de entrega debe ser posterior a la fecha de recogida\n",
    "- El número de pasajeros debe estar en un rango válido\n",
    "\n",
    "Puedes utilizar el ejemplo ETL como referencia, pero asegúrate de entender cada parte del código que implementes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuración de la Base de Datos con SQLAlchemy (database.py)\n",
    "\n",
    "El archivo `database.py` contiene plantillas para los modelos SQLAlchemy que definirán el esquema de la base de datos. Debes completar los TODOs para definir las tablas y relaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Mostrar el contenido del archivo database.py\n",
    "!cat ../etl_exercise/database.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tarea 2: Completar los Modelos SQLAlchemy\n",
    "\n",
    "Completa el archivo `database.py` implementando los modelos SQLAlchemy para las tablas de ubicaciones y viajes de taxi. Asegúrate de definir todas las columnas necesarias y las relaciones entre las tablas.\n",
    "\n",
    "Para la clase `TaxiLocation`, define las siguientes columnas:\n",
    "- `location_id` (Integer, primary_key=True)\n",
    "- `borough` (String, nullable=False)\n",
    "- `zone` (String, nullable=False)\n",
    "- `service_zone` (String, nullable=False)\n",
    "\n",
    "Y las relaciones con los viajes:\n",
    "- `pickup_trips` (relationship con TaxiTripRecord, foreign_keys=\"TaxiTripRecord.pickup_location_id\")\n",
    "- `dropoff_trips` (relationship con TaxiTripRecord, foreign_keys=\"TaxiTripRecord.dropoff_location_id\")\n",
    "\n",
    "Para la clase `TaxiTripRecord`, define las columnas correspondientes a los campos del modelo Pydantic, incluyendo las claves foráneas para las ubicaciones de recogida y entrega.\n",
    "\n",
    "También implementa las funciones `init_db()` y `get_session()` para inicializar la base de datos y obtener una sesión de SQLAlchemy.\n",
    "\n",
    "Puedes utilizar el ejemplo ETL como referencia, pero asegúrate de entender cada parte del código que implementes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configuración de Logging (logger.py)\n",
    "\n",
    "El archivo `logger.py` contiene una plantilla para configurar el sistema de logging. Debes completar los TODOs para implementar la función `setup_logger`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Mostrar el contenido del archivo logger.py\n",
    "!cat ../etl_exercise/logger.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tarea 3: Implementar la Configuración de Logging\n",
    "\n",
    "Completa el archivo `logger.py` implementando la función `setup_logger` para configurar y devolver un logger. La función debe:\n",
    "\n",
    "1. Crear el directorio de logs si no existe\n",
    "2. Configurar el logger con el nombre especificado\n",
    "3. Establecer el nivel de logging\n",
    "4. Crear un manejador para archivo\n",
    "5. Crear un manejador para consola\n",
    "6. Crear el formato para los mensajes de log\n",
    "7. Agregar los manejadores al logger\n",
    "8. Devolver el logger configurado\n",
    "\n",
    "Puedes utilizar el ejemplo ETL como referencia, pero asegúrate de entender cada parte del código que implementes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Implementación de DAGs con Prefect (etl_dag.py)\n",
    "\n",
    "El archivo `etl_dag.py` contiene plantillas para las tareas y el flujo principal del ETL. Debes completar los TODOs para implementar cada tarea y el flujo completo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Mostrar el contenido del archivo etl_dag.py\n",
    "!cat ../etl_exercise/etl_dag.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tarea 4: Implementar las Tareas y el Flujo ETL\n",
    "\n",
    "Completa el archivo `etl_dag.py` implementando las siguientes tareas y el flujo principal:\n",
    "\n",
    "1. **Configurar el logger**: Descomenta y utiliza la función `setup_logger` para configurar el logger.\n",
    "\n",
    "2. **Implementar la tarea `extract_taxi_data`**: Esta tarea debe:\n",
    "   - Leer el archivo parquet con Polars\n",
    "   - Renombrar las columnas para que coincidan con nuestro modelo\n",
    "   - Registrar información sobre los datos extraídos\n",
    "   - Devolver el DataFrame\n",
    "\n",
    "3. **Implementar la tarea `transform_taxi_data`**: Esta tarea debe:\n",
    "   - Filtrar viajes con distancia válida (mayor a 0)\n",
    "   - Filtrar viajes con tarifa válida (mayor o igual a 0)\n",
    "   - Calcular la duración del viaje en minutos\n",
    "   - Filtrar viajes con duración válida (mayor a 0)\n",
    "   - Calcular la velocidad promedio (millas por hora)\n",
    "   - Filtrar velocidades razonables (menos de 100 mph)\n",
    "   - Manejar valores nulos\n",
    "   - Registrar información sobre la transformación\n",
    "   - Devolver el DataFrame transformado\n",
    "\n",
    "4. **Implementar la tarea `validate_taxi_data`**: Esta tarea debe:\n",
    "   - Convertir el DataFrame a diccionarios\n",
    "   - Validar cada registro con el modelo Pydantic\n",
    "   - Registrar información sobre la validación\n",
    "   - Devolver la lista de registros validados\n",
    "\n",
    "5. **Implementar la tarea `load_taxi_data`**: Esta tarea debe:\n",
    "   - Inicializar la base de datos\n",
    "   - Obtener una sesión\n",
    "   - Procesar los datos en lotes\n",
    "   - Registrar información sobre la carga\n",
    "\n",
    "6. **Implementar la función auxiliar `_load_batch`**: Esta función debe:\n",
    "   - Asegurar que las ubicaciones existan\n",
    "   - Crear los registros de viajes\n",
    "   - Manejar errores y hacer rollback si es necesario\n",
    "\n",
    "7. **Implementar el flujo principal `nyc_taxi_etl_flow`**: Este flujo debe:\n",
    "   - Extraer datos\n",
    "   - Transformar datos\n",
    "   - Validar datos\n",
    "   - Cargar datos\n",
    "   - Registrar información sobre el flujo\n",
    "\n",
    "Puedes utilizar el ejemplo ETL como referencia, pero asegúrate de entender cada parte del código que implementes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probando tu Implementación\n",
    "\n",
    "Una vez que hayas completado todas las tareas, puedes probar tu implementación ejecutando el flujo ETL. Primero, asegúrate de que todos los archivos estén correctamente implementados y guardados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Añadir el directorio raíz al path para poder importar los módulos\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "# Importar el flujo ETL\n",
    "from etl_exercise.etl_dag import nyc_taxi_etl_flow\n",
    "from etl_exercise.etl_config import DB_PATH, OUTPUT_DIR\n",
    "\n",
    "# Asegurar que el directorio de salida existe\n",
    "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Eliminar la base de datos si existe para empezar desde cero\n",
    "if DB_PATH.exists():\n",
    "    DB_PATH.unlink()\n",
    "\n",
    "print(f\"Configuración completada. La base de datos se creará en: {DB_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Ejecutar el flujo ETL y medir el tiempo\n",
    "start_time = time.time()\n",
    "\n",
    "# Ejecutar el flujo\n",
    "nyc_taxi_etl_flow()\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print(f\"\\nETL completado en {execution_time:.2f} segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificando los Resultados\n",
    "\n",
    "Verifica que los datos se hayan cargado correctamente en la base de datos SQLite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Conectar a la base de datos\n",
    "conn = sqlite3.connect(str(DB_PATH))\n",
    "\n",
    "# Consultar el número de registros en la tabla de viajes\n",
    "query_count = \"SELECT COUNT(*) FROM taxi_trips\"\n",
    "trip_count = pd.read_sql_query(query_count, conn).iloc[0, 0]\n",
    "\n",
    "# Consultar el número de ubicaciones\n",
    "location_count = pd.read_sql_query(\"SELECT COUNT(*) FROM locations\", conn).iloc[0, 0]\n",
    "\n",
    "print(f\"Número de viajes en la base de datos: {trip_count}\")\n",
    "print(f\"Número de ubicaciones en la base de datos: {location_count}\")\n",
    "\n",
    "# Consultar algunos viajes para verificar\n",
    "query_sample = \"SELECT * FROM taxi_trips LIMIT 5\"\n",
    "sample_trips = pd.read_sql_query(query_sample, conn)\n",
    "\n",
    "print(\"\\nMuestra de viajes:\")\n",
    "sample_trips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensiones del Ejercicio (Opcional)\n",
    "\n",
    "Si has completado todas las tareas y tu ETL funciona correctamente, puedes intentar estas extensiones para profundizar en el tema:\n",
    "\n",
    "1. **Implementar la versión con PySpark**: Si has utilizado Polars, intenta implementar el mismo ETL utilizando PySpark y compara el rendimiento.\n",
    "\n",
    "2. **Añadir más transformaciones**: Implementa transformaciones adicionales, como calcular la propina promedio por zona o identificar patrones de viaje.\n",
    "\n",
    "3. **Mejorar la validación de datos**: Añade más validadores a los modelos Pydantic para asegurar la integridad de los datos.\n",
    "\n",
    "4. **Implementar tests unitarios**: Crea tests unitarios para verificar que cada componente del ETL funciona correctamente.\n",
    "\n",
    "5. **Visualizar los resultados**: Utiliza matplotlib o seaborn para visualizar los datos procesados y extraer insights.\n",
    "\n",
    "6. **Optimizar el rendimiento**: Experimenta con diferentes configuraciones y técnicas para mejorar el rendimiento del ETL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusión\n",
    "\n",
    "En este ejercicio, has implementado un ETL completo utilizando tecnologías modernas como Polars o PySpark, Pydantic, SQLAlchemy y Prefect. Has aprendido a:\n",
    "\n",
    "1. Extraer datos de archivos Parquet\n",
    "2. Transformar y limpiar datos utilizando Polars o PySpark\n",
    "3. Validar datos con Pydantic\n",
    "4. Cargar datos en una base de datos SQLite utilizando SQLAlchemy\n",
    "5. Implementar DAGs con Prefect\n",
    "6. Configurar un sistema de logging\n",
    "\n",
    "Estas habilidades son fundamentales para un Data Engineer y te permitirán construir pipelines de datos robustos y eficientes en tu carrera profesional."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
